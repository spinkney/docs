% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  10pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[]{Lucida Console}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Stan Functions Reference},
  pdfauthor={Stan Development Team},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\let\digamma\undefined
\IfFileExists{lucimatx.sty}{
  % ----- if Lucida availalable------------------
  \usepackage[
    scale=0.875,
    stdmathitalics=true,
    stdmathdigits=true]{lucimatx}
 \linespread{1.02}
}{
  % ------ if Lucida not available --------------
  \usepackage{charter}
  % \usepackage{mathpazo}    % palatino with math
  % \usepackage{mathptmx}    % times with math
  \usepackage{amssymb}
  \linespread{1.03}
}

\usepackage{titlesec}
\titleformat{\chapter}{\bfseries\LARGE}{\thechapter.}{1.5pc}{}{}
\titlespacing{\chapter}{0pt}{0pt}{12pt}
\titleformat{\section}{\bfseries\large}{\thesection.}{1em}{}
\titlespacing{\section}{0pt}{6pt}{0pt}
\titleformat{\subsection}{\bfseries}{}{0em}{}
\titlespacing{\subsection}{0pt}{6pt}{0pt}
\titleformat{\subsubsection}{\slshape}{}{0em}{}
\titlespacing{\subsection}{0pt}{6pt}{0pt}

\usepackage[titles]{tocloft}
\renewcommand{\cftpartfont}{\normalsize\rm\bfseries}
\renewcommand{\cftpartpagefont}{\normalsize\rm\bfseries}
\setlength{\cftbeforechapskip}{2ex}
\setlength{\cftchapnumwidth}{3ex}
\renewcommand{\cftchapfont}{\normalsize\bf}
\renewcommand{\cftchappagefont}{\footnotesize\rm}
\renewcommand{\cftchapaftersnum}{.}
\renewcommand{\cftchapaftersnumb}{\hspace*{8pt}}
\renewcommand{\cftchapleader}{\hspace*{1em}}
\renewcommand{\cftchapafterpnum}{\cftparfillskip}
\renewcommand{\cftpnumalign}{l}
\setlength{\cftbeforesecskip}{0.5ex}
\setlength{\cftsecnumwidth}{5.5ex}
\setlength{\cftsecindent}{5ex}
\renewcommand{\cftsecfont}{\small}
\renewcommand{\cftsecpagefont}{\footnotesize}
\renewcommand{\cftsecaftersnum}{}
\renewcommand{\cftsecaftersnumb}{\hspace*{8pt}}
\renewcommand{\cftsecleader}{\hspace*{1em}}
\renewcommand{\cftsecafterpnum}{\cftparfillskip}
\renewcommand{\cftpnumalign}{l}

\usepackage{xcolor}
\definecolor{linkcolor}{RGB}{0,0,128}

\setlength{\paperwidth}{6in}
\setlength{\paperheight}{8in}
\pdfpagewidth=\paperwidth
\pdfpageheight=\paperheight

\setlength{\textwidth}{5in}
\setlength{\textheight}{6.75in}
\setlength{\oddsidemargin}{-0.5in}
\setlength{\evensidemargin}{-0.5in}
\setlength{\topmargin}{-0.75in}
\setlength{\headsep}{18pt}

\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}
\renewcommand{\textfraction}{0.07}

\setcounter{tocdepth}{0}

\raggedbottom
\sloppy

\let\cleardoublepage\clearpage

\pagestyle{empty}
\usepackage{makeidx}
\makeindex


\frontmatter

\usepackage{fancyhdr}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Stan Functions Reference}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 2.25}
\author{Stan Development Team}
\date{}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{overview}{%
\chapter*{Overview}\label{overview}}
\addcontentsline{toc}{chapter}{Overview}

This is the reference for the functions defined in the Stan math
library and available in the Stan programming language.

The Stan project comprises a domain-specific language for
probabilistic programming, a differentiable mathematics and
probability library, algorithms for Bayesian posterior inference and
posterior analysis, along with interfaces and analysis tools in all of
the popular data analysis languages.

In addition to this reference manual, there is a user's guide and
a language reference manual for the Stan language and algorithms.
The \href{https://mc-stan.org/docs/stan-users-guide/index.html}{\emph{Stan User's Guide}}
provides example models and programming
techniques for coding statistical models in Stan.
The \href{https://mc-stan.org/docs/reference-manual/index.html}{\emph{Stan Reference Manual}}
specifies the Stan programming language
and inference algorithms.

There is also a separate installation and getting started guide for
each of the Stan interfaces (R, Python, Julia, Stata, MATLAB,
Mathematica, and command line).

\hypertarget{interfaces-and-platforms}{%
\subsubsection*{Interfaces and platforms}\label{interfaces-and-platforms}}
\addcontentsline{toc}{subsubsection}{Interfaces and platforms}

Stan runs under Windows, Mac OS X, and Linux.

Stan uses a domain-specific programming language that is portable
across data analysis languages. Stan has interfaces for R, Python,
Julia, MATLAB, Mathematica, Stata, and the command line, as well
as an alternative language interface in Scala. See the web site
\url{https://mc-stan.org} for interface-specific
links and getting started instructions

\hypertarget{web-site}{%
\subsubsection*{Web site}\label{web-site}}
\addcontentsline{toc}{subsubsection}{Web site}

The official resource for all things related to Stan is the web site:

\url{https://mc-stan.org}

The web site links to all of the packages comprising Stan for both
users and developers. This is the place to get started with Stan.
Find the interface in the language you want to use and follow the
download, installation, and getting started instructions.

\hypertarget{github-organization}{%
\subsubsection*{GitHub organization}\label{github-organization}}
\addcontentsline{toc}{subsubsection}{GitHub organization}

Stan's source code and much of the developer process is hosted on
GitHub. Stan's organization is:

\url{https://github.com/stan-dev}

Each package has its own repository within the \texttt{stan-dev}
organization. The web site is also hosted and managed through GitHub.
This is the place to peruse the source code, request features, and
report bugs. Much of the ongoing design discussion is hosted on the
GitHub Wiki.

\hypertarget{forums}{%
\subsubsection*{Forums}\label{forums}}
\addcontentsline{toc}{subsubsection}{Forums}

Stan hosts message boards for discussing all things
related to Stan.

\url{https://discourse.mc-stan.org}

This is the place to ask questions about Stan, including modeling,
programming, and installation.

\hypertarget{licensing}{%
\subsubsection*{Licensing}\label{licensing}}
\addcontentsline{toc}{subsubsection}{Licensing}

\begin{itemize}
\tightlist
\item
  \emph{Computer code:} \href{https://opensource.org/licenses/BSD-3-Clause}{BSD 3-clause license}
\end{itemize}

The core C++ code underlying Stan, including the math library,
language, and inference algorithms, is licensed under the BSD 3-clause
licensed as detailed in each repository and on the web site along
with the distribution links.

\begin{itemize}
\tightlist
\item
  \emph{Logo:} \href{https://mc-stan.org/about/logo/}{Stan logo usage guidelines}
\end{itemize}

\hypertarget{acknowledgements}{%
\subsubsection*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{subsubsection}{Acknowledgements}

The Stan project could not exist without the generous grant
funding of many grant agencies to the participants in the project.
For more details of direct funding for the project, see the web site
and project pages of the Stan developers.

The Stan project could also not exist without the generous
contributions of its users in reporting and in many cases fixing bugs
in the code and its documentation. We used to try to list all of
those who contributed patches and bug reports for the manual here, but
when that number passed into the hundreds, it became too difficult to
manage reliably. Instead, we will defer to GitHub (link above), where
all contributions to the project are made and tracked.

Finally, we should all thank the Stan developers, without whom this
project could not exist. We used to try and list the developers here,
but like the bug reporters, once the list grew into the dozens, it
became difficult to track. Instead, we will defer to the Stan web
page and GitHub itself for a list of core developers and all developer
contributions respectively.

\pagestyle{headings}
\mainmatter

\hypertarget{built-in-functions}{%
\chapter*{Built-In Functions}\label{built-in-functions}}
\addcontentsline{toc}{chapter}{Built-In Functions}

\hypertarget{void-functions}{%
\chapter{Void Functions}\label{void-functions}}

Stan does not technically support functions that do not return values.
It does support two types of statements, one printing and one for rejecting
outputs.

Although \texttt{print} and \texttt{reject} appear to have the syntax of functions,
they are actually special kinds of statements with slightly different
form and behavior than other functions. First, they are the
constructs that allow a variable number of arguments. Second, they
are the the only constructs to accept string literals (e.g., \texttt{"hello\ world"}) as arguments. Third, they have no effect on the log density
function and operate solely through side effects.

The special keyword \texttt{void} is used for their return type because they behave
like variadic functions with void return type, even though they are special
kinds of statements.

\hypertarget{print-statement}{%
\section{Print statement}\label{print-statement}}

Printing has no effect on the model's log probability function. Its
sole purpose is the side effect (i.e., an effect not represented in a
return value) of arguments being printed to whatever the standard
output stream is connected to (e.g., the terminal in command-line Stan
or the R console in RStan).

\index{{\tt \bfseries print }!{\tt (T1 x1,..., TN xN): void}|hyperpage}

\texttt{void} \textbf{\texttt{print}}\texttt{(T1\ x1,...,\ TN\ xN)}\newline
Print the values denoted by the arguments x1 through xN on the output
message stream. There are no spaces between items in the print, but a
line feed (LF; Unicode U+000A; C++ literal \texttt{\textquotesingle{}\textbackslash{}n\textquotesingle{}}) is inserted at
the end of the printed line. The types \texttt{T1} through \texttt{TN} can be any of
Stan's built-in numerical types or double quoted strings of ASCII
characters.

\hypertarget{reject-statement}{%
\section{Reject statement}\label{reject-statement}}

The reject statement has the same syntax as the print statement,
accepting an arbitrary number of arguments of any type (including
string literals). The effect of executing a reject statement is to
throw an exception internally that terminates the current iteration
with a rejection (the behavior of which will depend on the algorithmic
context in which it occurs).

\index{{\tt \bfseries reject }!{\tt (T1 x1,..., TN xN): void}|hyperpage}

\texttt{void} \textbf{\texttt{reject}}\texttt{(T1\ x1,...,\ TN\ xN)}\newline
Reject the current iteration and print the values denoted by the
arguments x1 through xN on the output message stream. There are no
spaces between items in the print, but a line feed (LF; Unicode
U+000A; C++ literal \texttt{\textquotesingle{}\textbackslash{}n\textquotesingle{}}) is inserted at the end of the printed
line. The types \texttt{T1} through \texttt{TN} can be any of Stan's built-in
numerical types or double quoted strings of ASCII characters.

\hypertarget{integer-valued-basic-functions}{%
\chapter{Integer-Valued Basic Functions}\label{integer-valued-basic-functions}}

This chapter describes Stan's built-in function that take various
types of arguments and return results of type integer.

\hypertarget{int-arithmetic}{%
\section{Integer-valued arithmetic operators}\label{int-arithmetic}}

Stan's arithmetic is based on standard double-precision C++ integer
and floating-point arithmetic. If the arguments to an arithmetic
operator are both integers, as in \texttt{2\ +\ 2}, integer arithmetic is used.
If one argument is an integer and the other a floating-point value, as
in \texttt{2.0\ +\ 2} and \texttt{2\ +\ 2.0}, then the integer is promoted to a floating
point value and floating-point arithmetic is used.

Integer arithmetic behaves slightly differently than floating point
arithmetic. The first difference is how overflow is treated. If the
sum or product of two integers overflows the maximum integer
representable, the result is an undesirable wraparound behavior at the
bit level. If the integers were first promoted to real numbers, they
would not overflow a floating-point representation. There are no
extra checks in Stan to flag overflows, so it is up to the user to
make sure it does not occur.

Secondly, because the set of integers is not closed under division and
there is no special infinite value for integers, integer division
implicitly rounds the result. If both arguments are positive, the
result is rounded down. For example, \texttt{1\ /\ 2} evaluates to 0 and \texttt{5\ /\ 3} evaluates to 1.

If one of the integer arguments to division is negative, the latest
C++ specification ( C++11), requires rounding toward zero. This would
have \texttt{1\ /\ 2} and \texttt{-1\ /\ 2} evaluate to 0, \texttt{-7\ /\ 2} evaluate to -3, and
\texttt{7\ /\ 2} evaluate to 3. Before the C++11 specification, the behavior
was platform dependent, allowing rounding up or down. All compilers
recent enough to be able to deal with Stan's templating should follow
the C++11 specification, but it may be worth testing if you are not
sure and plan to use integer division with negative values.

Unlike floating point division, where \texttt{1.0\ /\ 0.0} produces the special
positive infinite value, integer division by zero, as in \texttt{1\ /\ 0}, has
undefined behavior in the C++ standard. For example, the clang++
compiler on Mac OS X returns 3764, whereas the g++ compiler throws an
exception and aborts the program with a warning. As with overflow, it
is up to the user to make sure integer divide-by-zero does not occur.

\hypertarget{binary-infix-operators}{%
\subsection{Binary infix operators}\label{binary-infix-operators}}

Operators are described using the C++ syntax. For instance, the
binary operator of addition, written \texttt{X\ +\ Y}, would have the Stan
signature \texttt{int\ operator+(int,int)} indicating it takes two real
arguments and returns a real value. As noted previously, the value of
integer division is platform-dependent when rounding is platform
dependent before C++11; the descriptions below provide the C++11
definition.

\index{{\tt \bfseries operator\_add }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator+}}\texttt{(int\ x,\ int\ y)}\newline
The sum of the addends x and y \[ \text{operator+}(x,y) = (x + y) \]

\index{{\tt \bfseries operator\_subtract }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator-}}\texttt{(int\ x,\ int\ y)}\newline
The difference between the minuend x and subtrahend y \[
\text{operator-}(x,y) = (x - y) \]

\index{{\tt \bfseries operator\_multiply }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator*}}\texttt{(int\ x,\ int\ y)}\newline
The product of the factors x and y \[ \text{operator*}(x,y) = (x
\times y) \]

\index{{\tt \bfseries operator\_divide }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator/}}\texttt{(int\ x,\ int\ y)}\newline
The integer quotient of the dividend x and divisor y \[
\text{operator/}(x,y) = \begin{cases} \lfloor x / y \rfloor & \text{if
} x / y \geq 0 \\ - \lfloor \text{floor}(-x / y) \rfloor & \text{if }
x / y < 0. \end{cases} \]

\index{{\tt \bfseries operator\_mod }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\%}}\texttt{(int\ x,\ int\ y)}\newline
x modulo y, which is the positive remainder after dividing x by y. If
both x and y are non-negative, so is the result; otherwise, the sign
of the result is platform dependent. \[ \mathrm{operator\%}(x, y) \ =
\ x \ \text{mod} \ y \ = \ x - y * \lfloor x / y \rfloor \]

\hypertarget{unary-prefix-operators}{%
\subsection{Unary prefix operators}\label{unary-prefix-operators}}

\index{{\tt \bfseries operator\_subtract }!{\tt (int x): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator-}}\texttt{(int\ x)}\newline
The negation of the subtrahend x {[} \text{operator-}(x) = -x

\index{{\tt \bfseries operator\_add }!{\tt (int x): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator+}}\texttt{(int\ x)}\newline
This is a no-op. \[ \text{operator+}(x) = x \]

\hypertarget{absolute-functions}{%
\section{Absolute functions}\label{absolute-functions}}

\index{{\tt \bfseries abs }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{abs}}\texttt{(T\ x)}\newline
absolute value of x

\index{{\tt \bfseries int\_step }!{\tt (int x): int}|hyperpage}

\texttt{int} \textbf{\texttt{int\_step}}\texttt{(int\ x)}\newline

\index{{\tt \bfseries int\_step }!{\tt (real x): int}|hyperpage}

\texttt{int} \textbf{\texttt{int\_step}}\texttt{(real\ x)}\newline
Return the step function of x as an integer, \[ \mathrm{int\_step}(x)
= \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \text{
or } x \text{ is } NaN \end{cases} \] \emph{\textbf{Warning:}} \texttt{int\_step(0)} and
\texttt{int\_step(NaN)} return 0 whereas \texttt{step(0)} and \texttt{step(NaN)} return 1.

See the warning in section \protect\hyperlink{step-functions}{step functions} about the dangers of
step functions applied to anything other than data.

\hypertarget{bound-functions}{%
\section{Bound functions}\label{bound-functions}}

\index{{\tt \bfseries min }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{min}}\texttt{(int\ x,\ int\ y)}\newline
Return the minimum of x and y. \[ \text{min}(x, y) = \begin{cases} x &
\text{if } x < y\\ y & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries max }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{max}}\texttt{(int\ x,\ int\ y)}\newline
Return the maximum of x and y. \[ \text{max}(x, y) = \begin{cases} x &
\text{if } x > y\\ y & \text{otherwise} \end{cases} \]

\hypertarget{real-valued-basic-functions}{%
\chapter{Real-Valued Basic Functions}\label{real-valued-basic-functions}}

This chapter describes built-in functions that take zero or more real
or integer arguments and return real values.

\hypertarget{fun-vectorization}{%
\section{Vectorization of real-valued functions}\label{fun-vectorization}}

Although listed in this chapter, many of Stan's built-in functions are
vectorized so that they may be applied to any argument type. The
vectorized form of these functions is not any faster than writing an
explicit loop that iterates over the elements applying the
function---it's just easier to read and write and less error prone.

\hypertarget{unary-function-vectorization}{%
\subsection{Unary function vectorization}\label{unary-function-vectorization}}

Many of Stan's unary functions can be applied to any argument type.
For example, the exponential function, \texttt{exp}, can be applied to \texttt{real}
arguments or arrays of \texttt{real} arguments. Other than for integer
arguments, the result type is the same as the argument type, including
dimensionality and size. Integer arguments are first promoted to real
values, but the result will still have the same dimensionality and
size as the argument.

\hypertarget{real-and-real-array-arguments}{%
\subsubsection{Real and real array arguments}\label{real-and-real-array-arguments}}

When applied to a simple real value, the result is a real value. When
applied to arrays, vectorized functions like \texttt{exp()} are defined
elementwise. For example,

\begin{verbatim}
 // declare some variables for arguments
 real x0;
 real x1[5];
 real x2[4, 7];
 ...
 // declare some variables for results
 real y0;
 real y1[5];
 real y2[4, 7];
 ...
 // calculate and assign results
 y0 = exp(x0);
 y1 = exp(x1);
 y2 = exp(x2);
\end{verbatim}

When \texttt{exp} is applied to an array, it applies elementwise. For
example, the statement above,

\begin{verbatim}
 y2 = exp(x2);
\end{verbatim}

produces the same result for \texttt{y2} as the explicit loop

\begin{verbatim}
 for (i in 1:4)
   for (j in 1:7)
     y2[i, j] = exp(x2[i, j]);
\end{verbatim}

\hypertarget{vector-and-matrix-arguments}{%
\subsubsection{Vector and matrix arguments}\label{vector-and-matrix-arguments}}

Vectorized functions also apply elementwise to vectors and matrices.
For example,

\begin{verbatim}
 vector[5] xv;
 row_vector[7] xrv;
 matrix[10, 20] xm;
 
 vector[5] yv;
 row_vector[7] yrv;
 matrix[10, 20] ym;
 
 yv = exp(xv);
 yrv = exp(xrv);
 ym = exp(xm);
\end{verbatim}

Arrays of vectors and matrices work the same way. For example,

\begin{verbatim}
 matrix[17, 93] u[12];
 
 matrix[17, 93] z[12];
 
 z = exp(u);
\end{verbatim}

After this has been executed, \texttt{z{[}i,\ j,\ k{]}} will be equal to \texttt{exp(u{[}i,\ j,\ k{]})}.

\hypertarget{integer-and-integer-array-arguments}{%
\subsubsection{Integer and integer array arguments}\label{integer-and-integer-array-arguments}}

Integer arguments are promoted to real values in vectorized unary
functions. Thus if \texttt{n} is of type \texttt{int}, \texttt{exp(n)} is of type \texttt{real}.
Arrays work the same way, so that if \texttt{n2} is a one dimensional array
of integers, then \texttt{exp(n2)} will be a one-dimensional array of reals
with the same number of elements as \texttt{n2}. For example,

\begin{verbatim}
 int n1[23];
 real z1[23];
 z1 = exp(n1);
\end{verbatim}

It would be illegal to try to assign \texttt{exp(n1)} to an array of
integers; the return type is a real array.

\hypertarget{binary-function-vectorization}{%
\subsection{Binary function vectorization}\label{binary-function-vectorization}}

Like the unary functions, many of Stan's binary functions have been
vectorized, and can be applied elementwise to combinations of both
scalars or container types.

\hypertarget{scalar-and-scalar-array-arguments}{%
\subsubsection{Scalar and scalar array arguments}\label{scalar-and-scalar-array-arguments}}

When applied to two scalar values, the result is a scalar value. When
applied to two arrays, or combination of a scalar value and an array,
vectorized functions like \texttt{pow()} are defined elementwise. For example,

\begin{verbatim}
 // declare some variables for arguments
 real x00;
 real x01;
 real x10[5];
 real x11[5];
 real x20[4, 7];
 real x21[4, 7];
 ...
 // declare some variables for results
 real y0;
 real y1[5];
 real y2[4, 7];
 ...
 // calculate and assign results
 y0 = pow(x00, x01);
 y1 = pow(x10, x11);
 y2 = pow(x20, x21);
\end{verbatim}

When \texttt{pow} is applied to two arrays, it applies elementwise. For
example, the statement above,

\begin{verbatim}
 y2 = pow(x20, x21);
\end{verbatim}

produces the same result for \texttt{y2} as the explicit loop

\begin{verbatim}
 for (i in 1:4)
   for (j in 1:7)
     y2[i, j] = pow(x20[i, j], x21[i, j]);
\end{verbatim}

Alternatively, if a combination of an array and a scalar are
provided, the scalar value is broadcast to be applied to each
value of the array. For example, the following statement:

\begin{verbatim}
y2 = pow(x20, x00);
\end{verbatim}

produces the same result for \texttt{y2} as the explicit loop:

\begin{verbatim}
 for (i in 1:4)
   for (j in 1:7)
     y2[i, j] = pow(x20[i, j], x00);
\end{verbatim}

\hypertarget{vector-and-matrix-arguments-1}{%
\subsubsection{Vector and matrix arguments}\label{vector-and-matrix-arguments-1}}

Vectorized binary functions also apply elementwise to vectors and matrices,
and to combinations of these with scalar values.
For example,

\begin{verbatim}
 real x00;
 vector[5] xv00;
 vector[5] xv01;
 row_vector[7] xrv;
 matrix[10, 20] xm;
 
 vector[5] yv;
 row_vector[7] yrv;
 matrix[10, 20] ym;
 
 yv = pow(xv00, xv01);
 yrv = pow(xrv, x00);
 ym = pow(x00, xm);
\end{verbatim}

Arrays of vectors and matrices work the same way. For example,

\begin{verbatim}
 matrix[17, 93] u[12];
 
 matrix[17, 93] z[12];
 
 z = pow(u, x00);
\end{verbatim}

After this has been executed, \texttt{z{[}i,\ j,\ k{]}} will be equal to \texttt{pow(u{[}i,\ j,\ k{]},\ x00)}.

\hypertarget{input-return-types}{%
\subsubsection{Input \& return types}\label{input-return-types}}

Vectorised binary functions require that both inputs, unless one is a real,
be containers of the same type and size. For example, the following statements
are legal:

\begin{verbatim}
 vector[5] xv;
 row_vector[7] xrv;
 matrix[10, 20] xm;
 
 vector[5] yv = pow(xv, xv)
 row_vector[7] yrv = pow(xrv, xrv)
 matrix[10, 20] = pow(xm, xm)
\end{verbatim}

But the following statements are not:

\begin{verbatim}
 vector[5] xv;
 vector[7] xv2;
 row_vector[5] xrv;
 
 // Cannot mix different types
 vector[5] yv = pow(xv, xrv)

 // Cannot mix different sizes of the same type
 vector[5] yv = pow(xv, xv2)
\end{verbatim}

While the vectorized binary functions generally require the same input types,
the only exception to this is for binary functions that require one input to be
an integer and the other to be a real (e.g., \texttt{bessel\_first\_kind}). For these
functions, one argument can be a container of any type while the other can be
an integer array, as long as the dimensions of both are the same. For example,
the following statements are legal:

\begin{verbatim}
 vector[5] xv;
 matrix[5, 5] xm;
 int xi[5];
 int xii[5, 5];
 
 vector[5] yv = bessel_first_kind(xi, xv);
 matrix[5, 5] ym = bessel_first_kind(xii, xm);
\end{verbatim}

Whereas these are not:

\begin{verbatim}
 vector[5] xv;
 matrix[5, 5] xm;
 int xi[7];

 // Dimensions of containers do not match
 vector[5] yv = bessel_first_kind(xi, xv);

 // Function requires first argument be an integer type
 matrix[5, 5] ym = bessel_first_kind(xm, xm);
\end{verbatim}

\hypertarget{built-in-constants}{%
\section{Mathematical constants}\label{built-in-constants}}

Constants are represented as functions with no arguments and must be
called as such. For instance, the mathematical constant \(\pi\) must be
written in a Stan program as \texttt{pi()}.

\index{{\tt \bfseries pi }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{pi}}\texttt{()}\newline
\(\pi\), the ratio of a circle's circumference to its diameter

\index{{\tt \bfseries e }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{e}}\texttt{()}\newline
\(e\), the base of the natural logarithm

\index{{\tt \bfseries sqrt2 }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{sqrt2}}\texttt{()}\newline
The square root of 2

\index{{\tt \bfseries log2 }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{log2}}\texttt{()}\newline
The natural logarithm of 2

\index{{\tt \bfseries log10 }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{log10}}\texttt{()}\newline
The natural logarithm of 10

\hypertarget{special-values}{%
\section{Special values}\label{special-values}}

\index{{\tt \bfseries not\_a\_number }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{not\_a\_number}}\texttt{()}\newline
Not-a-number, a special non-finite real value returned to signal an
error

\index{{\tt \bfseries positive\_infinity }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{positive\_infinity}}\texttt{()}\newline
Positive infinity, a special non-finite real value larger than all
finite numbers

\index{{\tt \bfseries negative\_infinity }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{negative\_infinity}}\texttt{()}\newline
Negative infinity, a special non-finite real value smaller than all
finite numbers

\index{{\tt \bfseries machine\_precision }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{machine\_precision}}\texttt{()}\newline
The smallest number \(x\) such that \((x + 1) \neq 1\) in floating-point
arithmetic on the current hardware platform

\hypertarget{get-log-prob}{%
\section{Log probability function}\label{get-log-prob}}

The basic purpose of a Stan program is to compute a log probability
function and its derivatives. The log probability function in a Stan
model outputs the log density on the unconstrained scale. A log
probability accumulator starts at zero and is then incremented in
various ways by a Stan program. The variables are first transformed
from unconstrained to constrained, and the log Jacobian determinant
added to the log probability accumulator. Then the model block is
executed on the constrained parameters, with each sampling statement
(\texttt{\textasciitilde{}}) and log probability increment statement (\texttt{increment\_log\_prob})
adding to the accumulator. At the end of the model block execution,
the value of the log probability accumulator is the log probability
value returned by the Stan program.

Stan provides a special built-in function \texttt{target()} that takes no
arguments and returns the current value of the log probability
accumulator.\footnote{This function used to be called \texttt{get\_lp()}, but that name
  has been deprecated; using it will print a warning. The function
  \texttt{get\_lp()} will be removed in a future release.} This function is primarily useful for debugging
purposes, where for instance, it may be used with a print statement to
display the log probability accumulator at various stages of execution
to see where it becomes ill defined.

\index{{\tt \bfseries target }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{target}}\texttt{()}\newline
Return the current value of the log probability accumulator.

\index{{\tt \bfseries get\_lp }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{get\_lp}}\texttt{()}\newline
Return the current value of the log probability accumulator;
\textbf{deprecated;} - use \texttt{target()} instead.

Both \texttt{target} and the deprecated \texttt{get\_lp} act like other functions
ending in \texttt{\_lp}, meaning that they may only may only be used in the
model block.

\hypertarget{logical-functions}{%
\section{Logical functions}\label{logical-functions}}

Like C++, BUGS, and R, Stan uses 0 to encode false, and 1 to encode
true. Stan supports the usual boolean comparison operations and
boolean operators. These all have the same syntax and precedence as
in C++; for the full list of operators and precedences, see the
reference manual.

\hypertarget{comparison-operators}{%
\subsection{Comparison operators}\label{comparison-operators}}

All comparison operators return boolean values, either 0 or 1. Each
operator has two signatures, one for integer comparisons and one for
floating-point comparisons. Comparing an integer and real value is
carried out by first promoting the integer value.

\index{{\tt \bfseries operator\_logical\_less\_than }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textless{}}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_less\_than }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textless{}}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is less than y and 0 otherwise. \[ \text{operator<}(x,y)
= \begin{cases} 1 & \text{if $x < y$} \\ 0 & \text{otherwise}
\end{cases} \]

\index{{\tt \bfseries operator\_logical\_less\_than\_equal }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textless{}=}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_less\_than\_equal }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textless{}=}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is less than or equal y and 0 otherwise. \[
\text{operator<=}(x,y) = \begin{cases} 1 & \text{if $x \leq y$} \\ 0 &
\text{otherwise} \end{cases} \]

\index{{\tt \bfseries operator\_logical\_greater\_than }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textgreater{}}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_greater\_than }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textgreater{}}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is greater than y and 0 otherwise. \[ \text{operator>} =
\begin{cases} 1 & \text{if $x > y$} \\ 0 & \text{otherwise}
\end{cases} \]

\index{{\tt \bfseries operator\_logical\_greater\_than\_equal }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textgreater{}=}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_greater\_than\_equal }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textgreater{}=}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is greater than or equal to y and 0 otherwise. \[
\text{operator>=} = \begin{cases} 1 & \text{if $x \geq y$} \\ 0 &
\text{otherwise} \end{cases} \]

\index{{\tt \bfseries operator\_logial\_equal }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator==}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logial\_equal }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator==}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is equal to y and 0 otherwise. \[ \text{operator==}(x,y)
= \begin{cases} 1 & \text{if $x = y$} \\ 0 & \text{otherwise}
\end{cases} \]

\index{{\tt \bfseries operator\_logical\_not\_equal }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator!=}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_not\_equal }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator!=}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is not equal to y and 0 otherwise. \[
\text{operator!=}(x,y) = \begin{cases} 1 & \text{if $x \neq y$} \\ 0 &
\text{otherwise} \end{cases} \]

\hypertarget{boolean-operators}{%
\subsection{Boolean operators}\label{boolean-operators}}

Boolean operators return either 0 for false or 1 for true. Inputs may
be any real or integer values, with non-zero values being treated as
true and zero values treated as false. These operators have the usual
precedences, with negation (not) binding the most tightly, conjunction
the next and disjunction the weakest; all of the operators bind more
tightly than the comparisons. Thus an expression such as \texttt{!a\ \&\&\ b} is
interpreted as \texttt{(!a)\ \&\&\ b}, and \texttt{a\ \textless{}\ b\ \textbar{}\textbar{}\ c\ \textgreater{}=\ d\ \&\&\ e\ !=\ f} as \texttt{(a\ \textless{}\ b)\ \textbar{}\textbar{}\ (((c\ \textgreater{}=\ d)\ \&\&\ (e\ !=\ f)))}.

\index{{\tt \bfseries operator\_negation }!{\tt (int x): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator!}}\texttt{(int\ x)}\newline

\index{{\tt \bfseries operator\_negation }!{\tt (real x): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator!}}\texttt{(real\ x)}\newline
Return 1 if x is zero and 0 otherwise. \[ \text{operator!}(x) =
\begin{cases} 0 & \text{if $x \neq 0$} \\ 1 & \text{if $x = 0$}
\end{cases} \]

\index{{\tt \bfseries operator\_logical\_and }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\&\&}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_and }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\&\&}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is unequal to 0 and y is unequal to 0. \[
\mathrm{operator\&\&}(x,y) = \begin{cases} 1 & \text{if $x \neq 0$}
\text{ and } y \neq 0\\ 0 & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries operator\_logical\_or }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textbar{}\textbar{}}}\texttt{(int\ x,\ int\ y)}\newline

\index{{\tt \bfseries operator\_logical\_or }!{\tt (real x, real y): int}|hyperpage}

\texttt{int} \textbf{\texttt{operator\textbar{}\textbar{}}}\texttt{(real\ x,\ real\ y)}\newline
Return 1 if x is unequal to 0 or y is unequal to 0. \[
\text{operator||}(x,y) = \begin{cases} 1 & \text{if $x \neq 0$}
\textrm{ or } y \neq 0\\ 0 & \text{otherwise} \end{cases} \]

\hypertarget{boolean-operator-short-circuiting}{%
\subsubsection{Boolean operator short circuiting}\label{boolean-operator-short-circuiting}}

Like in C++, the boolean operators \texttt{\&\&} and \texttt{\textbar{}\textbar{}} and are implemented
to short circuit directly to a return value after evaluating the first
argument if it is sufficient to resolve the result. In evaluating \texttt{a\ \textbar{}\textbar{}\ b}, if \texttt{a} evaluates to a value other than zero, the expression
returns the value 1 without evaluating the expression \texttt{b}. Similarly,
evaluating \texttt{a\ \&\&\ b} first evaluates \texttt{a}, and if the result is zero,
returns 0 without evaluating \texttt{b}.

\hypertarget{logical-functions-1}{%
\subsection{Logical functions}\label{logical-functions-1}}

The logical functions introduce conditional behavior functionally and
are primarily provided for compatibility with BUGS and JAGS.

\index{{\tt \bfseries step }!{\tt (real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{step}}\texttt{(real\ x)}\newline
Return 1 if x is positive and 0 otherwise. \[ \text{step}(x) =
\begin{cases} 0 & \text{if } x < 0 \\ 1 & \text{otherwise} \end{cases}
\] \emph{\textbf{Warning:}} \texttt{int\_step(0)} and \texttt{int\_step(NaN)} return 0 whereas
\texttt{step(0)} and \texttt{step(NaN)} return 1.

The step function is often used in BUGS to perform conditional
operations. For instance, \texttt{step(a-b)} evaluates to 1 if \texttt{a} is
greater than \texttt{b} and evaluates to 0 otherwise. \texttt{step} is a step-like
functions; see the warning in section \protect\hyperlink{step-functions}{step functions} applied to
expressions dependent on parameters.

\index{{\tt \bfseries is\_inf }!{\tt (real x): int}|hyperpage}

\texttt{int} \textbf{\texttt{is\_inf}}\texttt{(real\ x)}\newline
Return 1 if x is infinite (positive or negative) and 0 otherwise.

\index{{\tt \bfseries is\_nan }!{\tt (real x): int}|hyperpage}

\texttt{int} \textbf{\texttt{is\_nan}}\texttt{(real\ x)}\newline
Return 1 if x is NaN and 0 otherwise.

Care must be taken because both of these indicator functions are
step-like and thus can cause discontinuities in gradients when applied
to parameters; see section \protect\hyperlink{step-functions}{step-like functions} for details.

\hypertarget{real-valued-arithmetic-operators}{%
\section{Real-valued arithmetic operators}\label{real-valued-arithmetic-operators}}

The arithmetic operators are presented using C++ notation. For
instance \texttt{operator+(x,y)} refers to the binary addition operator and
\texttt{operator-(x)} to the unary negation operator. In Stan programs,
these are written using the usual infix and prefix notations as \texttt{x\ +\ y} and \texttt{-x}, respectively.

\hypertarget{binary-infix-operators-1}{%
\subsection{Binary infix operators}\label{binary-infix-operators-1}}

\index{{\tt \bfseries operator\_add }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator+}}\texttt{(real\ x,\ real\ y)}\newline
Return the sum of x and y. \[ (x + y) = \text{operator+}(x,y) = x+y \]

\index{{\tt \bfseries operator\_subtract }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator-}}\texttt{(real\ x,\ real\ y)}\newline
Return the difference between x and y. \[ (x - y) =
\text{operator-}(x,y) = x - y \]

\index{{\tt \bfseries operator\_multiply }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator*}}\texttt{(real\ x,\ real\ y)}\newline
Return the product of x and y. \[ (x * y) = \text{operator*}(x,y) = xy
\]

\index{{\tt \bfseries operator\_divide }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator/}}\texttt{(real\ x,\ real\ y)}\newline
Return the quotient of x and y. \[ (x / y) = \text{operator/}(x,y) =
\frac{x}{y} \]

\index{{\tt \bfseries operator\_pow }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator\^{}}}\texttt{(real\ x,\ real\ y)}\newline
Return x raised to the power of y. \[ (x^\mathrm{\wedge}y) =
\text{operator}^\mathrm{\wedge}(x,y) = x^y \]

\hypertarget{unary-prefix-operators-1}{%
\subsection{Unary prefix operators}\label{unary-prefix-operators-1}}

\index{{\tt \bfseries operator\_subtract }!{\tt (real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator-}}\texttt{(real\ x)}\newline
Return the negation of the subtrahend x. \[ \text{operator-}(x) = (-x)
\]

\index{{\tt \bfseries operator\_add }!{\tt (real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator+}}\texttt{(real\ x)}\newline
Return the value of x. \[ \text{operator+}(x) = x \]

\hypertarget{step-functions}{%
\section{Step-like functions}\label{step-functions}}

\emph{\textbf{Warning:}} \emph{These functions can seriously hinder sampling and
optimization efficiency for gradient-based methods (e.g., NUTS, HMC,
BFGS) if applied to parameters (including transformed parameters and
local variables in the transformed parameters or model block). The
problem is that they break gradients due to discontinuities coupled
with zero gradients elsewhere. They do not hinder sampling when
used in the data, transformed data, or generated quantities blocks.}

\hypertarget{absolute-value-functions}{%
\subsection{Absolute value functions}\label{absolute-value-functions}}

\index{{\tt \bfseries fabs }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{fabs}}\texttt{(T\ x)}\newline
absolute value of x

\index{{\tt \bfseries fdim }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{fdim}}\texttt{(real\ x,\ real\ y)}\newline
Return the positive difference between x and y, which is x - y if x is
greater than y and 0 otherwise; see warning above.
\[ \text{fdim}(x,y) = \begin{cases} x-y &
\text{if } x \geq y \\ 0 & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries fdim }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{fdim}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{fdim} function
\#\#\# Bounds functions

\index{{\tt \bfseries fmin }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{fmin}}\texttt{(real\ x,\ real\ y)}\newline
Return the minimum of x and y; see warning above.
\[ \text{fmin}(x,y) = \begin{cases} x &
\text{if } x \leq y \\ y & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries fmin }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{fmin}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{fmin} function
\index{{\tt \bfseries fmax }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{fmax}}\texttt{(real\ x,\ real\ y)}\newline
Return the maximum of x and y; see warning above.
\[ \text{fmax}(x,y) = \begin{cases} x &
\text{if } x \geq y \\ y & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries fmax }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{fmax}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{fmax} function
\#\#\# Arithmetic functions

\index{{\tt \bfseries fmod }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{fmod}}\texttt{(real\ x,\ real\ y)}\newline
Return the real value remainder after dividing x by y; see warning above.
\[ \text{fmod}(x,y) = x - \left\lfloor \frac{x}{y} \right\rfloor \, y \]
The operator \(\lfloor u \rfloor\) is the floor operation; see below.

\index{{\tt \bfseries fmod }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{fmod}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{fmod} function
\#\#\# Rounding functions

\emph{\textbf{Warning:}} Rounding functions convert real values to integers.
Because the output is an integer, any gradient information resulting
from functions applied to the integer is not passed to the real value
it was derived from. With MCMC sampling using HMC or NUTS, the MCMC
acceptance procedure will correct for any error due to poor gradient
calculations, but the result is likely to be reduced acceptance
probabilities and less efficient sampling.

The rounding functions cannot be used as indices to arrays because
they return real values. Stan may introduce integer-valued versions
of these in the future, but as of now, there is no good workaround.

\index{{\tt \bfseries floor }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{floor}}\texttt{(T\ x)}\newline
floor of x, which is the largest integer less than or equal to x,
converted to a real value; see warning at start of section
\protect\hyperlink{step-functions}{step-like functions}

\index{{\tt \bfseries ceil }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{ceil}}\texttt{(T\ x)}\newline
ceiling of x, which is the smallest integer greater than or equal to
x, converted to a real value; see warning at start of section
\protect\hyperlink{step-functions}{step-like functions}

\index{{\tt \bfseries round }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{round}}\texttt{(T\ x)}\newline
nearest integer to x, converted to a real value; see warning at start
of section \protect\hyperlink{step-functions}{step-like functions}

\index{{\tt \bfseries trunc }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{trunc}}\texttt{(T\ x)}\newline
integer nearest to but no larger in magnitude than x, converted to a
double value; see warning at start of section \protect\hyperlink{step-functions}{step-like functions}

\hypertarget{power-and-logarithm-functions}{%
\section{Power and logarithm functions}\label{power-and-logarithm-functions}}

\index{{\tt \bfseries sqrt }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{sqrt}}\texttt{(T\ x)}\newline
square root of x

\index{{\tt \bfseries cbrt }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{cbrt}}\texttt{(T\ x)}\newline
cube root of x

\index{{\tt \bfseries square }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{square}}\texttt{(T\ x)}\newline
square of x

\index{{\tt \bfseries exp }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{exp}}\texttt{(T\ x)}\newline
natural exponential of x

\index{{\tt \bfseries exp2 }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{exp2}}\texttt{(T\ x)}\newline
base-2 exponential of x

\index{{\tt \bfseries log }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log}}\texttt{(T\ x)}\newline
natural logarithm of x

\index{{\tt \bfseries log2 }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log2}}\texttt{(T\ x)}\newline
base-2 logarithm of x

\index{{\tt \bfseries log10 }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log10}}\texttt{(T\ x)}\newline
base-10 logarithm of x

\index{{\tt \bfseries pow }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{pow}}\texttt{(real\ x,\ real\ y)}\newline
Return x raised to the power of y. \[ \text{pow}(x,y) = x^y \]

\index{{\tt \bfseries pow }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{pow}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{pow} function
\index{{\tt \bfseries inv }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv}}\texttt{(T\ x)}\newline
inverse of x

\index{{\tt \bfseries inv\_sqrt }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_sqrt}}\texttt{(T\ x)}\newline
inverse of the square root of x

\index{{\tt \bfseries inv\_square }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_square}}\texttt{(T\ x)}\newline
inverse of the square of x

\hypertarget{trigonometric-functions}{%
\section{Trigonometric functions}\label{trigonometric-functions}}

\index{{\tt \bfseries hypot }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{hypot}}\texttt{(real\ x,\ real\ y)}\newline
Return the length of the hypotenuse of a right triangle with sides of
length x and y. \[ \text{hypot}(x,y) = \begin{cases} \sqrt{x^2+y^2} &
\text{if } x,y\geq 0 \\ \textrm{NaN} & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries hypot }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{hypot}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{hypot} function
\index{{\tt \bfseries cos }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{cos}}\texttt{(T\ x)}\newline
cosine of the angle x (in radians)

\index{{\tt \bfseries sin }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{sin}}\texttt{(T\ x)}\newline
sine of the angle x (in radians)

\index{{\tt \bfseries tan }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{tan}}\texttt{(T\ x)}\newline
tangent of the angle x (in radians)

\index{{\tt \bfseries acos }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{acos}}\texttt{(T\ x)}\newline
principal arc (inverse) cosine (in radians) of x

\index{{\tt \bfseries asin }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{asin}}\texttt{(T\ x)}\newline
principal arc (inverse) sine (in radians) of x

\index{{\tt \bfseries atan }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{atan}}\texttt{(T\ x)}\newline
principal arc (inverse) tangent (in radians) of x, with values from
\(-\pi\) to \(\pi\)

\index{{\tt \bfseries atan2 }!{\tt (real y, real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{atan2}}\texttt{(real\ y,\ real\ x)}\newline
Return the principal arc (inverse) tangent (in radians) of y divided
by x, \[ \text{atan2}(y, x) = \arctan\left(\frac{y}{x}\right) \]

\hypertarget{hyperbolic-trigonometric-functions}{%
\section{Hyperbolic trigonometric functions}\label{hyperbolic-trigonometric-functions}}

\index{{\tt \bfseries cosh }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{cosh}}\texttt{(T\ x)}\newline
hyperbolic cosine of x (in radians)

\index{{\tt \bfseries sinh }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{sinh}}\texttt{(T\ x)}\newline
hyperbolic sine of x (in radians)

\index{{\tt \bfseries tanh }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{tanh}}\texttt{(T\ x)}\newline
hyperbolic tangent of x (in radians)

\index{{\tt \bfseries acosh }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{acosh}}\texttt{(T\ x)}\newline
inverse hyperbolic cosine (in radians)

\index{{\tt \bfseries asinh }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{asinh}}\texttt{(T\ x)}\newline
inverse hyperbolic cosine (in radians)

\index{{\tt \bfseries atanh }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{atanh}}\texttt{(T\ x)}\newline
inverse hyperbolic tangent (in radians) of x

\hypertarget{link-functions}{%
\section{Link functions}\label{link-functions}}

The following functions are commonly used as link functions in
generalized linear models. The function \(\Phi\) is also commonly used
as a link function (see section \protect\hyperlink{Phi-function}{probability-related functions}).

\index{{\tt \bfseries logit }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{logit}}\texttt{(T\ x)}\newline
log odds, or logit, function applied to x

\index{{\tt \bfseries inv\_logit }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_logit}}\texttt{(T\ x)}\newline
logistic sigmoid function applied to x

\index{{\tt \bfseries inv\_cloglog }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_cloglog}}\texttt{(T\ x)}\newline
inverse of the complementary log-log function applied to x

\hypertarget{Phi-function}{%
\section{Probability-related functions}\label{Phi-function}}

\hypertarget{normal-cumulative-distribution-functions}{%
\subsection{Normal cumulative distribution functions}\label{normal-cumulative-distribution-functions}}

The error function \texttt{erf} is related to the standard normal cumulative
distribution function \(\Phi\) by scaling. See section
\protect\hyperlink{normal-distribution}{normal distribution} for the general normal cumulative
distribution function (and its complement).

\index{{\tt \bfseries erf }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{erf}}\texttt{(T\ x)}\newline
error function, also known as the Gauss error function, of x

\index{{\tt \bfseries erfc }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{erfc}}\texttt{(T\ x)}\newline
complementary error function of x

\index{{\tt \bfseries phi }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{Phi}}\texttt{(T\ x)}\newline
standard normal cumulative distribution function of x

\index{{\tt \bfseries inv\_phi }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_Phi}}\texttt{(T\ x)}\newline
standard normal inverse cumulative distribution function of p,
otherwise known as the quantile function

\index{{\tt \bfseries phi\_approx }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{Phi\_approx}}\texttt{(T\ x)}\newline
fast approximation of the unit (may replace \texttt{Phi} for probit
regression with maximum absolute error of 0.00014, see
(\protect\hyperlink{ref-BowlingEtAl:2009}{Bowling et al. 2009}) for details)

\hypertarget{other-probability-related-functions}{%
\subsection{Other probability-related functions}\label{other-probability-related-functions}}

\index{{\tt \bfseries binary\_log\_loss }!{\tt (int y, real y\_hat): real}|hyperpage}

\texttt{real} \textbf{\texttt{binary\_log\_loss}}\texttt{(int\ y,\ real\ y\_hat)}\newline
Return the log loss function for for predicting \(\hat{y} \in [0,1]\)
for boolean outcome \(y \in \{0,1\}\). \[
\mathrm{binary\_log\_loss}(y,\hat{y}) = \begin{cases} -\log \hat{y} &
\text{if } y = 0\\ -\log (1 - \hat{y}) & \text{otherwise} \end{cases}
\]

\index{{\tt \bfseries binary\_log\_loss }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{binary\_log\_loss}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{binary\_log\_loss} function

\index{{\tt \bfseries owens\_t }!{\tt (real h, real a): real}|hyperpage}

\texttt{real} \textbf{\texttt{owens\_t}}\texttt{(real\ h,\ real\ a)}\newline
Return the Owen's T function for the probability of the event \(X > h\)
and \(0<Y<aX\) where X and Y are independent standard normal random
variables. \[ \mathrm{owens\_t}(h,a) = \frac{1}{2\pi} \int_0^a
\frac{\exp(-\frac{1}{2}h^2(1+x^2))}{1+x^2}dx \]

\index{{\tt \bfseries owens\_t }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{owens\_t}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{owens\_t} function

\hypertarget{betafun}{%
\section{Combinatorial functions}\label{betafun}}

\index{{\tt \bfseries beta }!{\tt (real alpha, real beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta}}\texttt{(real\ alpha,\ real\ beta)}\newline
Return the beta function applied to alpha and beta. The beta function,
\(\text{B}(\alpha,\beta)\), computes the normalizing constant for the beta
distribution, and is defined for \(\alpha > 0\) and \(\beta > 0\). See section
\protect\hyperlink{beta-appendix}{appendix} for definition of \(\text{B}(\alpha, \beta)\).

\index{{\tt \bfseries beta }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{beta}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{beta} function

\index{{\tt \bfseries inc\_beta }!{\tt (real alpha, real beta, real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{inc\_beta}}\texttt{(real\ alpha,\ real\ beta,\ real\ x)}\newline
Return the regularized incomplete beta function up to x applied to alpha and beta.
See section \protect\hyperlink{inc-beta-appendix}{appendix} for a definition.

\index{{\tt \bfseries lbeta }!{\tt (real alpha, real beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{lbeta}}\texttt{(real\ alpha,\ real\ beta)}\newline
Return the natural logarithm of the beta function applied to alpha and
beta. The beta function, \(\text{B}(\alpha,\beta)\), computes the
normalizing constant for the beta distribution, and is defined for
\(\alpha > 0\) and \(\beta > 0\). \[ \text{lbeta}(\alpha,\beta) = \log
\Gamma(a) + \log \Gamma(b) - \log \Gamma(a+b) \] See section
\protect\hyperlink{beta-appendix}{appendix} for definition of \(\text{B}(\alpha, \beta)\).

\index{{\tt \bfseries lbeta }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{lbeta}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{lbeta} function

\index{{\tt \bfseries tgamma }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{tgamma}}\texttt{(T\ x)}\newline
gamma function applied to x. The gamma function is the generalization
of the factorial function to continuous variables, defined so that
\(\Gamma(n+1) = n!\). See for a full definition of \(\Gamma(x)\). The
function is defined for positive numbers and non-integral negative
numbers,

\index{{\tt \bfseries lgamma }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{lgamma}}\texttt{(T\ x)}\newline
natural logarithm of the gamma function applied to x,

\index{{\tt \bfseries digamma }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{digamma}}\texttt{(T\ x)}\newline
digamma function applied to x. The digamma function is the derivative
of the natural logarithm of the Gamma function. The function is
defined for positive numbers and non-integral negative numbers

\index{{\tt \bfseries trigamma }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{trigamma}}\texttt{(T\ x)}\newline
trigamma function applied to x. The trigamma function is the second
derivative of the natural logarithm of the Gamma function

\index{{\tt \bfseries lmgamma }!{\tt (int n, real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{lmgamma}}\texttt{(int\ n,\ real\ x)}\newline
Return the natural logarithm of the multivariate gamma function
\(\Gamma_n\) with n dimensions applied to x. \[ \text{lmgamma}(n,x) =
\begin{cases} \frac{n(n-1)}{4} \log \pi + \sum_{j=1}^n \log
\Gamma\left(x + \frac{1 - j}{2}\right) & \text{if } x\not\in
\{\dots,-3,-2,-1,0\}\\ \textrm{error} & \text{otherwise} \end{cases}
\]

\index{{\tt \bfseries lmgamma }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{lmgamma}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{lmgamma} function

\index{{\tt \bfseries gamma\_p }!{\tt (real a, real z): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_p}}\texttt{(real\ a,\ real\ z)}\newline
Return the normalized lower incomplete gamma function of a and z
defined for positive a and nonnegative z. \[ \mathrm{gamma\_p}(a,z) =
\begin{cases} \frac{1}{\Gamma(a)}\int_0^zt^{a-1}e^{-t}dt & \text{if }
a > 0, z \geq 0 \\ \textrm{error} & \text{otherwise} \end{cases} \]

\index{{\tt \bfseries gamma\_p }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{gamma\_p}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{gamma\_p} function

\index{{\tt \bfseries gamma\_q }!{\tt (real a, real z): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_q}}\texttt{(real\ a,\ real\ z)}\newline
Return the normalized upper incomplete gamma function of a and z
defined for positive a and nonnegative z. \[ \mathrm{gamma\_q}(a,z) =
\begin{cases} \frac{1}{\Gamma(a)}\int_z^\infty t^{a-1}e^{-t}dt &
\text{if } a > 0, z \geq 0 \\[6pt] \textrm{error} & \text{otherwise}
\end{cases} \]

\index{{\tt \bfseries gamma\_q }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{gamma\_q}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{gamma\_q} function

\index{{\tt \bfseries binomial\_coefficient\_log }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_coefficient\_log}}\texttt{(real\ x,\ real\ y)}\newline
\emph{\textbf{Warning:}} This function is deprecated and should be replaced with
\texttt{lchoose}. Return the natural logarithm of the binomial coefficient of
x and y. For non-negative integer inputs, the binomial coefficient
function is written as \(\binom{x}{y}\) and pronounced ``x choose y.''
This function generalizes to real numbers using the gamma function.
For \(0 \leq y \leq x\), \[ \mathrm{binomial\_coefficient\_log}(x,y) =
\log\Gamma(x+1) - \log\Gamma(y+1) - \log\Gamma(x-y+1). \]

\index{{\tt \bfseries binomial\_coefficient\_log }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{binomial\_coefficient\_log}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{binomial\_coefficient\_log} function

\index{{\tt \bfseries choose }!{\tt (int x, int y): int}|hyperpage}

\texttt{int} \textbf{\texttt{choose}}\texttt{(int\ x,\ int\ y)}\newline
Return the binomial coefficient of x and y. For non-negative integer
inputs, the binomial coefficient function is written as \(\binom{x}{y}\)
and pronounced ``x choose y.'' In its the antilog of the \texttt{lchoose}
function but returns an integer rather than a real number with no
non-zero decimal places. For \(0 \leq y \leq x\), the binomial
coefficient function can be defined via the factorial function \[
\text{choose}(x,y) = \frac{x!}{\left(y!\right)\left(x - y\right)!}. \]

\index{{\tt \bfseries choose }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{choose}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{choose} function

\index{{\tt \bfseries bessel\_first\_kind }!{\tt (int v, real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{bessel\_first\_kind}}\texttt{(int\ v,\ real\ x)}\newline
Return the Bessel function of the first kind with order v applied to
x. \[ \mathrm{bessel\_first\_kind}(v,x) = J_v(x), \] where \[
J_v(x)=\left(\frac{1}{2}x\right)^v \sum_{k=0}^\infty
\frac{\left(-\frac{1}{4}x^2\right)^k}{k!\, \Gamma(v+k+1)} \]

\index{{\tt \bfseries bessel\_first\_kind }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{bessel\_first\_kind}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{bessel\_first\_kind} function

\index{{\tt \bfseries bessel\_second\_kind }!{\tt (int v, real x): real}|hyperpage}

\texttt{real} \textbf{\texttt{bessel\_second\_kind}}\texttt{(int\ v,\ real\ x)}\newline
Return the Bessel function of the second kind with order v applied to
x defined for positive x and v. For \(x,v > 0\), \[
\mathrm{bessel\_second\_kind}(v,x) = \begin{cases} Y_v(x) & \text{if }
x > 0 \\ \textrm{error} & \text{otherwise} \end{cases} \] where \[
Y_v(x)=\frac{J_v(x)\cos(v\pi)-J_{-v}(x)}{\sin(v\pi)} \]

\index{{\tt \bfseries bessel\_second\_kind }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{bessel\_second\_kind}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{bessel\_second\_kind} function

\index{{\tt \bfseries modified\_bessel\_first\_kind }!{\tt (int v, real z): real}|hyperpage}

\texttt{real} \textbf{\texttt{modified\_bessel\_first\_kind}}\texttt{(int\ v,\ real\ z)}\newline
Return the modified Bessel function of the first kind with order v
applied to z defined for all z and integer v. \[
\mathrm{modified\_bessel\_first\_kind}(v,z) = I_v(z) \] where \[
{I_v}(z) = \left(\frac{1}{2}z\right)^v\sum_{k=0}^\infty
\frac{\left(\frac{1}{4}z^2\right)^k}{k!\Gamma(v+k+1)} \]

\index{{\tt \bfseries modified\_bessel\_first\_kind }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{modified\_bessel\_first\_kind}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{modified\_bessel\_first\_kind} function

\index{{\tt \bfseries log\_modified\_bessel\_first\_kind }!{\tt (real v, real z): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_modified\_bessel\_first\_kind}}\texttt{(real\ v,\ real\ z)}\newline
Return the log of the modified Bessel function of the first kind. v does
not have to be an integer.

\index{{\tt \bfseries log\_modified\_bessel\_first\_kind }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{log\_modified\_bessel\_first\_kind}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{log\_modified\_bessel\_first\_kind} function

\index{{\tt \bfseries modified\_bessel\_second\_kind }!{\tt (int v, real z): real}|hyperpage}

\texttt{real} \textbf{\texttt{modified\_bessel\_second\_kind}}\texttt{(int\ v,\ real\ z)}\newline
Return the modified Bessel function of the second kind with order v
applied to z defined for positive z and integer v. \[
\mathrm{modified\_bessel\_second\_kind}(v,z) = \begin{cases} K_v(z) &
\text{if } z > 0 \\ \textrm{error} & \text{if } z \leq 0 \end{cases}
\] where \[ {K_v}(z) = \frac{\pi}{2}\cdot\frac{I_{-v}(z) -
I_{v}(z)}{\sin(v\pi)} \]

\index{{\tt \bfseries modified\_bessel\_second\_kind }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{modified\_bessel\_second\_kind}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{modified\_bessel\_second\_kind} function

\index{{\tt \bfseries falling\_factorial }!{\tt (real x, real n): real}|hyperpage}

\texttt{real} \textbf{\texttt{falling\_factorial}}\texttt{(real\ x,\ real\ n)}\newline
Return the falling factorial of x with power n defined for positive x
and real n.~\[ \mathrm{falling\_factorial}(x,n) = \begin{cases} (x)_n
& \text{if } x > 0 \\ \textrm{error} & \text{if } x \leq 0 \end{cases}
\] where \[ (x)_n=\frac{\Gamma(x+1)}{\Gamma(x-n+1)} \]

\index{{\tt \bfseries falling\_factorial }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{falling\_factorial}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{falling\_factorial} function

\index{{\tt \bfseries lchoose }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{lchoose}}\texttt{(real\ x,\ real\ y)}\newline
Return the natural logarithm of the generalized binomial coefficient
of x and y. For non-negative integer inputs, the binomial coefficient
function is written as \(\binom{x}{y}\) and pronounced ``x choose y.''
This function generalizes to real numbers using the gamma function.
For \(0 \leq y \leq x\), \[ \mathrm{binomial\_coefficient\_log}(x,y) =
\log\Gamma(x+1) - \log\Gamma(y+1) - \log\Gamma(x-y+1). \]

\index{{\tt \bfseries log\_falling\_factorial }!{\tt (real x, real n): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_falling\_factorial}}\texttt{(real\ x,\ real\ n)}\newline
Return the log of the falling factorial of x with power n defined for
positive x and real n.~\[ \mathrm{log\_falling\_factorial}(x,n) =
\begin{cases} \log (x)_n & \text{if } x > 0 \\ \textrm{error} &
\text{if } x \leq 0 \end{cases} \]

\index{{\tt \bfseries rising\_factorial }!{\tt (real x, int n): real}|hyperpage}

\texttt{real} \textbf{\texttt{rising\_factorial}}\texttt{(real\ x,\ int\ n)}\newline
Return the rising factorial of x with power n defined for positive x
and integer n.~\[ \mathrm{rising\_factorial}(x,n) = \begin{cases} x^{(n)}
& \text{if } x > 0 \\ \textrm{error} & \text{if } x \leq 0 \end{cases}
\] where \[ x^{(n)}=\frac{\Gamma(x+n)}{\Gamma(x)} \]

\index{{\tt \bfseries rising\_factorial }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{rising\_factorial}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{rising\_factorial} function

\index{{\tt \bfseries log\_rising\_factorial }!{\tt (real x, real n): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_rising\_factorial}}\texttt{(real\ x,\ real\ n)}\newline
Return the log of the rising factorial of x with power n defined for
positive x and real n.~\[ \mathrm{log\_rising\_factorial}(x,n) =
\begin{cases} \log x^{(n)} & \text{if } x > 0 \\ \textrm{error} &
\text{if } x \leq 0 \end{cases} \]

\index{{\tt \bfseries log\_rising\_factorial }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{log\_rising\_factorial}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{log\_rising\_factorial} function

\hypertarget{composed-functions}{%
\section{Composed functions}\label{composed-functions}}

The functions in this section are equivalent in theory to combinations
of other functions. In practice, they are implemented to be more
efficient and more numerically stable than defining them directly
using more basic Stan functions.

\index{{\tt \bfseries expm1 }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{expm1}}\texttt{(T\ x)}\newline
natural exponential of x minus 1

\index{{\tt \bfseries fma }!{\tt (real x, real y, real z): real}|hyperpage}

\texttt{real} \textbf{\texttt{fma}}\texttt{(real\ x,\ real\ y,\ real\ z)}\newline
Return z plus the result of x multiplied by y. \[ \text{fma}(x,y,z) =
(x \times y) + z \]

\index{{\tt \bfseries multiply\_log }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{multiply\_log}}\texttt{(real\ x,\ real\ y)}\newline
\emph{\textbf{Warning:}} This function is deprecated and should be replaced with
\texttt{lmultiply}. Return the product of x and the natural logarithm of y.
\[ \mathrm{multiply\_log}(x,y) = \begin{cases} 0 & \text{if } x = y =
0 \\ x \log y & \text{if } x, y \neq 0 \\ \text{NaN} &
\text{otherwise} \end{cases} \]

\index{{\tt \bfseries multiply\_log }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{multiply\_log}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{multiply\_log} function

\index{{\tt \bfseries ldexp }!{\tt (real x, int y): real}|hyperpage}

\texttt{real} \textbf{\texttt{ldexp}}\texttt{(real\ x,\ int\ y)}\newline
Return the product of x and two raised to the y power. \[
\text{ldexp}(x,y) = x 2^y  \]

\index{{\tt \bfseries ldexp }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{ldexp}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{ldexp} function

\index{{\tt \bfseries lmultiply }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{lmultiply}}\texttt{(real\ x,\ real\ y)}\newline
Return the product of x and the natural logarithm of y. \[
\text{lmultiply}(x,y) = \begin{cases} 0 & \text{if } x = y = 0 \\ x
\log y & \text{if } x, y \neq 0 \\ \text{NaN} & \text{otherwise}
\end{cases} \]

\index{{\tt \bfseries lmultiply }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{lmultiply}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{lmultiply} function

\index{{\tt \bfseries log1p }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log1p}}\texttt{(T\ x)}\newline
natural logarithm of 1 plus x

\index{{\tt \bfseries log1m }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log1m}}\texttt{(T\ x)}\newline
natural logarithm of 1 minus x

\index{{\tt \bfseries log1p\_exp }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log1p\_exp}}\texttt{(T\ x)}\newline
natural logarithm of one plus the natural exponentiation of x

\index{{\tt \bfseries log1m\_exp }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log1m\_exp}}\texttt{(T\ x)}\newline
logarithm of one minus the natural exponentiation of x

\index{{\tt \bfseries log\_diff\_exp }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_diff\_exp}}\texttt{(real\ x,\ real\ y)}\newline
Return the natural logarithm of the difference of the natural
exponentiation of x and the natural exponentiation of y. \[
\mathrm{log\_diff\_exp}(x,y) = \begin{cases} \log(\exp(x)-\exp(y)) &
\text{if } x > y \\[6pt] \textrm{NaN} & \text{otherwise} \end{cases}
\]

\index{{\tt \bfseries log\_diff\_exp }!{\tt (T1 x, T2 y): R}|hyperpage}

\texttt{R} \textbf{\texttt{log\_diff\_exp}}\texttt{(T1\ x,\ T2\ y)}\newline
Vectorized implementation of the \texttt{log\_diff\_exp} function

\index{{\tt \bfseries log\_mix }!{\tt (real theta, real lp1, real lp2): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_mix}}\texttt{(real\ theta,\ real\ lp1,\ real\ lp2)}\newline
Return the log mixture of the log densities lp1 and lp2 with mixing
proportion theta, defined by \begin{eqnarray*}
\mathrm{log\_mix}(\theta, \lambda_1, \lambda_2) & = & \log \!\left(
\theta \exp(\lambda_1) + \left( 1 - \theta \right) \exp(\lambda_2)
\right) \\[3pt] & = & \mathrm{log\_sum\_exp}\!\left(\log(\theta) +
\lambda_1, \ \log(1 - \theta) + \lambda_2\right). \end{eqnarray*}

\index{{\tt \bfseries log\_sum\_exp }!{\tt (real x, real y): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_sum\_exp}}\texttt{(real\ x,\ real\ y)}\newline
Return the natural logarithm of the sum of the natural exponentiation
of x and the natural exponentiation of y. \[
\mathrm{log\_sum\_exp}(x,y) = \log(\exp(x)+\exp(y)) \]

\index{{\tt \bfseries log\_inv\_logit }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log\_inv\_logit}}\texttt{(T\ x)}\newline
natural logarithm of the inverse logit function of x

\index{{\tt \bfseries log1m\_inv\_logit }!{\tt (T x): R}|hyperpage}

\texttt{R} \textbf{\texttt{log1m\_inv\_logit}}\texttt{(T\ x)}\newline
natural logarithm of 1 minus the inverse logit function of x

\hypertarget{array-operations}{%
\chapter{Array Operations}\label{array-operations}}

\hypertarget{array-reductions}{%
\section{Reductions}\label{array-reductions}}

The following operations take arrays as input and produce single
output values. The boundary values for size 0 arrays are the unit
with respect to the combination operation (min, max, sum, or product).

\hypertarget{minimum-and-maximum}{%
\subsection{Minimum and maximum}\label{minimum-and-maximum}}

\index{{\tt \bfseries min }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{min}}\texttt{(real{[}{]}\ x)}\newline
The minimum value in x, or \(+\infty\) if x is size 0.

\index{{\tt \bfseries min }!{\tt (int[] x): int}|hyperpage}

\texttt{int} \textbf{\texttt{min}}\texttt{(int{[}{]}\ x)}\newline
The minimum value in x, or error if x is size 0.

\index{{\tt \bfseries max }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{max}}\texttt{(real{[}{]}\ x)}\newline
The maximum value in x, or \(-\infty\) if x is size 0.

\index{{\tt \bfseries max }!{\tt (int[] x): int}|hyperpage}

\texttt{int} \textbf{\texttt{max}}\texttt{(int{[}{]}\ x)}\newline
The maximum value in x, or error if x is size 0.

\hypertarget{sum-product-and-log-sum-of-exp}{%
\subsection{Sum, product, and log sum of exp}\label{sum-product-and-log-sum-of-exp}}

\index{{\tt \bfseries sum }!{\tt (int[] x): int}|hyperpage}

\texttt{int} \textbf{\texttt{sum}}\texttt{(int{[}{]}\ x)}\newline
The sum of the elements in x, defined for \(x\) of size \(N\) by \[
\text{sum}(x) = \begin{cases} \sum_{n=1}^N x_n & \text{if} N > 0
\\[4pt] 0 & \text{if} N = 0 \end{cases} \]

\index{{\tt \bfseries sum }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sum}}\texttt{(real{[}{]}\ x)}\newline
The sum of the elements in x; see definition above.

\index{{\tt \bfseries prod }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{prod}}\texttt{(real{[}{]}\ x)}\newline
The product of the elements in x, or 1 if x is size 0.

\index{{\tt \bfseries prod }!{\tt (int[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{prod}}\texttt{(int{[}{]}\ x)}\newline
The product of the elements in x, \[ \text{product}(x) = \begin{cases}
\prod_{n=1}^N x_n & \text{if} N > 0 \\[4pt] 1 & \text{if} N = 0
\end{cases} \]

\index{{\tt \bfseries log\_sum\_exp }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_sum\_exp}}\texttt{(real{[}{]}\ x)}\newline
The natural logarithm of the sum of the exponentials of the elements
in x, or \(-\infty\) if the array is empty.

\hypertarget{sample-mean-variance-and-standard-deviation}{%
\subsection{Sample mean, variance, and standard deviation}\label{sample-mean-variance-and-standard-deviation}}

The sample mean, variance, and standard deviation are calculated in
the usual way. For i.i.d. draws from a distribution of finite mean,
the sample mean is an unbiased estimate of the mean of the
distribution. Similarly, for i.i.d. draws from a distribution of
finite variance, the sample variance is an unbiased estimate of the
variance.\footnote{Dividing by \(N\) rather than \((N-1)\) produces a maximum
  likelihood estimate of variance, which is biased to underestimate
  variance.} The sample deviation is defined as the square root
of the sample deviation, but is not unbiased.

\index{{\tt \bfseries mean }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{mean}}\texttt{(real{[}{]}\ x)}\newline
The sample mean of the elements in x. For an array \(x\) of size \(N > 0\), \[ \text{mean}(x) \ = \ \bar{x} \ = \ \frac{1}{N} \sum_{n=1}^N
x_n. \] It is an error to the call the mean function with an array of
size \(0\).

\index{{\tt \bfseries variance }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{variance}}\texttt{(real{[}{]}\ x)}\newline
The sample variance of the elements in x. For \(N > 0\), \[
\text{variance}(x) \ = \ \begin{cases} \frac{1}{N-1} \sum_{n=1}^N (x_n
- \bar{x})^2 & \text{if } N > 1 \\[4pt] 0 & \text{if } N = 1
\end{cases} \] It is an error to call the \texttt{variance} function with an
array of size 0.

\index{{\tt \bfseries sd }!{\tt (real[] x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sd}}\texttt{(real{[}{]}\ x)}\newline
The sample standard deviation of elements in x. \[ \text{sd}(x) =
\begin{cases} \sqrt{\, \text{variance}(x)} & \text{if } N > 1 \\[4pt]
0 & \text{if } N = 0 \end{cases} \] It is an error to call the \texttt{sd}
function with an array of size 0.

\hypertarget{euclidean-distance-and-squared-distance}{%
\subsection{Euclidean distance and squared distance}\label{euclidean-distance-and-squared-distance}}

\index{{\tt \bfseries distance }!{\tt (vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{distance}}\texttt{(vector\ x,\ vector\ y)}\newline
The Euclidean distance between x and y, defined by \[
\text{distance}(x,y) \ = \ \sqrt{\textstyle \sum_{n=1}^N (x_n -
y_n)^2} \] where \texttt{N} is the size of x and y. It is an error to call
\texttt{distance} with arguments of unequal size.

\index{{\tt \bfseries distance }!{\tt (vector x, row\_vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{distance}}\texttt{(vector\ x,\ row\_vector\ y)}\newline
The Euclidean distance between x and y

\index{{\tt \bfseries distance }!{\tt (row\_vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{distance}}\texttt{(row\_vector\ x,\ vector\ y)}\newline
The Euclidean distance between x and y

\index{{\tt \bfseries distance }!{\tt (row\_vector x, row\_vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{distance}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The Euclidean distance between x and y

\index{{\tt \bfseries squared\_distance }!{\tt (vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{squared\_distance}}\texttt{(vector\ x,\ vector\ y)}\newline
The squared Euclidean distance between x and y, defined by \[
\mathrm{squared\_distance}(x,y) \ = \ \text{distance}(x,y)^2 \ = \
\textstyle \sum_{n=1}^N (x_n - y_n)^2, \] where \texttt{N} is the size of x
and y. It is an error to call \texttt{squared\_distance} with arguments of
unequal size.

\index{{\tt \bfseries squared\_distance }!{\tt (vector x, row\_vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{squared\_distance}}\texttt{(vector\ x,\ row\_vector\ y)}\newline
The squared Euclidean distance between x and y

\index{{\tt \bfseries squared\_distance }!{\tt (row\_vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{squared\_distance}}\texttt{(row\_vector\ x,\ vector\ y)}\newline
The squared Euclidean distance between x and y

\index{{\tt \bfseries squared\_distance }!{\tt (row\_vector x, row\_vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{squared\_distance}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The Euclidean distance between x and y

\hypertarget{array-size-and-dimension-function}{%
\section{Array size and dimension function}\label{array-size-and-dimension-function}}

The size of an array or matrix can be obtained using the \texttt{dims()}
function. The \texttt{dims()} function is defined to take an argument
consisting of any variable with up to 8 array dimensions (and up to 2
additional matrix dimensions) and returns an array of integers with
the dimensions. For example, if two variables are declared as
follows,

\begin{verbatim}
 real x[7,8,9];
 matrix[8,9] y[7];
\end{verbatim}

then calling \texttt{dims(x)} or \texttt{dims(y)} returns an integer array of size 3
containing the elements 7, 8, and 9 in that order.

The \texttt{size()} function extracts the number of elements in an array.
This is just the top-level elements, so if the array is declared as

\begin{verbatim}
 real a[M,N];
\end{verbatim}

the size of \texttt{a} is \texttt{M}.

The function \texttt{num\_elements}, on the other hand, measures all of the
elements, so that the array \texttt{a} above has \(M \times N\) elements.

The specialized functions \texttt{rows()} and \texttt{cols()} should be used to
extract the dimensions of vectors and matrices.

\index{{\tt \bfseries dims }!{\tt (T x): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{dims}}\texttt{(T\ x)}\newline
Return an integer array containing the dimensions of x; the type of
the argument T can be any Stan type with up to 8 array dimensions.

\index{{\tt \bfseries num\_elements }!{\tt (T[] x): int}|hyperpage}

\texttt{int} \textbf{\texttt{num\_elements}}\texttt{(T{[}{]}\ x)}\newline
Return the total number of elements in the array x including all
elements in contained arrays, vectors, and matrices. T can be any
array type. For example, if \texttt{x} is of type \texttt{real{[}4,3{]}} then
\texttt{num\_elements(x)} is 12, and if \texttt{y} is declared as \texttt{matrix{[}3,4{]}\ y{[}5{]}},
then \texttt{size(y)} evaluates to 60.

\index{{\tt \bfseries size }!{\tt (T[] x): int}|hyperpage}

\texttt{int} \textbf{\texttt{size}}\texttt{(T{[}{]}\ x)}\newline
Return the number of elements in the array x; the type of the array T
can be any type, but the size is just the size of the top level array,
not the total number of elements contained. For example, if \texttt{x} is of
type \texttt{real{[}4,3{]}} then \texttt{size(x)} is 4.

\hypertarget{array-broadcasting}{%
\section{Array broadcasting}\label{array-broadcasting}}

The following operations create arrays by repeating elements to fill
an array of a specified size. These operations work for all input
types T, including reals, integers, vectors, row vectors, matrices, or
arrays.

\index{{\tt \bfseries rep\_array }!{\tt (T x, int n): T[]}|hyperpage}

\texttt{T{[}{]}} \textbf{\texttt{rep\_array}}\texttt{(T\ x,\ int\ n)}\newline
Return the n array with every entry assigned to x.

\index{{\tt \bfseries rep\_array }!{\tt (T x, int m, int n): T[,]}|hyperpage}

\texttt{T{[},{]}} \textbf{\texttt{rep\_array}}\texttt{(T\ x,\ int\ m,\ int\ n)}\newline
Return the m by n array with every entry assigned to x.

\index{{\tt \bfseries rep\_array }!{\tt (T x, int k, int m, int n): T[,,]}|hyperpage}

\texttt{T{[},,{]}} \textbf{\texttt{rep\_array}}\texttt{(T\ x,\ int\ k,\ int\ m,\ int\ n)}\newline
Return the k by m by n array with every entry assigned to x.

For example, \texttt{rep\_array(1.0,5)} produces a real array (type \texttt{real{[}{]}})
of size 5 with all values set to 1.0. On the other hand,
\texttt{rep\_array(1,5)} produces an integer array (type \texttt{int{[}{]}}) of size 5
with all values set to 1. This distinction is important because it is
not possible to assign an integer array to a real array. For example,
the following example contrasts legal with illegal array creation and
assignment

\begin{verbatim}
 real y[5];
 int x[5];
 
 x = rep_array(1,5);     // ok
 y = rep_array(1.0,5);   // ok
 
 x = rep_array(1.0,5);   // illegal
 y = rep_array(1,5);     // illegal
 
 x = y;                  // illegal
 y = x;                  // illegal
\end{verbatim}

If the value being repeated \texttt{v} is a vector (i.e., \texttt{T} is \texttt{vector}),
then \texttt{rep\_array(v,27)} is a size 27 array consisting of 27 copies of
the vector \texttt{v}.

\begin{verbatim}
 vector[5] v;
 vector[5] a[3];
 
 a = rep_array(v,3);  // fill a with copies of v
 a[2,4] = 9.0;        // v[4], a[1,4], a[2,4] unchanged
\end{verbatim}

If the type T of x is itself an array type, then the result will be an
array with one, two, or three added dimensions, depending on which of
the \texttt{rep\_array} functions is called. For instance, consider the
following legal code snippet.

\begin{verbatim}
 real a[5,6];
 real b[3,4,5,6];
 
 b = rep_array(a,3,4); //  make (3 x 4) copies of a
 b[1,1,1,1] = 27.9;    //  a[1,1] unchanged
\end{verbatim}

After the assignment to \texttt{b}, the value for \texttt{b{[}j,k,m,n{]}} is equal to
\texttt{a{[}m,n{]}} where it is defined, for \texttt{j} in \texttt{1:3}, \texttt{k} in \texttt{1:4}, \texttt{m} in
\texttt{1:5}, and \texttt{n} in \texttt{1:6}.

\hypertarget{array-concatenation}{%
\section{Array concatenation}\label{array-concatenation}}

\index{{\tt \bfseries append\_array }!{\tt (T x, T y): T}|hyperpage}

\texttt{T} \textbf{\texttt{append\_array}}\texttt{(T\ x,\ T\ y)}\newline
Return the concatenation of two arrays in the order of the arguments.
T must be an N-dimensional array of any Stan type (with a maximum N of
7). All dimensions but the first must match.

For example, the following code appends two three dimensional arrays
of matrices together. Note that all dimensions except the first match.
Any mismatches will cause an error to be thrown.

\begin{verbatim}
 matrix[4, 6] x1[2, 1, 7];
 matrix[4, 6] x2[3, 1, 7];
 matrix[4, 6] x3[5, 1, 7];
 
 x3 = append_array(x1, x2);
\end{verbatim}

\hypertarget{sorting-functions}{%
\section{Sorting functions}\label{sorting-functions}}

Sorting can be used to sort values or the indices of those values in
either ascending or descending order. For example, if \texttt{v} is declared
as a real array of size 3, with values \[ \text{v} = (1, -10.3,
20.987), \] then the various sort routines produce \begin{eqnarray*}
\mathrm{sort\_asc(v)} & = &  (-10.3,1,20.987) \\[4pt]
\mathrm{sort\_desc(v)} & = &  (20.987,1,-10.3) \\[4pt]
\mathrm{sort\_indices\_asc(v)} & = &  (2,1,3) \\[4pt]
\text{sort\_indices\_desc(v)} & = &  (3,1,2) \end{eqnarray*}

\index{{\tt \bfseries sort\_asc }!{\tt (real[] v): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{sort\_asc}}\texttt{(real{[}{]}\ v)}\newline
Sort the elements of v in ascending order

\index{{\tt \bfseries sort\_asc }!{\tt (int[] v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_asc}}\texttt{(int{[}{]}\ v)}\newline
Sort the elements of v in ascending order

\index{{\tt \bfseries sort\_desc }!{\tt (real[] v): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{sort\_desc}}\texttt{(real{[}{]}\ v)}\newline
Sort the elements of v in descending order

\index{{\tt \bfseries sort\_desc }!{\tt (int[] v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_desc}}\texttt{(int{[}{]}\ v)}\newline
Sort the elements of v in descending order

\index{{\tt \bfseries sort\_indices\_asc }!{\tt (real[] v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_asc}}\texttt{(real{[}{]}\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in ascending order.

\index{{\tt \bfseries sort\_indices\_asc }!{\tt (int[] v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_asc}}\texttt{(int{[}{]}\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in ascending order.

\index{{\tt \bfseries sort\_indices\_desc }!{\tt (real[] v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_desc}}\texttt{(real{[}{]}\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in descending order.

\index{{\tt \bfseries sort\_indices\_desc }!{\tt (int[] v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_desc}}\texttt{(int{[}{]}\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in descending order.

\index{{\tt \bfseries rank }!{\tt (real[] v, int s): int}|hyperpage}

\texttt{int} \textbf{\texttt{rank}}\texttt{(real{[}{]}\ v,\ int\ s)}\newline
Number of components of v less than v{[}s{]}

\index{{\tt \bfseries rank }!{\tt (int[] v, int s): int}|hyperpage}

\texttt{int} \textbf{\texttt{rank}}\texttt{(int{[}{]}\ v,\ int\ s)}\newline
Number of components of v less than v{[}s{]}

\hypertarget{reversing-functions}{%
\section{Reversing functions}\label{reversing-functions}}

Stan provides functions to create a new array by reversing the order of
elements in an existing array. For example, if \texttt{v} is declared as a real
array of size 3, with values
\[ \text{v} = (1,\, -10.3,\, 20.987), \] then
\[ \mathrm{reverse(v)} = (20.987,\, -10.3,\, 1). \]

\index{{\tt \bfseries reverse }!{\tt (T[] v): T[]}|hyperpage}

\texttt{T{[}{]}} \textbf{\texttt{reverse}}\texttt{(T{[}{]}\ v)}\newline
Return a new array containing the elements of the argument in reverse order.

\hypertarget{matrix-operations}{%
\chapter{Matrix Operations}\label{matrix-operations}}

\hypertarget{integer-valued-matrix-size-functions}{%
\section{Integer-valued matrix size functions}\label{integer-valued-matrix-size-functions}}

\index{{\tt \bfseries num\_elements }!{\tt (vector x): int}|hyperpage}

\texttt{int} \textbf{\texttt{num\_elements}}\texttt{(vector\ x)}\newline
The total number of elements in the vector x (same as function \texttt{rows})

\index{{\tt \bfseries num\_elements }!{\tt (row\_vector x): int}|hyperpage}

\texttt{int} \textbf{\texttt{num\_elements}}\texttt{(row\_vector\ x)}\newline
The total number of elements in the vector x (same as function \texttt{cols})

\index{{\tt \bfseries num\_elements }!{\tt (matrix x): int}|hyperpage}

\texttt{int} \textbf{\texttt{num\_elements}}\texttt{(matrix\ x)}\newline
The total number of elements in the matrix x. For example, if \texttt{x} is a
\(5 \times 3\) matrix, then \texttt{num\_elements(x)} is 15

\index{{\tt \bfseries rows }!{\tt (vector x): int}|hyperpage}

\texttt{int} \textbf{\texttt{rows}}\texttt{(vector\ x)}\newline
The number of rows in the vector x

\index{{\tt \bfseries rows }!{\tt (row\_vector x): int}|hyperpage}

\texttt{int} \textbf{\texttt{rows}}\texttt{(row\_vector\ x)}\newline
The number of rows in the row vector x, namely 1

\index{{\tt \bfseries rows }!{\tt (matrix x): int}|hyperpage}

\texttt{int} \textbf{\texttt{rows}}\texttt{(matrix\ x)}\newline
The number of rows in the matrix x

\index{{\tt \bfseries cols }!{\tt (vector x): int}|hyperpage}

\texttt{int} \textbf{\texttt{cols}}\texttt{(vector\ x)}\newline
The number of columns in the vector x, namely 1

\index{{\tt \bfseries cols }!{\tt (row\_vector x): int}|hyperpage}

\texttt{int} \textbf{\texttt{cols}}\texttt{(row\_vector\ x)}\newline
The number of columns in the row vector x

\index{{\tt \bfseries cols }!{\tt (matrix x): int}|hyperpage}

\texttt{int} \textbf{\texttt{cols}}\texttt{(matrix\ x)}\newline
The number of columns in the matrix x

\hypertarget{matrix-arithmetic-operators}{%
\section{Matrix arithmetic operators}\label{matrix-arithmetic-operators}}

Stan supports the basic matrix operations using infix, prefix and
postfix operations. This section lists the operations supported by
Stan along with their argument and result types.

\hypertarget{negation-prefix-operators}{%
\subsection{Negation prefix operators}\label{negation-prefix-operators}}

\index{{\tt \bfseries operator\_subtract }!{\tt (vector x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator-}}\texttt{(vector\ x)}\newline
The negation of the vector x.

\index{{\tt \bfseries operator\_subtract }!{\tt (row\_vector x): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator-}}\texttt{(row\_vector\ x)}\newline
The negation of the row vector x.

\index{{\tt \bfseries operator\_subtract }!{\tt (matrix x): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator-}}\texttt{(matrix\ x)}\newline
The negation of the matrix x.

\hypertarget{infix-matrix-operators}{%
\subsection{Infix matrix operators}\label{infix-matrix-operators}}

\index{{\tt \bfseries operator\_add }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator+}}\texttt{(vector\ x,\ vector\ y)}\newline
The sum of the vectors x and y.

\index{{\tt \bfseries operator\_add }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator+}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The sum of the row vectors x and y.

\index{{\tt \bfseries operator\_add }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator+}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The sum of the matrices x and y

\index{{\tt \bfseries operator\_subtract }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator-}}\texttt{(vector\ x,\ vector\ y)}\newline
The difference between the vectors x and y.

\index{{\tt \bfseries operator\_subtract }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator-}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The difference between the row vectors x and y

\index{{\tt \bfseries operator\_subtract }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator-}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The difference between the matrices x and y

\index{{\tt \bfseries operator\_multiply }!{\tt (real x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator*}}\texttt{(real\ x,\ vector\ y)}\newline
The product of the scalar x and vector y

\index{{\tt \bfseries operator\_multiply }!{\tt (real x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator*}}\texttt{(real\ x,\ row\_vector\ y)}\newline
The product of the scalar x and the row vector y

\index{{\tt \bfseries operator\_multiply }!{\tt (real x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator*}}\texttt{(real\ x,\ matrix\ y)}\newline
The product of the scalar x and the matrix y

\index{{\tt \bfseries operator\_multiply }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator*}}\texttt{(vector\ x,\ real\ y)}\newline
The product of the scalar y and vector x

\index{{\tt \bfseries operator\_multiply }!{\tt (vector x, row\_vector y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator*}}\texttt{(vector\ x,\ row\_vector\ y)}\newline
The product of the vector x and row vector y

\index{{\tt \bfseries operator\_multiply }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator*}}\texttt{(row\_vector\ x,\ real\ y)}\newline
The product of the scalar y and row vector x

\index{{\tt \bfseries operator\_multiply }!{\tt (row\_vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{operator*}}\texttt{(row\_vector\ x,\ vector\ y)}\newline
The product of the row vector x and vector y

\index{{\tt \bfseries operator\_multiply }!{\tt (row\_vector x, matrix y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator*}}\texttt{(row\_vector\ x,\ matrix\ y)}\newline
The product of the row vector x and matrix y

\index{{\tt \bfseries operator\_multiply }!{\tt (matrix x, real y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator*}}\texttt{(matrix\ x,\ real\ y)}\newline
The product of the scalar y and matrix x

\index{{\tt \bfseries operator\_multiply }!{\tt (matrix x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator*}}\texttt{(matrix\ x,\ vector\ y)}\newline
The product of the matrix x and vector y

\index{{\tt \bfseries operator\_multiply }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator*}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The product of the matrices x and y

\hypertarget{broadcast-infix-operators}{%
\subsection{Broadcast infix operators}\label{broadcast-infix-operators}}

\index{{\tt \bfseries operator\_add }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator+}}\texttt{(vector\ x,\ real\ y)}\newline
The result of adding y to every entry in the vector x

\index{{\tt \bfseries operator\_add }!{\tt (real x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator+}}\texttt{(real\ x,\ vector\ y)}\newline
The result of adding x to every entry in the vector y

\index{{\tt \bfseries operator\_add }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator+}}\texttt{(row\_vector\ x,\ real\ y)}\newline
The result of adding y to every entry in the row vector x

\index{{\tt \bfseries operator\_add }!{\tt (real x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator+}}\texttt{(real\ x,\ row\_vector\ y)}\newline
The result of adding x to every entry in the row vector y

\index{{\tt \bfseries operator\_add }!{\tt (matrix x, real y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator+}}\texttt{(matrix\ x,\ real\ y)}\newline
The result of adding y to every entry in the matrix x

\index{{\tt \bfseries operator\_add }!{\tt (real x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator+}}\texttt{(real\ x,\ matrix\ y)}\newline
The result of adding x to every entry in the matrix y

\index{{\tt \bfseries operator\_subtract }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator-}}\texttt{(vector\ x,\ real\ y)}\newline
The result of subtracting y from every entry in the vector x

\index{{\tt \bfseries operator\_subtract }!{\tt (real x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator-}}\texttt{(real\ x,\ vector\ y)}\newline
The result of adding x to every entry in the negation of the vector y

\index{{\tt \bfseries operator\_subtract }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator-}}\texttt{(row\_vector\ x,\ real\ y)}\newline
The result of subtracting y from every entry in the row vector x

\index{{\tt \bfseries operator\_subtract }!{\tt (real x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator-}}\texttt{(real\ x,\ row\_vector\ y)}\newline
The result of adding x to every entry in the negation of the row
vector y

\index{{\tt \bfseries operator\_subtract }!{\tt (matrix x, real y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator-}}\texttt{(matrix\ x,\ real\ y)}\newline
The result of subtracting y from every entry in the matrix x

\index{{\tt \bfseries operator\_subtract }!{\tt (real x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator-}}\texttt{(real\ x,\ matrix\ y)}\newline
The result of adding x to every entry in negation of the matrix y

\index{{\tt \bfseries operator\_divide }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator/}}\texttt{(vector\ x,\ real\ y)}\newline
The result of dividing each entry in the vector x by y

\index{{\tt \bfseries operator\_divide }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator/}}\texttt{(row\_vector\ x,\ real\ y)}\newline
The result of dividing each entry in the row vector x by y

\index{{\tt \bfseries operator\_divide }!{\tt (matrix x, real y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator/}}\texttt{(matrix\ x,\ real\ y)}\newline
The result of dividing each entry in the matrix x by y

\hypertarget{elementwise-arithmetic-operations}{%
\subsection{Elementwise arithmetic operations}\label{elementwise-arithmetic-operations}}

\index{{\tt \bfseries operator\_elt\_multiply }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator.*}}\texttt{(vector\ x,\ vector\ y)}\newline
The elementwise product of y and x

\index{{\tt \bfseries operator\_elt\_multiply }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator.*}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The elementwise product of y and x

\index{{\tt \bfseries operator\_elt\_multiply }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator.*}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The elementwise product of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator./}}\texttt{(vector\ x,\ vector\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator./}}\texttt{(vector\ x,\ real\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (real x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator./}}\texttt{(real\ x,\ vector\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator./}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator./}}\texttt{(row\_vector\ x,\ real\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (real x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator./}}\texttt{(real\ x,\ row\_vector\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator./}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (matrix x, real y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator./}}\texttt{(matrix\ x,\ real\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_divide }!{\tt (real x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator./}}\texttt{(real\ x,\ matrix\ y)}\newline
The elementwise quotient of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator.\^{}}}\texttt{(vector\ x,\ vector\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator.\^{}}}\texttt{(vector\ x,\ real\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (real x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator.\^{}}}\texttt{(real\ x,\ vector\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator.\^{}}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator.\^{}}}\texttt{(row\_vector\ x,\ real\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (real x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator.\^{}}}\texttt{(real\ x,\ row\_vector\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator.\^{}}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (matrix x, real y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator.\^{}}}\texttt{(matrix\ x,\ real\ y)}\newline
The elementwise power of y and x

\index{{\tt \bfseries operator\_elt\_pow }!{\tt (real x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator.\^{}}}\texttt{(real\ x,\ matrix\ y)}\newline
The elementwise power of y and x

\hypertarget{transposition-operator}{%
\section{Transposition operator}\label{transposition-operator}}

Matrix transposition is represented using a postfix operator.

\index{{\tt \bfseries operator\_transpose }!{\tt (matrix x): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator\textquotesingle{}}}\texttt{(matrix\ x)}\newline
The transpose of the matrix x, written as \texttt{x\textquotesingle{}}

\index{{\tt \bfseries operator\_transpose }!{\tt (vector x): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator\textquotesingle{}}}\texttt{(vector\ x)}\newline
The transpose of the vector x, written as \texttt{x\textquotesingle{}}

\index{{\tt \bfseries operator\_transpose }!{\tt (row\_vector x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator\textquotesingle{}}}\texttt{(row\_vector\ x)}\newline
The transpose of the row vector x, written as \texttt{x\textquotesingle{}}

\hypertarget{elementwise-functions}{%
\section{Elementwise functions}\label{elementwise-functions}}

Elementwise functions apply a function to each element of a vector or
matrix, returning a result of the same shape as the argument. There
are many functions that are vectorized in addition to the ad hoc cases
listed in this section; see section \protect\hyperlink{fun-vectorization}{function vectorization}for the
general cases.

\hypertarget{dot-products-and-specialized-products}{%
\section{Dot products and specialized products}\label{dot-products-and-specialized-products}}

\index{{\tt \bfseries dot\_product }!{\tt (vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{dot\_product}}\texttt{(vector\ x,\ vector\ y)}\newline
The dot product of x and y

\index{{\tt \bfseries dot\_product }!{\tt (vector x, row\_vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{dot\_product}}\texttt{(vector\ x,\ row\_vector\ y)}\newline
The dot product of x and y

\index{{\tt \bfseries dot\_product }!{\tt (row\_vector x, vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{dot\_product}}\texttt{(row\_vector\ x,\ vector\ y)}\newline
The dot product of x and y

\index{{\tt \bfseries dot\_product }!{\tt (row\_vector x, row\_vector y): real}|hyperpage}

\texttt{real} \textbf{\texttt{dot\_product}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The dot product of x and y

\index{{\tt \bfseries columns\_dot\_product }!{\tt (vector x, vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{columns\_dot\_product}}\texttt{(vector\ x,\ vector\ y)}\newline
The dot product of the columns of x and y

\index{{\tt \bfseries columns\_dot\_product }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{columns\_dot\_product}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The dot product of the columns of x and y

\index{{\tt \bfseries columns\_dot\_product }!{\tt (matrix x, matrix y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{columns\_dot\_product}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The dot product of the columns of x and y

\index{{\tt \bfseries rows\_dot\_product }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rows\_dot\_product}}\texttt{(vector\ x,\ vector\ y)}\newline
The dot product of the rows of x and y

\index{{\tt \bfseries rows\_dot\_product }!{\tt (row\_vector x, row\_vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rows\_dot\_product}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
The dot product of the rows of x and y

\index{{\tt \bfseries rows\_dot\_product }!{\tt (matrix x, matrix y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rows\_dot\_product}}\texttt{(matrix\ x,\ matrix\ y)}\newline
The dot product of the rows of x and y

\index{{\tt \bfseries dot\_self }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{dot\_self}}\texttt{(vector\ x)}\newline
The dot product of the vector x with itself

\index{{\tt \bfseries dot\_self }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{dot\_self}}\texttt{(row\_vector\ x)}\newline
The dot product of the row vector x with itself

\index{{\tt \bfseries columns\_dot\_self }!{\tt (vector x): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{columns\_dot\_self}}\texttt{(vector\ x)}\newline
The dot product of the columns of x with themselves

\index{{\tt \bfseries columns\_dot\_self }!{\tt (row\_vector x): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{columns\_dot\_self}}\texttt{(row\_vector\ x)}\newline
The dot product of the columns of x with themselves

\index{{\tt \bfseries columns\_dot\_self }!{\tt (matrix x): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{columns\_dot\_self}}\texttt{(matrix\ x)}\newline
The dot product of the columns of x with themselves

\index{{\tt \bfseries rows\_dot\_self }!{\tt (vector x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rows\_dot\_self}}\texttt{(vector\ x)}\newline
The dot product of the rows of x with themselves

\index{{\tt \bfseries rows\_dot\_self }!{\tt (row\_vector x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rows\_dot\_self}}\texttt{(row\_vector\ x)}\newline
The dot product of the rows of x with themselves

\index{{\tt \bfseries rows\_dot\_self }!{\tt (matrix x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rows\_dot\_self}}\texttt{(matrix\ x)}\newline
The dot product of the rows of x with themselves

\hypertarget{specialized-products}{%
\subsection{Specialized products}\label{specialized-products}}

\index{{\tt \bfseries tcrossprod }!{\tt (matrix x): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{tcrossprod}}\texttt{(matrix\ x)}\newline
The product of x postmultiplied by its own transpose, similar to the
tcrossprod(x) function in R. The result is a symmetric matrix
\(\text{x}\,\text{x}^{\top}\).

\index{{\tt \bfseries crossprod }!{\tt (matrix x): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{crossprod}}\texttt{(matrix\ x)}\newline
The product of x premultiplied by its own transpose, similar to the
crossprod(x) function in R. The result is a symmetric matrix
\(\text{x}^{\top}\,\text{x}\).

The following functions all provide shorthand forms for common
expressions, which are also much more efficient.

\index{{\tt \bfseries quad\_form }!{\tt (matrix A, matrix B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{quad\_form}}\texttt{(matrix\ A,\ matrix\ B)}\newline
The quadratic form, i.e., \texttt{B\textquotesingle{}\ *\ A\ *\ B}.

\index{{\tt \bfseries quad\_form }!{\tt (matrix A, vector B): real}|hyperpage}

\texttt{real} \textbf{\texttt{quad\_form}}\texttt{(matrix\ A,\ vector\ B)}\newline
The quadratic form, i.e., \texttt{B\textquotesingle{}\ *\ A\ *\ B}.

\index{{\tt \bfseries quad\_form\_diag }!{\tt (matrix m, vector v): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{quad\_form\_diag}}\texttt{(matrix\ m,\ vector\ v)}\newline
The quadratic form using the column vector v as a diagonal matrix,
i.e., \texttt{diag\_matrix(v)\ *\ m\ *\ diag\_matrix(v)}.

\index{{\tt \bfseries quad\_form\_diag }!{\tt (matrix m, row\_vector rv): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{quad\_form\_diag}}\texttt{(matrix\ m,\ row\_vector\ rv)}\newline
The quadratic form using the row vector rv as a diagonal matrix, i.e.,
\texttt{diag\_matrix(rv)\ *\ m\ *\ diag\_matrix(rv)}.

\index{{\tt \bfseries quad\_form\_sym }!{\tt (matrix A, matrix B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{quad\_form\_sym}}\texttt{(matrix\ A,\ matrix\ B)}\newline
Similarly to quad\_form, gives \texttt{B\textquotesingle{}\ *\ A\ *\ B}, but additionally checks if
A is symmetric and ensures that the result is also symmetric.

\index{{\tt \bfseries quad\_form\_sym }!{\tt (matrix A, vector B): real}|hyperpage}

\texttt{real} \textbf{\texttt{quad\_form\_sym}}\texttt{(matrix\ A,\ vector\ B)}\newline
Similarly to quad\_form, gives \texttt{B\textquotesingle{}\ *\ A\ *\ B}, but additionally checks if
A is symmetric and ensures that the result is also symmetric.

\index{{\tt \bfseries trace\_quad\_form }!{\tt (matrix A, matrix B): real}|hyperpage}

\texttt{real} \textbf{\texttt{trace\_quad\_form}}\texttt{(matrix\ A,\ matrix\ B)}\newline
The trace of the quadratic form, i.e., \texttt{trace(B\textquotesingle{}\ *\ A\ *\ B)}.

\index{{\tt \bfseries trace\_gen\_quad\_form }!{\tt (matrix D,matrix A, matrix B): real}|hyperpage}

\texttt{real} \textbf{\texttt{trace\_gen\_quad\_form}}\texttt{(matrix\ D,matrix\ A,\ matrix\ B)}\newline
The trace of a generalized quadratic form, i.e., \texttt{trace(D\ *\ B\textquotesingle{}\ *\ A\ *\ B).}

\index{{\tt \bfseries multiply\_lower\_tri\_self\_transpose }!{\tt (matrix x): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{multiply\_lower\_tri\_self\_transpose}}\texttt{(matrix\ x)}\newline
The product of the lower triangular portion of x (including the
diagonal) times its own transpose; that is, if \texttt{L} is a matrix of the
same dimensions as x with \texttt{L(m,n)} equal to \texttt{x(m,n)} for \(\text{n} \leq \text{m}\) and \texttt{L(m,n)} equal to 0 if \(\text{n} > \text{m}\), the
result is the symmetric matrix \(\text{L}\,\text{L}^{\top}\). This is a
specialization of tcrossprod(x) for lower-triangular matrices. The
input matrix does not need to be square.

\index{{\tt \bfseries diag\_pre\_multiply }!{\tt (vector v, matrix m): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{diag\_pre\_multiply}}\texttt{(vector\ v,\ matrix\ m)}\newline
Return the product of the diagonal matrix formed from the vector v and
the matrix m, i.e., \texttt{diag\_matrix(v)\ *\ m}.

\index{{\tt \bfseries diag\_pre\_multiply }!{\tt (row\_vector rv, matrix m): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{diag\_pre\_multiply}}\texttt{(row\_vector\ rv,\ matrix\ m)}\newline
Return the product of the diagonal matrix formed from the vector rv
and the matrix m, i.e., \texttt{diag\_matrix(rv)\ *\ m}.

\index{{\tt \bfseries diag\_post\_multiply }!{\tt (matrix m, vector v): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{diag\_post\_multiply}}\texttt{(matrix\ m,\ vector\ v)}\newline
Return the product of the matrix m and the diagonal matrix formed from
the vector v, i.e., \texttt{m\ *\ diag\_matrix(v)}.

\index{{\tt \bfseries diag\_post\_multiply }!{\tt (matrix m, row\_vector rv): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{diag\_post\_multiply}}\texttt{(matrix\ m,\ row\_vector\ rv)}\newline
Return the product of the matrix \texttt{m} and the diagonal matrix formed
from the the row vector \texttt{rv}, i.e., \texttt{m\ *\ diag\_matrix(rv)}.

\hypertarget{reductions}{%
\section{Reductions}\label{reductions}}

\hypertarget{log-sum-of-exponents}{%
\subsection{Log sum of exponents}\label{log-sum-of-exponents}}

\index{{\tt \bfseries log\_sum\_exp }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_sum\_exp}}\texttt{(vector\ x)}\newline
The natural logarithm of the sum of the exponentials of the elements
in x

\index{{\tt \bfseries log\_sum\_exp }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_sum\_exp}}\texttt{(row\_vector\ x)}\newline
The natural logarithm of the sum of the exponentials of the elements
in x

\index{{\tt \bfseries log\_sum\_exp }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_sum\_exp}}\texttt{(matrix\ x)}\newline
The natural logarithm of the sum of the exponentials of the elements
in x

\hypertarget{minimum-and-maximum-1}{%
\subsection{Minimum and maximum}\label{minimum-and-maximum-1}}

\index{{\tt \bfseries min }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{min}}\texttt{(vector\ x)}\newline
The minimum value in x, or \(+\infty\) if x is empty

\index{{\tt \bfseries min }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{min}}\texttt{(row\_vector\ x)}\newline
The minimum value in x, or \(+\infty\) if x is empty

\index{{\tt \bfseries min }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{min}}\texttt{(matrix\ x)}\newline
The minimum value in x, or \(+\infty\) if x is empty

\index{{\tt \bfseries max }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{max}}\texttt{(vector\ x)}\newline
The maximum value in x, or \(-\infty\) if x is empty

\index{{\tt \bfseries max }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{max}}\texttt{(row\_vector\ x)}\newline
The maximum value in x, or \(-\infty\) if x is empty

\index{{\tt \bfseries max }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{max}}\texttt{(matrix\ x)}\newline
The maximum value in x, or \(-\infty\) if x is empty

\hypertarget{sums-and-products}{%
\subsection{Sums and products}\label{sums-and-products}}

\index{{\tt \bfseries sum }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sum}}\texttt{(vector\ x)}\newline
The sum of the values in x, or 0 if x is empty

\index{{\tt \bfseries sum }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sum}}\texttt{(row\_vector\ x)}\newline
The sum of the values in x, or 0 if x is empty

\index{{\tt \bfseries sum }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sum}}\texttt{(matrix\ x)}\newline
The sum of the values in x, or 0 if x is empty

\index{{\tt \bfseries prod }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{prod}}\texttt{(vector\ x)}\newline
The product of the values in x, or 1 if x is empty

\index{{\tt \bfseries prod }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{prod}}\texttt{(row\_vector\ x)}\newline
The product of the values in x, or 1 if x is empty

\index{{\tt \bfseries prod }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{prod}}\texttt{(matrix\ x)}\newline
The product of the values in x, or 1 if x is empty

\hypertarget{sample-moments}{%
\subsection{Sample moments}\label{sample-moments}}

Full definitions are provided for sample moments in section
\protect\hyperlink{array-reductions}{array reductions}.

\index{{\tt \bfseries mean }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{mean}}\texttt{(vector\ x)}\newline
The sample mean of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries mean }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{mean}}\texttt{(row\_vector\ x)}\newline
The sample mean of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries mean }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{mean}}\texttt{(matrix\ x)}\newline
The sample mean of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries variance }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{variance}}\texttt{(vector\ x)}\newline
The sample variance of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries variance }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{variance}}\texttt{(row\_vector\ x)}\newline
The sample variance of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries variance }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{variance}}\texttt{(matrix\ x)}\newline
The sample variance of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries sd }!{\tt (vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sd}}\texttt{(vector\ x)}\newline
The sample standard deviation of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries sd }!{\tt (row\_vector x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sd}}\texttt{(row\_vector\ x)}\newline
The sample standard deviation of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\index{{\tt \bfseries sd }!{\tt (matrix x): real}|hyperpage}

\texttt{real} \textbf{\texttt{sd}}\texttt{(matrix\ x)}\newline
The sample standard deviation of the values in x; see section
\protect\hyperlink{array-reductions}{array reductions} for details.

\hypertarget{matrix-broadcast}{%
\section{Broadcast functions}\label{matrix-broadcast}}

The following broadcast functions allow vectors, row vectors and
matrices to be created by copying a single element into all of their
cells. Matrices may also be created by stacking copies of row vectors
vertically or stacking copies of column vectors horizontally.

\index{{\tt \bfseries rep\_vector }!{\tt (real x, int m): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{rep\_vector}}\texttt{(real\ x,\ int\ m)}\newline
Return the size m (column) vector consisting of copies of x.

\index{{\tt \bfseries rep\_row\_vector }!{\tt (real x, int n): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{rep\_row\_vector}}\texttt{(real\ x,\ int\ n)}\newline
Return the size n row vector consisting of copies of x.

\index{{\tt \bfseries rep\_matrix }!{\tt (real x, int m, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{rep\_matrix}}\texttt{(real\ x,\ int\ m,\ int\ n)}\newline
Return the m by n matrix consisting of copies of x.

\index{{\tt \bfseries rep\_matrix }!{\tt (vector v, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{rep\_matrix}}\texttt{(vector\ v,\ int\ n)}\newline
Return the m by n matrix consisting of n copies of the (column) vector
v of size m.

\index{{\tt \bfseries rep\_matrix }!{\tt (row\_vector rv, int m): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{rep\_matrix}}\texttt{(row\_vector\ rv,\ int\ m)}\newline
Return the m by n matrix consisting of m copies of the row vector rv
of size n.

Unlike the situation with array broadcasting (see section
\protect\hyperlink{array-broadcasting}{array broadcasting}), where there is a distinction between
integer and real arguments, the following two statements produce the
same result for vector broadcasting; row vector and matrix
broadcasting behave similarly.

\begin{verbatim}
 vector[3] x;
 x = rep_vector(1, 3);
 x = rep_vector(1.0, 3);
\end{verbatim}

There are no integer vector or matrix types, so integer values are
automatically promoted.

\hypertarget{diagonal-matrix-functions}{%
\section{Diagonal matrix functions}\label{diagonal-matrix-functions}}

\index{{\tt \bfseries add\_diag }!{\tt (matrix m, row\_vector d): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{add\_diag}}\texttt{(matrix\ m,\ row\_vector\ d)}\newline
Add row\_vector \texttt{d} to the diagonal of matrix \texttt{m}.

\index{{\tt \bfseries add\_diag }!{\tt (matrix m, vector d): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{add\_diag}}\texttt{(matrix\ m,\ vector\ d)}\newline
Add vector \texttt{d} to the diagonal of matrix \texttt{m}.

\index{{\tt \bfseries add\_diag }!{\tt (matrix m, real d): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{add\_diag}}\texttt{(matrix\ m,\ real\ d)}\newline
Add scalar \texttt{d} to every diagonal element of matrix \texttt{m}.

\index{{\tt \bfseries diagonal }!{\tt (matrix x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{diagonal}}\texttt{(matrix\ x)}\newline
The diagonal of the matrix x

\index{{\tt \bfseries diag\_matrix }!{\tt (vector x): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{diag\_matrix}}\texttt{(vector\ x)}\newline
The diagonal matrix with diagonal x

Although the \texttt{diag\_matrix} function is available, it is unlikely to
ever show up in an efficient Stan program. For example, rather than
converting a diagonal to a full matrix for use as a covariance matrix,

\begin{verbatim}
 y ~ multi_normal(mu, diag_matrix(square(sigma)));
\end{verbatim}

it is much more efficient to just use a univariate normal, which
produces the same density,

\begin{verbatim}
 y ~ normal(mu, sigma);
\end{verbatim}

Rather than writing \texttt{m\ *\ diag\_matrix(v)} where \texttt{m} is a matrix and \texttt{v}
is a vector, it is much more efficient to write \texttt{diag\_post\_multiply(m,\ v)} (and similarly for pre-multiplication). By the same token, it is
better to use \texttt{quad\_form\_diag(m,\ v)} rather than \texttt{quad\_form(m,\ diag\_matrix(v))}.

\hypertarget{slicing-and-blocking-functions}{%
\section{Slicing and blocking functions}\label{slicing-and-blocking-functions}}

Stan provides several functions for generating slices or blocks or
diagonal entries for matrices.

\hypertarget{columns-and-rows}{%
\subsection{Columns and rows}\label{columns-and-rows}}

\index{{\tt \bfseries col }!{\tt (matrix x, int n): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{col}}\texttt{(matrix\ x,\ int\ n)}\newline
The n-th column of matrix x

\index{{\tt \bfseries row }!{\tt (matrix x, int m): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{row}}\texttt{(matrix\ x,\ int\ m)}\newline
The m-th row of matrix x

The \texttt{row} function is special in that it may be used as an lvalue in
an assignment statement (i.e., something to which a value may be
assigned). The row function is also special in that the indexing
notation \texttt{x{[}m{]}} is just an alternative way of writing \texttt{row(x,m)}. The
\texttt{col} function may \textbf{not}, be used as an lvalue, nor is there an
indexing based shorthand for it.

\hypertarget{block-operations}{%
\subsection{Block operations}\label{block-operations}}

\hypertarget{matrix-slicing-operations}{%
\subsubsection{Matrix slicing operations}\label{matrix-slicing-operations}}

Block operations may be used to extract a sub-block of a matrix.

\index{{\tt \bfseries block }!{\tt (matrix x, int i, int j, int n\_rows, int n\_cols): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{block}}\texttt{(matrix\ x,\ int\ i,\ int\ j,\ int\ n\_rows,\ int\ n\_cols)}\newline
Return the submatrix of x that starts at row i and column j and
extends n\_rows rows and n\_cols columns.

The sub-row and sub-column operations may be used to extract a slice
of row or column from a matrix

\index{{\tt \bfseries sub\_col }!{\tt (matrix x, int i, int j, int n\_rows): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{sub\_col}}\texttt{(matrix\ x,\ int\ i,\ int\ j,\ int\ n\_rows)}\newline
Return the sub-column of x that starts at row i and column j and
extends n\_rows rows and 1 column.

\index{{\tt \bfseries sub\_row }!{\tt (matrix x, int i, int j, int n\_cols): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{sub\_row}}\texttt{(matrix\ x,\ int\ i,\ int\ j,\ int\ n\_cols)}\newline
Return the sub-row of x that starts at row i and column j and extends
1 row and n\_cols columns.

\hypertarget{vector-and-array-slicing-operations}{%
\subsubsection{Vector and array slicing operations}\label{vector-and-array-slicing-operations}}

The head operation extracts the first \(n\) elements of a vector and the
tail operation the last. The segment operation extracts an arbitrary
subvector.

\index{{\tt \bfseries head }!{\tt (vector v, int n): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{head}}\texttt{(vector\ v,\ int\ n)}\newline
Return the vector consisting of the first n elements of v.

\index{{\tt \bfseries head }!{\tt (row\_vector rv, int n): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{head}}\texttt{(row\_vector\ rv,\ int\ n)}\newline
Return the row vector consisting of the first n elements of rv.

\index{{\tt \bfseries head }!{\tt (T[] sv, int n): T[]}|hyperpage}

\texttt{T{[}{]}} \textbf{\texttt{head}}\texttt{(T{[}{]}\ sv,\ int\ n)}\newline
Return the array consisting of the first n elements of sv; applies to
up to three-dimensional arrays containing any type of elements \texttt{T}.

\index{{\tt \bfseries tail }!{\tt (vector v, int n): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{tail}}\texttt{(vector\ v,\ int\ n)}\newline
Return the vector consisting of the last n elements of v.

\index{{\tt \bfseries tail }!{\tt (row\_vector rv, int n): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{tail}}\texttt{(row\_vector\ rv,\ int\ n)}\newline
Return the row vector consisting of the last n elements of rv.

\index{{\tt \bfseries tail }!{\tt (T[] sv, int n): T[]}|hyperpage}

\texttt{T{[}{]}} \textbf{\texttt{tail}}\texttt{(T{[}{]}\ sv,\ int\ n)}\newline
Return the array consisting of the last n elements of sv; applies to
up to three-dimensional arrays containing any type of elements \texttt{T}.

\index{{\tt \bfseries segment }!{\tt (vector v, int i, int n): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{segment}}\texttt{(vector\ v,\ int\ i,\ int\ n)}\newline
Return the vector consisting of the n elements of v starting at i;
i.e., elements i through through i + n - 1.

\index{{\tt \bfseries segment }!{\tt (row\_vector rv, int i, int n): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{segment}}\texttt{(row\_vector\ rv,\ int\ i,\ int\ n)}\newline
Return the row vector consisting of the n elements of rv starting at
i; i.e., elements i through through i + n - 1.

\index{{\tt \bfseries segment }!{\tt (T[] sv, int i, int n): T[]}|hyperpage}

\texttt{T{[}{]}} \textbf{\texttt{segment}}\texttt{(T{[}{]}\ sv,\ int\ i,\ int\ n)}\newline
Return the array consisting of the n elements of sv starting at i;
i.e., elements i through through i + n - 1. Applies to up to
three-dimensional arrays containing any type of elements \texttt{T}.

\hypertarget{matrix-concatenation}{%
\section{Matrix concatenation}\label{matrix-concatenation}}

Stan's matrix concatenation operations \texttt{append\_col} and \texttt{append\_row}
are like the operations \texttt{cbind} and \texttt{rbind} in R.

\hypertarget{horizontal-concatenation}{%
\subsubsection{Horizontal concatenation}\label{horizontal-concatenation}}

\index{{\tt \bfseries append\_col }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_col}}\texttt{(matrix\ x,\ matrix\ y)}\newline
Combine matrices x and y by columns. The matrices must have the same
number of rows.

\index{{\tt \bfseries append\_col }!{\tt (matrix x, vector y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_col}}\texttt{(matrix\ x,\ vector\ y)}\newline
Combine matrix x and vector y by columns. The matrix and the vector
must have the same number of rows.

\index{{\tt \bfseries append\_col }!{\tt (vector x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_col}}\texttt{(vector\ x,\ matrix\ y)}\newline
Combine vector x and matrix y by columns. The vector and the matrix
must have the same number of rows.

\index{{\tt \bfseries append\_col }!{\tt (vector x, vector y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_col}}\texttt{(vector\ x,\ vector\ y)}\newline
Combine vectors x and y by columns. The vectors must have the same
number of rows.

\index{{\tt \bfseries append\_col }!{\tt (row\_vector x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{append\_col}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
Combine row vectors x and y of any size into another row vector.

\index{{\tt \bfseries append\_col }!{\tt (real x, row\_vector y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{append\_col}}\texttt{(real\ x,\ row\_vector\ y)}\newline
Append x to the front of y, returning another row vector.

\index{{\tt \bfseries append\_col }!{\tt (row\_vector x, real y): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{append\_col}}\texttt{(row\_vector\ x,\ real\ y)}\newline
Append y to the end of x, returning another row vector.

\hypertarget{vertical-concatenation}{%
\subsubsection{Vertical concatenation}\label{vertical-concatenation}}

\index{{\tt \bfseries append\_row }!{\tt (matrix x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_row}}\texttt{(matrix\ x,\ matrix\ y)}\newline
Combine matrices x and y by rows. The matrices must have the same
number of columns.

\index{{\tt \bfseries append\_row }!{\tt (matrix x, row\_vector y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_row}}\texttt{(matrix\ x,\ row\_vector\ y)}\newline
Combine matrix x and row vector y by rows. The matrix and the row
vector must have the same number of columns.

\index{{\tt \bfseries append\_row }!{\tt (row\_vector x, matrix y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_row}}\texttt{(row\_vector\ x,\ matrix\ y)}\newline
Combine row vector x and matrix y by rows. The row vector and the
matrix must have the same number of columns.

\index{{\tt \bfseries append\_row }!{\tt (row\_vector x, row\_vector y): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{append\_row}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
Combine row vectors x and y by row. The row vectors must have the same
number of columns.

\index{{\tt \bfseries append\_row }!{\tt (vector x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{append\_row}}\texttt{(vector\ x,\ vector\ y)}\newline
Concatenate vectors x and y of any size into another vector.

\index{{\tt \bfseries append\_row }!{\tt (real x, vector y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{append\_row}}\texttt{(real\ x,\ vector\ y)}\newline
Append x to the top of y, returning another vector.

\index{{\tt \bfseries append\_row }!{\tt (vector x, real y): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{append\_row}}\texttt{(vector\ x,\ real\ y)}\newline
Append y to the bottom of x, returning another vector.

\hypertarget{softmax}{%
\section{Special matrix functions}\label{softmax}}

\hypertarget{softmax-1}{%
\subsection{Softmax}\label{softmax-1}}

The softmax function maps\footnote{The softmax function is so called because in the limit
  as \(y_n \rightarrow \infty\) with \(y_m\) for \(m \neq n\) held constant,
  the result tends toward the ``one-hot'' vector \(\theta\) with \(\theta_n = 1\) and \(\theta_m = 0\) for \(m \neq n\), thus providing a ``soft''
  version of the maximum function.} \(y \in \mathbb{R}^K\) to the
\(K\)-simplex by \[ \text{softmax}(y)  = \frac{\exp(y)}
{\sum_{k=1}^K \exp(y_k)}, \] where \(\exp(y)\) is the componentwise
exponentiation of \(y\). Softmax is usually calculated on the log scale,
\begin{eqnarray*} \log \text{softmax}(y) & = & \ y - \log \sum_{k=1}^K
\exp(y_k) \\[4pt] & = & y - \mathrm{log\_sum\_exp}(y). \end{eqnarray*}
where the vector \(y\) minus the scalar \(\mathrm{log\_sum\_exp}(y)\)
subtracts the scalar from each component of \(y\).

Stan provides the following functions for softmax and its log.

\index{{\tt \bfseries softmax }!{\tt (vector x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{softmax}}\texttt{(vector\ x)}\newline
The softmax of x

\index{{\tt \bfseries log\_softmax }!{\tt (vector x): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{log\_softmax}}\texttt{(vector\ x)}\newline
The natural logarithm of the softmax of x

\hypertarget{cumulative-sums}{%
\subsection{Cumulative sums}\label{cumulative-sums}}

The cumulative sum of a sequence \(x_1,\ldots,x_N\) is the sequence
\(y_1,\ldots,y_N\), where \[ y_n = \sum_{m = 1}^{n} x_m. \]

\index{{\tt \bfseries cumulative\_sum }!{\tt (real[] x): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{cumulative\_sum}}\texttt{(real{[}{]}\ x)}\newline
The cumulative sum of x

\index{{\tt \bfseries cumulative\_sum }!{\tt (vector v): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{cumulative\_sum}}\texttt{(vector\ v)}\newline
The cumulative sum of v

\index{{\tt \bfseries cumulative\_sum }!{\tt (row\_vector rv): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{cumulative\_sum}}\texttt{(row\_vector\ rv)}\newline
The cumulative sum of rv

\hypertarget{covariance}{%
\section{Covariance functions}\label{covariance}}

\hypertarget{exponentiated-quadratic-covariance-function}{%
\subsection{Exponentiated quadratic covariance function}\label{exponentiated-quadratic-covariance-function}}

The exponentiated quadratic kernel defines the covariance between
\(f(x_i)\) and \(f(x_j)\) where \(f\colon \mathbb{R}^D \mapsto \mathbb{R}\)
as a function of the squared Euclidian distance between \(x_i \in \mathbb{R}^D\) and \(x_j \in \mathbb{R}^D\): \[   \text{cov}(f(x_i),
f(x_j)) = k(x_i, x_j) = \alpha^2 \exp \left(         -
\dfrac{1}{2\rho^2} \sum_{d=1}^D (x_{i,d} - x_{j,d})^2 \right) \] with
\(\alpha\) and \(\rho\) constrained to be positive.

There are two variants of the exponentiated quadratic covariance
function in Stan. One builds a covariance matrix, \(K \in \mathbb{R}^{N \times N}\) for \(x_1, \dots, x_N\), where \(K_{i,j} = k(x_i, x_j)\), which
is necessarily symmetric and positive semidefinite by construction.
There is a second variant of the exponentiated quadratic covariance
function that builds a \(K \in \mathbb{R}^{N \times M}\) covariance
matrix for \(x_1, \dots, x_N\) and \(x^\prime_1, \dots, x^\prime_M\),
where \(x_i \in \mathbb{R}^D\) and \(x^\prime_i \in \mathbb{R}^D\) and
\(K_{i,j} = k(x_i, x^\prime_j)\).

\index{{\tt \bfseries cov\_exp\_quad }!{\tt (row\_vectors x, real alpha, real rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cov\_exp\_quad}}\texttt{(row\_vectors\ x,\ real\ alpha,\ real\ rho)}\newline
The covariance matrix with an exponentiated quadratic kernel of x.

\index{{\tt \bfseries cov\_exp\_quad }!{\tt (vectors x, real alpha, real rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cov\_exp\_quad}}\texttt{(vectors\ x,\ real\ alpha,\ real\ rho)}\newline
The covariance matrix with an exponentiated quadratic kernel of x.

\index{{\tt \bfseries cov\_exp\_quad }!{\tt (real[] x, real alpha, real rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cov\_exp\_quad}}\texttt{(real{[}{]}\ x,\ real\ alpha,\ real\ rho)}\newline
The covariance matrix with an exponentiated quadratic kernel of x.

\index{{\tt \bfseries cov\_exp\_quad }!{\tt (row\_vectors x1, row\_vectors x2, real alpha, real rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cov\_exp\_quad}}\texttt{(row\_vectors\ x1,\ row\_vectors\ x2,\ real\ alpha,\ real\ rho)}\newline
The covariance matrix with an exponentiated quadratic kernel of x1 and
x2.

\index{{\tt \bfseries cov\_exp\_quad }!{\tt (vectors x1, vectors x2, real alpha, real rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cov\_exp\_quad}}\texttt{(vectors\ x1,\ vectors\ x2,\ real\ alpha,\ real\ rho)}\newline
The covariance matrix with an exponentiated quadratic kernel of x1 and
x2.

\index{{\tt \bfseries cov\_exp\_quad }!{\tt (real[] x1, real[] x2, real alpha, real rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cov\_exp\_quad}}\texttt{(real{[}{]}\ x1,\ real{[}{]}\ x2,\ real\ alpha,\ real\ rho)}\newline
The covariance matrix with an exponentiated quadratic kernel of x1 and
x2.

\hypertarget{linear-algebra-functions-and-solvers}{%
\section{Linear algebra functions and solvers}\label{linear-algebra-functions-and-solvers}}

\hypertarget{matrix-division-operators-and-functions}{%
\subsection{Matrix division operators and functions}\label{matrix-division-operators-and-functions}}

In general, it is much more efficient and also more arithmetically
stable to use matrix division than to multiply by an inverse. There
are specialized forms for lower triangular matrices and for symmetric,
positive-definite matrices.

\hypertarget{matrix-division-operators}{%
\subsubsection{Matrix division operators}\label{matrix-division-operators}}

\index{{\tt \bfseries operator\_divide }!{\tt (row\_vector b, matrix A): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{operator/}}\texttt{(row\_vector\ b,\ matrix\ A)}\newline
The right division of b by A; equivalently \texttt{b\ *\ inverse(A)}

\index{{\tt \bfseries operator\_divide }!{\tt (matrix B, matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator/}}\texttt{(matrix\ B,\ matrix\ A)}\newline
The right division of B by A; equivalently \texttt{B\ *\ inverse(A)}

\index{{\tt \bfseries operator\_left\_div }!{\tt (matrix A, vector b): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{operator\textbackslash{}}}\texttt{(matrix\ A,\ vector\ b)}\newline
The left division of A by b; equivalently \texttt{inverse(A)\ *\ b}

\index{{\tt \bfseries operator\_left\_div }!{\tt (matrix A, matrix B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{operator\textbackslash{}}}\texttt{(matrix\ A,\ matrix\ B)}\newline
The left division of A by B; equivalently \texttt{inverse(A)\ *\ B}

\hypertarget{lower-triangular-matrix-division-functions}{%
\subsubsection{Lower-triangular matrix division functions}\label{lower-triangular-matrix-division-functions}}

There are four division functions which use lower triangular views of
a matrix. The lower triangular view of a matrix \(\text{tri}(A)\) is
used in the definitions and defined by \[ \text{tri}(A)[m,n] = \left\{
\begin{array}{ll} A[m,n] & \text{if } m \geq n, \text{ and} \\[4pt] 0
& \text{otherwise}. \end{array} \right. \] When a lower triangular
view of a matrix is used, the elements above the diagonal are ignored.

\index{{\tt \bfseries mdivide\_left\_tri\_low }!{\tt (matrix A, vector b): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{mdivide\_left\_tri\_low}}\texttt{(matrix\ A,\ vector\ b)}\newline
The left division of b by a lower-triangular view of A; algebraically
equivalent to the less efficient and stable form \texttt{inverse(tri(A))\ *\ b}, where \texttt{tri(A)} is the lower-triangular portion of A with the
above-diagonal entries set to zero.

\index{{\tt \bfseries mdivide\_left\_tri\_low }!{\tt (matrix A, matrix B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{mdivide\_left\_tri\_low}}\texttt{(matrix\ A,\ matrix\ B)}\newline
The left division of B by a triangular view of A; algebraically
equivalent to the less efficient and stable form \texttt{inverse(tri(A))\ *\ B}, where \texttt{tri(A)} is the lower-triangular portion of A with the
above-diagonal entries set to zero.

\index{{\tt \bfseries mdivide\_right\_tri\_low }!{\tt (row\_vector b, matrix A): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{mdivide\_right\_tri\_low}}\texttt{(row\_vector\ b,\ matrix\ A)}\newline
The right division of b by a triangular view of A; algebraically
equivalent to the less efficient and stable form \texttt{b\ *\ inverse(tri(A))}, where \texttt{tri(A)} is the lower-triangular portion of A
with the above-diagonal entries set to zero.

\index{{\tt \bfseries mdivide\_right\_tri\_low }!{\tt (matrix B, matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{mdivide\_right\_tri\_low}}\texttt{(matrix\ B,\ matrix\ A)}\newline
The right division of B by a triangular view of A; algebraically
equivalent to the less efficient and stable form \texttt{B\ *\ inverse(tri(A))}, where \texttt{tri(A)} is the lower-triangular portion of A
with the above-diagonal entries set to zero.

\hypertarget{symmetric-positive-definite-matrix-division-functions}{%
\subsection{Symmetric positive-definite matrix division functions}\label{symmetric-positive-definite-matrix-division-functions}}

There are four division functions which are specialized for efficiency
and stability for symmetric positive-definite matrix dividends. If
the matrix dividend argument is not symmetric and positive definite,
these will reject and print warnings.

\index{{\tt \bfseries mdivide\_left\_spd }!{\tt (matrix A, vector b): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{mdivide\_left\_spd}}\texttt{(matrix\ A,\ vector\ b)}\newline
The left division of b by the symmetric, positive-definite matrix A;
algebraically equivalent to the less efficient and stable form
\texttt{inverse(A)\ *\ b}.

\index{{\tt \bfseries mdivide\_left\_spd }!{\tt (matrix A, matrix B): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{mdivide\_left\_spd}}\texttt{(matrix\ A,\ matrix\ B)}\newline
The left division of B by the symmetric, positive-definite matrix A;
algebraically equivalent to the less efficient and stable form
\texttt{inverse(A)\ *\ B}.

\index{{\tt \bfseries mdivide\_right\_spd }!{\tt (row\_vector b, matrix A): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{mdivide\_right\_spd}}\texttt{(row\_vector\ b,\ matrix\ A)}\newline
The right division of b by the symmetric, positive-definite matrix A;
algebraically equivalent to the less efficient and stable form \texttt{b\ *\ inverse(A)}.

\index{{\tt \bfseries mdivide\_right\_spd }!{\tt (matrix B, matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{mdivide\_right\_spd}}\texttt{(matrix\ B,\ matrix\ A)}\newline
The right division of B by the symmetric, positive-definite matrix A;
algebraically equivalent to the less efficient and stable form \texttt{B\ *\ inverse(A)}.

\hypertarget{matrix-exponential}{%
\subsection{Matrix exponential}\label{matrix-exponential}}

The exponential of the matrix \(A\) is formally defined by the
convergent power series: \[ e^A = \sum_{n=0}^{\infty} \dfrac{A^n}{n!}
\]

\index{{\tt \bfseries matrix\_exp }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{matrix\_exp}}\texttt{(matrix\ A)}\newline
The matrix exponential of A

\index{{\tt \bfseries matrix\_exp\_multiply }!{\tt (matrix A, matrix B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{matrix\_exp\_multiply}}\texttt{(matrix\ A,\ matrix\ B)}\newline
The multiplication of matrix exponential of A and matrix B;
algebraically equivalent to the less efficient form \texttt{matrix\_exp(A)\ *\ B}.

\index{{\tt \bfseries scale\_matrix\_exp\_multiply }!{\tt (real t, matrix A, matrix B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{scale\_matrix\_exp\_multiply}}\texttt{(real\ t,\ matrix\ A,\ matrix\ B)}\newline
The multiplication of matrix exponential of tA and matrix B;
algebraically equivalent to the less efficient form \texttt{matrix\_exp(t\ *\ A)\ *\ B}.

\hypertarget{matrix-power}{%
\subsection{Matrix power}\label{matrix-power}}

Returns the nth power of the specific matrix: \[ M^n = M_1 * ... * M_n \]

\index{{\tt \bfseries matrix\_power }!{\tt (matrix A, int B): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{matrix\_power}}\texttt{(matrix\ A,\ int\ B)}\newline
Matrix A raised to the power B.

\hypertarget{linear-algebra-functions}{%
\subsection{Linear algebra functions}\label{linear-algebra-functions}}

\hypertarget{trace}{%
\subsubsection{Trace}\label{trace}}

\index{{\tt \bfseries trace }!{\tt (matrix A): real}|hyperpage}

\texttt{real} \textbf{\texttt{trace}}\texttt{(matrix\ A)}\newline
The trace of A, or 0 if A is empty; A is not required to be diagonal

\hypertarget{determinants}{%
\subsubsection{Determinants}\label{determinants}}

\index{{\tt \bfseries determinant }!{\tt (matrix A): real}|hyperpage}

\texttt{real} \textbf{\texttt{determinant}}\texttt{(matrix\ A)}\newline
The determinant of A

\index{{\tt \bfseries log\_determinant }!{\tt (matrix A): real}|hyperpage}

\texttt{real} \textbf{\texttt{log\_determinant}}\texttt{(matrix\ A)}\newline
The log of the absolute value of the determinant of A

\hypertarget{inverses}{%
\subsubsection{Inverses}\label{inverses}}

It is almost never a good idea to use matrix inverses directly because
they are both inefficient and arithmetically unstable compared to the
alternatives. Rather than inverting a matrix \texttt{m} and post-multiplying
by a vector or matrix \texttt{a}, as in \texttt{inverse(m)\ *\ a}, it is better to
code this using matrix division, as in \texttt{m\ \textbackslash{}\ a}. The
pre-multiplication case is similar, with \texttt{b\ *\ inverse(m)} being more
efficiently coded as as \texttt{b\ /\ m}. There are also useful special cases
for triangular and symmetric, positive-definite matrices that use more
efficient solvers.

\emph{\textbf{Warning:}} The function \texttt{inv(m)} is the elementwise inverse
function, which returns \texttt{1\ /\ m{[}i,\ j{]}} for each element.

\index{{\tt \bfseries inverse }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{inverse}}\texttt{(matrix\ A)}\newline
The inverse of A

\index{{\tt \bfseries inverse\_spd }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{inverse\_spd}}\texttt{(matrix\ A)}\newline
The inverse of A where A is symmetric, positive definite. This version
is faster and more arithmetically stable when the input is symmetric
and positive definite.

\hypertarget{generalized-inverse}{%
\subsubsection{Generalized Inverse}\label{generalized-inverse}}

Computes the generalized inverse of a matrix. For a symmetric matrix \texttt{m}
the output is equivalent to using \texttt{inverse(m)}. When \texttt{m} is
rectangular the \texttt{generalized\_inverse(m)} satisfies \(m m^+ m = m\) where
\(m^+\) is the output of calling \texttt{generalized\_inverse(m)}. Note that the
dimensions of \texttt{generalized\_inverse(m)} are equivalent to the
dimensions of \texttt{transpose(m)}. The \texttt{generalized\_inverse} exists
for any matrix, so the matrix may be singular or less than full rank.

Even though the \texttt{generalized\_inverse} exists for
any arbitrary matrix, the derivatives of this function only exist
on matrices of constant rank. When automatric differentiation is applied
a matrix may change rank in small pertubations of the elements. For example,
let \(A\) and a pertubation matrix \(E\) as,

\[
A = \left( 
    \begin{array}{cccc} 
    1 & 2 & 1 \\
    2 & 4 & 2 
    \end{array} 
    \right)
\: \:
E = \left( 
    \begin{array}{cccc} 
    \epsilon_{11} & \epsilon_{12} & \epsilon_{13} \\
    \epsilon_{21} & \epsilon_{22} & \epsilon_{23}
    \end{array} 
    \right).
\]

\(A\) is of rank 1 since it consists of two linearly dependent columns (and one
linearly dependent row). The number of linear dependent rows and columns changes
as the elements in \(E\) fluctuate and matrix is not of constant rank.

\index{{\tt \bfseries generalized_inverse }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{generalized\_inverse}}\texttt{(matrix\ A)}\newline
The generalized inverse of A

\hypertarget{eigendecomposition}{%
\subsubsection{Eigendecomposition}\label{eigendecomposition}}

\index{{\tt \bfseries eigenvalues\_sym }!{\tt (matrix A): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{eigenvalues\_sym}}\texttt{(matrix\ A)}\newline
The vector of eigenvalues of a symmetric matrix A in ascending order

\index{{\tt \bfseries eigenvectors\_sym }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{eigenvectors\_sym}}\texttt{(matrix\ A)}\newline
The matrix with the (column) eigenvectors of symmetric matrix A in the
same order as returned by the function \texttt{eigenvalues\_sym}

Because multiplying an eigenvector by \(-1\) results in an eigenvector,
eigenvectors returned by a decomposition are only identified up to a
sign change. In order to compare the eigenvectors produced by Stan's
eigendecomposition to others, signs may need to be normalized in some
way, such as by fixing the sign of a component, or doing comparisons
allowing a multiplication by \(-1\).

The condition number of a symmetric matrix is defined to be the ratio
of the largest eigenvalue to the smallest eigenvalue. Large condition
numbers lead to difficulty in numerical algorithms such as computing
inverses, and thus known as ``ill conditioned.'' The ratio can even be
infinite in the case of singular matrices (i.e., those with
eigenvalues of 0).

\hypertarget{QR-decomposition}{%
\subsubsection{QR decomposition}\label{QR-decomposition}}

\index{{\tt \bfseries qr\_thin\_q }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{qr\_thin\_Q}}\texttt{(matrix\ A)}\newline
The orthogonal matrix in the thin QR decomposition of A, which implies
that the resulting matrix has the same dimensions as A

\index{{\tt \bfseries qr\_thin\_r }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{qr\_thin\_R}}\texttt{(matrix\ A)}\newline
The upper triangular matrix in the thin QR decomposition of A, which
implies that the resulting matrix is square with the same number of
columns as A

\index{{\tt \bfseries qr\_q }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{qr\_Q}}\texttt{(matrix\ A)}\newline
The orthogonal matrix in the fat QR decomposition of A, which implies
that the resulting matrix is square with the same number of rows as A

\index{{\tt \bfseries qr\_r }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{qr\_R}}\texttt{(matrix\ A)}\newline
The upper trapezoidal matrix in the fat QR decomposition of A, which
implies that the resulting matrix will be rectangular with the same
dimensions as A

The thin QR decomposition is always preferable because it will consume
much less memory when the input matrix is large than will the fat QR
decomposition. Both versions of the decomposition represent the input
matrix as \[ A = Q \, R. \] Multiplying a column of an orthogonal
matrix by \(-1\) still results in an orthogonal matrix, and you can
multiply the corresponding row of the upper trapezoidal matrix by \(-1\)
without changing the product. Thus, Stan adopts the normalization that
the diagonal elements of the upper trapezoidal matrix are strictly
positive and the columns of the orthogonal matrix are reflected if
necessary. Also, these QR decomposition algorithms do not utilize
pivoting and thus may be numerically unstable on input matrices that
have less than full rank.

\hypertarget{cholesky-decomposition}{%
\subsubsection{Cholesky decomposition}\label{cholesky-decomposition}}

Every symmetric, positive-definite matrix (such as a correlation or
covariance matrix) has a Cholesky decomposition. If \(\Sigma\) is a
symmetric, positive-definite matrix, its Cholesky decomposition is the
lower-triangular vector \(L\) such that \[ \Sigma = L \, L^{\top}. \]

\index{{\tt \bfseries cholesky\_decompose }!{\tt (matrix A): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{cholesky\_decompose}}\texttt{(matrix\ A)}\newline
The lower-triangular Cholesky factor of the symmetric
positive-definite matrix A

\hypertarget{singular-value-decomposition}{%
\subsubsection{Singular value decomposition}\label{singular-value-decomposition}}

Stan only provides functions for the singular values, not for the
singular vectors involved in a singular value decomposition (SVD).

\index{{\tt \bfseries singular\_values }!{\tt (matrix A): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{singular\_values}}\texttt{(matrix\ A)}\newline
The singular values of A in descending order

\hypertarget{sort-functions}{%
\section{Sort functions}\label{sort-functions}}

See the \protect\hyperlink{sorting-functions}{sorting functions section} for examples of how
the functions work.

\index{{\tt \bfseries sort\_asc }!{\tt (vector v): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{sort\_asc}}\texttt{(vector\ v)}\newline
Sort the elements of v in ascending order

\index{{\tt \bfseries sort\_asc }!{\tt (row\_vector v): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{sort\_asc}}\texttt{(row\_vector\ v)}\newline
Sort the elements of v in ascending order

\index{{\tt \bfseries sort\_desc }!{\tt (vector v): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{sort\_desc}}\texttt{(vector\ v)}\newline
Sort the elements of v in descending order

\index{{\tt \bfseries sort\_desc }!{\tt (row\_vector v): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{sort\_desc}}\texttt{(row\_vector\ v)}\newline
Sort the elements of v in descending order

\index{{\tt \bfseries sort\_indices\_asc }!{\tt (vector v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_asc}}\texttt{(vector\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in ascending order.

\index{{\tt \bfseries sort\_indices\_asc }!{\tt (row\_vector v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_asc}}\texttt{(row\_vector\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in ascending order.

\index{{\tt \bfseries sort\_indices\_desc }!{\tt (vector v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_desc}}\texttt{(vector\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in descending order.

\index{{\tt \bfseries sort\_indices\_desc }!{\tt (row\_vector v): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{sort\_indices\_desc}}\texttt{(row\_vector\ v)}\newline
Return an array of indices between 1 and the size of v, sorted to
index v in descending order.

\index{{\tt \bfseries rank }!{\tt (vector v, int s): int}|hyperpage}

\texttt{int} \textbf{\texttt{rank}}\texttt{(vector\ v,\ int\ s)}\newline
Number of components of v less than v{[}s{]}

\index{{\tt \bfseries rank }!{\tt (row\_vector v, int s): int}|hyperpage}

\texttt{int} \textbf{\texttt{rank}}\texttt{(row\_vector\ v,\ int\ s)}\newline
Number of components of v less than v{[}s{]}

\hypertarget{reverse-functions}{%
\section{Reverse functions}\label{reverse-functions}}

\index{{\tt \bfseries reverse }!{\tt (vector v): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{reverse}}\texttt{(vector\ v)}\newline
Return a new vector containing the elements of the argument in reverse order.

\index{{\tt \bfseries reverse }!{\tt (row\_vector v): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{reverse}}\texttt{(row\_vector\ v)}\newline
Return a new row vector containing the elements of the argument in reverse order.

\hypertarget{sparse-matrices}{%
\chapter{Sparse Matrix Operations}\label{sparse-matrices}}

For sparse matrices, for which many elements are zero, it is more
efficient to use specialized representations to save memory and speed
up matrix arithmetic (including derivative calculations). Given
Stan's implementation, there is substantial space (memory) savings by
using sparse matrices. Because of the ease of optimizing dense matrix
operations, speed improvements only arise at 90\% or even greater
sparsity; below that level, dense matrices are faster but use more
memory.

Because of this speedup and space savings, it may even be useful to
read in a dense matrix and convert it to a sparse matrix before
multiplying it by a vector. This chapter covers a very specific form
of sparsity consisting of a sparse matrix multiplied by a dense
vector.

\hypertarget{CSR}{%
\section{Compressed row storage}\label{CSR}}

Sparse matrices are represented in Stan using compressed row storage
(CSR). For example, the matrix \[ A = \begin{bmatrix} 19 & 27 & 0 & 0
\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 52 \\ 81 & 0 & 95 & 33 \end{bmatrix}
\] is translated into a vector of the non-zero real values, read by
row from the matrix \(A\), \[ w(A) = \begin{bmatrix} 19 & 27 & 52 & 81 &
95 & 33 \end{bmatrix}^{\top} \! \! \! , \] an array of integer column
indices for the values, \[ v(A) = \begin{bmatrix} 1 & 2 & 4 & 1 & 3 &
4 \end{bmatrix} \! , \] and an array of integer indices indicating
where in \(w(A)\) a given row's values start, \[ u(A) = \begin{bmatrix}
1 & 3 & 3 & 4 & 7 \end{bmatrix} \! , \] with a padded value at the end
to guarantee that \[ u(A)[n+1] - u(A)[n] \] is the number of non-zero
elements in row \(n\) of the matrix (here \(2\), \(0\), \(1\), and \(3\)). Note
that because the second row has no non-zero elements both the second
and third elements of \(u(A)\) correspond to the third element of
\(w(A)\), which is \(52\). The values \((w(A), \, v(A), \, u(A))\) are
sufficient to reconstruct \(A\).

The values are structured so that there is a real value and integer
column index for each non-zero entry in the array, plus one integer
for each row of the matrix, plus one for padding. There is also
underlying storage for internal container pointers and sizes. The
total memory usage is roughly \(12 K + M\) bytes plus a small constant
overhead, which is often considerably fewer bytes than the \(M \times N\) required to store a dense matrix. Even more importantly, zero
values do not introduce derivatives under multiplication or addition,
so many storage and evaluation steps are saved when sparse matrices
are multiplied.

\hypertarget{conversion-functions}{%
\section{Conversion functions}\label{conversion-functions}}

Conversion functions between dense and sparse matrices are provided.

\hypertarget{dense-to-sparse-conversion}{%
\subsection{Dense to sparse conversion}\label{dense-to-sparse-conversion}}

Converting a dense matrix \(m\) to a sparse representation produces a
vector \(w\) and two integer arrays, \(u\) and \(v\).

\index{{\tt \bfseries csr\_extract\_w }!{\tt (matrix a): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{csr\_extract\_w}}\texttt{(matrix\ a)}\newline
Return non-zero values in matrix a; see section \protect\hyperlink{CSR}{compressed row storage}.

\index{{\tt \bfseries csr\_extract\_v }!{\tt (matrix a): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{csr\_extract\_v}}\texttt{(matrix\ a)}\newline
Return column indices for values in \texttt{csr\_extract\_w(a)}; see
\protect\hyperlink{CSR}{compressed row storage}.

\index{{\tt \bfseries csr\_extract\_u }!{\tt (matrix a): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{csr\_extract\_u}}\texttt{(matrix\ a)}\newline
Return array of row starting indices for entries in \texttt{csr\_extract\_w(a)}
followed by the size of \texttt{csr\_extract\_w(a)} plus one; see section
\protect\hyperlink{CSR}{compressed row storage}.

\hypertarget{sparse-to-dense-conversion}{%
\subsection{Sparse to dense conversion}\label{sparse-to-dense-conversion}}

To convert a sparse matrix representation to a dense matrix, there is
a single function.

\index{{\tt \bfseries csr\_to\_dense\_matrix }!{\tt (int m, int n, vector w, int[] v, int[] u): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{csr\_to\_dense\_matrix}}\texttt{(int\ m,\ int\ n,\ vector\ w,\ int{[}{]}\ v,\ int{[}{]}\ u)}\newline
Return dense \(\text{m} \times \text{n}\) matrix with non-zero matrix
entries w, column indices v, and row starting indices u; the vector w
and array v must be the same size (corresponding to the total number of
nonzero entries in the matrix), array v must have index values bounded
by m, array u must have length equal to m + 1 and contain index values
bounded by the number of nonzeros (except for the last entry, which must
be equal to the number of nonzeros plus one). See section
\protect\hyperlink{CSR}{compressed row storage} for more details.

\hypertarget{sparse-matrix-arithmetic}{%
\section{Sparse matrix arithmetic}\label{sparse-matrix-arithmetic}}

\hypertarget{sparse-matrix-multiplication}{%
\subsection{Sparse matrix multiplication}\label{sparse-matrix-multiplication}}

The only supported operation is the multiplication of a sparse matrix
\(A\) and a dense vector \(b\) to produce a dense vector \(A\,b\).
Multiplying a dense row vector \(b\) and a sparse matrix \(A\) can be
coded using transposition as \[ b \, A = (A^{\top} \,
b^{\top})^{\top}, \] but care must be taken to represent \(A^{\top}\)
rather than \(A\) as a sparse matrix.

\index{{\tt \bfseries csr\_matrix\_times\_vector }!{\tt (int m, int n, vector w, int[] v, int[] u, vector b): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{csr\_matrix\_times\_vector}}\texttt{(int\ m,\ int\ n,\ vector\ w,\ int{[}{]}\ v,\ int{[}{]}\ u,\ vector\ b)}\newline
Multiply the \(\text{m} \times \text{n}\) matrix represented by values
w, column indices v, and row start indices u by the vector b; see
\protect\hyperlink{CSR}{compressed row storage}.

\hypertarget{mixed-operations}{%
\chapter{Mixed Operations}\label{mixed-operations}}

These functions perform conversions between Stan containers matrix,
vector, row vector and arrays.

\index{{\tt \bfseries to\_matrix }!{\tt (matrix m): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(matrix\ m)}\newline
Return the matrix m itself.

\index{{\tt \bfseries to\_matrix }!{\tt (vector v): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(vector\ v)}\newline
Convert the column vector v to a \texttt{size(v)} by 1 matrix.

\index{{\tt \bfseries to\_matrix }!{\tt (row\_vector v): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(row\_vector\ v)}\newline
Convert the row vector v to a 1 by \texttt{size(v)} matrix.

\index{{\tt \bfseries to\_matrix }!{\tt (matrix m, int m, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(matrix\ m,\ int\ m,\ int\ n)}\newline
Convert a matrix m to a matrix with m rows and n columns filled in
column-major order.

\index{{\tt \bfseries to\_matrix }!{\tt (vector v, int m, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(vector\ v,\ int\ m,\ int\ n)}\newline
Convert a vector v to a matrix with m rows and n columns filled in
column-major order.

\index{{\tt \bfseries to\_matrix }!{\tt (row\_vector v, int m, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(row\_vector\ v,\ int\ m,\ int\ n)}\newline
Convert a row\_vector a to a matrix with m rows and n columns filled in
column-major order.

\index{{\tt \bfseries to\_matrix }!{\tt (matrix m, int m, int n, int col\_major): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(matrix\ m,\ int\ m,\ int\ n,\ int\ col\_major)}\newline
Convert a matrix m to a matrix with m rows and n columns filled in
row-major order if col\_major equals 0 (otherwise, they get filled in
column-major order).

\index{{\tt \bfseries to\_matrix }!{\tt (vector v, int m, int n, int col\_major): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(vector\ v,\ int\ m,\ int\ n,\ int\ col\_major)}\newline
Convert a vector v to a matrix with m rows and n columns filled in
row-major order if col\_major equals 0 (otherwise, they get filled in
column-major order).

\index{{\tt \bfseries to\_matrix }!{\tt (row\_vector v, int m, int n, int col\_major): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(row\_vector\ v,\ int\ m,\ int\ n,\ int\ col\_major)}\newline
Convert a row\_vector a to a matrix with m rows and n columns filled in
row-major order if col\_major equals 0 (otherwise, they get filled in
column-major order).

\index{{\tt \bfseries to\_matrix }!{\tt (real[] a, int m, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(real{[}{]}\ a,\ int\ m,\ int\ n)}\newline
Convert a one-dimensional array a to a matrix with m rows and n
columns filled in column-major order.

\index{{\tt \bfseries to\_matrix }!{\tt (int[] a, int m, int n): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(int{[}{]}\ a,\ int\ m,\ int\ n)}\newline
Convert a one-dimensional array a to a matrix with m rows and n
columns filled in column-major order.

\index{{\tt \bfseries to\_matrix }!{\tt (real[] a, int m, int n, int col\_major): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(real{[}{]}\ a,\ int\ m,\ int\ n,\ int\ col\_major)}\newline
Convert a one-dimensional array a to a matrix with m rows and n
columns filled in row-major order if col\_major equals 0 (otherwise,
they get filled in column-major order).

\index{{\tt \bfseries to\_matrix }!{\tt (int[] a, int m, int n, int col\_major): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(int{[}{]}\ a,\ int\ m,\ int\ n,\ int\ col\_major)}\newline
Convert a one-dimensional array a to a matrix with m rows and n
columns filled in row-major order if col\_major equals 0 (otherwise,
they get filled in column-major order).

\index{{\tt \bfseries to\_matrix }!{\tt (real[,] a): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(real{[},{]}\ a)}\newline
Convert the two dimensional array a to a matrix with the same
dimensions and indexing order.

\index{{\tt \bfseries to\_matrix }!{\tt (int[,] a): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{to\_matrix}}\texttt{(int{[},{]}\ a)}\newline
Convert the two dimensional array a to a matrix with the same
dimensions and indexing order. If any of the dimensions of a are zero,
the result will be a \(0 \times 0\) matrix.

\index{{\tt \bfseries to\_vector }!{\tt (matrix m): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{to\_vector}}\texttt{(matrix\ m)}\newline
Convert the matrix m to a column vector in column-major order.

\index{{\tt \bfseries to\_vector }!{\tt (vector v): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{to\_vector}}\texttt{(vector\ v)}\newline
Return the column vector v itself.

\index{{\tt \bfseries to\_vector }!{\tt (row\_vector v): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{to\_vector}}\texttt{(row\_vector\ v)}\newline
Convert the row vector v to a column vector.

\index{{\tt \bfseries to\_vector }!{\tt (real[] a): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{to\_vector}}\texttt{(real{[}{]}\ a)}\newline
Convert the one-dimensional array a to a column vector.

\index{{\tt \bfseries to\_vector }!{\tt (int[] a): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{to\_vector}}\texttt{(int{[}{]}\ a)}\newline
Convert the one-dimensional integer array a to a column vector.

\index{{\tt \bfseries to\_row\_vector }!{\tt (matrix m): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{to\_row\_vector}}\texttt{(matrix\ m)}\newline
Convert the matrix m to a row vector in column-major order.

\index{{\tt \bfseries to\_row\_vector }!{\tt (vector v): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{to\_row\_vector}}\texttt{(vector\ v)}\newline
Convert the column vector v to a row vector.

\index{{\tt \bfseries to\_row\_vector }!{\tt (row\_vector v): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{to\_row\_vector}}\texttt{(row\_vector\ v)}\newline
Return the row vector v itself.

\index{{\tt \bfseries to\_row\_vector }!{\tt (real[] a): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{to\_row\_vector}}\texttt{(real{[}{]}\ a)}\newline
Convert the one-dimensional array a to a row vector.

\index{{\tt \bfseries to\_row\_vector }!{\tt (int[] a): row\_vector}|hyperpage}

\texttt{row\_vector} \textbf{\texttt{to\_row\_vector}}\texttt{(int{[}{]}\ a)}\newline
Convert the one-dimensional array a to a row vector.

\index{{\tt \bfseries to\_array\_2d }!{\tt (matrix m): real[,]}|hyperpage}

\texttt{real{[},{]}} \textbf{\texttt{to\_array\_2d}}\texttt{(matrix\ m)}\newline
Convert the matrix m to a two dimensional array with the same
dimensions and indexing order.

\index{{\tt \bfseries to\_array\_1d }!{\tt (vector v): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{to\_array\_1d}}\texttt{(vector\ v)}\newline
Convert the column vector v to a one-dimensional array.

\index{{\tt \bfseries to\_array\_1d }!{\tt (row\_vector v): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{to\_array\_1d}}\texttt{(row\_vector\ v)}\newline
Convert the row vector v to a one-dimensional array.

\index{{\tt \bfseries to\_array\_1d }!{\tt (matrix m): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{to\_array\_1d}}\texttt{(matrix\ m)}\newline
Convert the matrix m to a one-dimensional array in column-major order.

\index{{\tt \bfseries to\_array\_1d }!{\tt (real[...] a): real[]}|hyperpage}

\texttt{real{[}{]}} \textbf{\texttt{to\_array\_1d}}\texttt{(real{[}...{]}\ a)}\newline
Convert the array a (of any dimension up to 10) to a one-dimensional
array in row-major order.

\index{{\tt \bfseries to\_array\_1d }!{\tt (int[...] a): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{to\_array\_1d}}\texttt{(int{[}...{]}\ a)}\newline
Convert the array a (of any dimension up to 10) to a one-dimensional
array in row-major order.

\hypertarget{compound-arithmetic-and-assignment}{%
\chapter{Compound Arithmetic and Assignment}\label{compound-arithmetic-and-assignment}}

Compound arithmetic and assignment statements combine an arithmetic
operation and assignment,

\begin{verbatim}
 x = x op y;
\end{verbatim}

replacing them with the compound form

\begin{verbatim}
 x op= y;
\end{verbatim}

For example, \texttt{x\ =\ x\ +\ 1} may be replaced with \texttt{x\ +=\ 1}.

The signatures of the supported compound arithmetic and assignment
operations are as follows.

\hypertarget{compound-addition-and-assignment}{%
\section{Compound addition and assignment}\label{compound-addition-and-assignment}}

\index{{\tt \bfseries operator\_compound\_add }!{\tt (int x, int y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(int\ x,\ int\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (real x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(real\ x,\ real\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(vector\ x,\ real\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (row\_vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(row\_vector\ x,\ real\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (matrix x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(matrix\ x,\ real\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (vector x, vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(vector\ x,\ vector\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (row\_vector x, row\_vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\index{{\tt \bfseries operator\_compound\_add }!{\tt (matrix x, matrix y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator+=}}\texttt{(matrix\ x,\ matrix\ y)}\newline
\texttt{x\ +=\ y} is equivalent to \texttt{x\ =\ x\ +\ y}.

\hypertarget{compound-subtraction-and-assignment}{%
\section{Compound subtraction and assignment}\label{compound-subtraction-and-assignment}}

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (int x, int y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(int\ x,\ int\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (real x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(real\ x,\ real\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(vector\ x,\ real\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (row\_vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(row\_vector\ x,\ real\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (matrix x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(matrix\ x,\ real\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (vector x, vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(vector\ x,\ vector\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (row\_vector x, row\_vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\index{{\tt \bfseries operator\_compound\_subtract }!{\tt (matrix x, matrix y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator-=}}\texttt{(matrix\ x,\ matrix\ y)}\newline
\texttt{x\ -=\ y} is equivalent to \texttt{x\ =\ x\ -\ y}.

\hypertarget{compound-multiplication-and-assignment}{%
\section{Compound multiplication and assignment}\label{compound-multiplication-and-assignment}}

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (int x, int y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(int\ x,\ int\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (real x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(real\ x,\ real\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(vector\ x,\ real\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (row\_vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(row\_vector\ x,\ real\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (matrix x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(matrix\ x,\ real\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (row\_vector x, matrix y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(row\_vector\ x,\ matrix\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\index{{\tt \bfseries operator\_compound\_multiply }!{\tt (matrix x, matrix y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator*=}}\texttt{(matrix\ x,\ matrix\ y)}\newline
\texttt{x\ *=\ y} is equivalent to \texttt{x\ =\ x\ *\ y}.

\hypertarget{compound-division-and-assignment}{%
\section{Compound division and assignment}\label{compound-division-and-assignment}}

\index{{\tt \bfseries operator\_compound\_divide }!{\tt (int x, int y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator/=}}\texttt{(int\ x,\ int\ y)}\newline
\texttt{x\ /=\ y} is equivalent to \texttt{x\ =\ x\ /\ y}.

\index{{\tt \bfseries operator\_compound\_divide }!{\tt (real x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator/=}}\texttt{(real\ x,\ real\ y)}\newline
\texttt{x\ /=\ y} is equivalent to \texttt{x\ =\ x\ /\ y}.

\index{{\tt \bfseries operator\_compound\_divide }!{\tt (vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator/=}}\texttt{(vector\ x,\ real\ y)}\newline
\texttt{x\ /=\ y} is equivalent to \texttt{x\ =\ x\ /\ y}.

\index{{\tt \bfseries operator\_compound\_divide }!{\tt (row\_vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator/=}}\texttt{(row\_vector\ x,\ real\ y)}\newline
\texttt{x\ /=\ y} is equivalent to \texttt{x\ =\ x\ /\ y}.

\index{{\tt \bfseries operator\_compound\_divide }!{\tt (matrix x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator/=}}\texttt{(matrix\ x,\ real\ y)}\newline
\texttt{x\ /=\ y} is equivalent to \texttt{x\ =\ x\ /\ y}.

\hypertarget{compound-elementwise-multiplication-and-assignment}{%
\section{Compound elementwise multiplication and assignment}\label{compound-elementwise-multiplication-and-assignment}}

\index{{\tt \bfseries operator\_compound\_elt\_multiply }!{\tt (vector x, vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator.*=}}\texttt{(vector\ x,\ vector\ y)}\newline
\texttt{x\ .*=\ y} is equivalent to \texttt{x\ =\ x\ .*\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_multiply }!{\tt (row\_vector x, row\_vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator.*=}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
\texttt{x\ .*=\ y} is equivalent to \texttt{x\ =\ x\ .*\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_multiply }!{\tt (matrix x, matrix y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator.*=}}\texttt{(matrix\ x,\ matrix\ y)}\newline
\texttt{x\ .*=\ y} is equivalent to \texttt{x\ =\ x\ .*\ y}.

\hypertarget{compound-elementwise-division-and-assignment}{%
\section{Compound elementwise division and assignment}\label{compound-elementwise-division-and-assignment}}

\index{{\tt \bfseries operator\_compound\_elt\_divide }!{\tt (vector x, vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator./=}}\texttt{(vector\ x,\ vector\ y)}\newline
\texttt{x\ ./=\ y} is equivalent to \texttt{x\ =\ x\ ./\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_divide }!{\tt (row\_vector x, row\_vector y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator./=}}\texttt{(row\_vector\ x,\ row\_vector\ y)}\newline
\texttt{x\ ./=\ y} is equivalent to \texttt{x\ =\ x\ ./\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_divide }!{\tt (matrix x, matrix y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator./=}}\texttt{(matrix\ x,\ matrix\ y)}\newline
\texttt{x\ ./=\ y} is equivalent to \texttt{x\ =\ x\ ./\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_divide }!{\tt (vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator./=}}\texttt{(vector\ x,\ real\ y)}\newline
\texttt{x\ ./=\ y} is equivalent to \texttt{x\ =\ x\ ./\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_divide }!{\tt (row\_vector x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator./=}}\texttt{(row\_vector\ x,\ real\ y)}\newline
\texttt{x\ ./=\ y} is equivalent to \texttt{x\ =\ x\ ./\ y}.

\index{{\tt \bfseries operator\_compound\_elt\_divide }!{\tt (matrix x, real y): void}|hyperpage}

\texttt{void} \textbf{\texttt{operator./=}}\texttt{(matrix\ x,\ real\ y)}\newline
\texttt{x\ ./=\ y} is equivalent to \texttt{x\ =\ x\ ./\ y}.

\hypertarget{higher-order-functions}{%
\chapter{Higher-Order Functions}\label{higher-order-functions}}

Stan provides a few higher-order functions that act on other
functions. In all cases, the function arguments to the higher-order
functions are defined as functions within the Stan language and passed
by name to the higher-order functions.

\hypertarget{functions-algebraic-solver}{%
\section{Algebraic equation solver}\label{functions-algebraic-solver}}

Stan provides two built-in algebraic equation solvers,
respectively based on Powell's and Newton's methods.
The Newton method constitutes a more recent addition to Stan;
its use is recommended for most problems.
Although they look like other function applications,
algebraic solvers are special in two ways.

First, an algebraic solver is a higher-order function, i.e.~it takes
another function as one of its arguments. Other functions in
Stan which share this feature are the ordinary differential equation
solvers (see section \protect\hyperlink{functions-ode-solver}{Ordinary Differential Equation (ODE) Solvers}).
Ordinary Stan functions do not allow functions as arguments.

Second, some of the arguments of the algebraic solvers are restricted
to data only expressions. These expressions must not contain variables
other than those declared in the data or transformed data blocks.
Ordinary Stan functions place no restriction on the origin of
variables in their argument expressions.

\hypertarget{specifying-an-algebraic-equation-as-a-function}{%
\subsection{Specifying an algebraic equation as a function}\label{specifying-an-algebraic-equation-as-a-function}}

An algebraic system is specified as an ordinary function in Stan
within the function block. The algebraic system function must have
this signature:

\begin{verbatim}
 vector algebra_system(vector y, vector theta,
                              real[] x_r, int[] x_i)
\end{verbatim}

The algebraic system function should return the value of the algebraic
function which goes to 0, when we plug in the solution to the
algebraic system.

The argument of this function are:

\begin{itemize}
\item
  \emph{\texttt{y}}, the unknowns we wish to solve for
\item
  \emph{\texttt{theta}}, parameter values used to evaluate the algebraic system
\item
  \emph{\texttt{x\_r}}, data values used to evaluate the algebraic system
\item
  \emph{\texttt{x\_i}}, integer data used to evaluate the algebraic system
\end{itemize}

The algebraic system function separates parameter values, \emph{\texttt{theta}},
from data values, \emph{\texttt{x\_r}}, for efficiency in propagating the derivatives
through the algebraic system.

\hypertarget{call-to-the-algebraic-solver}{%
\subsection{Call to the algebraic solver}\label{call-to-the-algebraic-solver}}

\index{{\tt \bfseries algebra\_solver }!{\tt (function algebra\_system, vector y\_guess, vector theta, real[] x\_r, int[] x\_i): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{algebra\_solver}}\texttt{(function\ algebra\_system,\ vector\ y\_guess,\ vector\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i)}\newline
Solves the algebraic system, given an initial guess, using the Powell
hybrid algorithm.

\index{{\tt \bfseries algebra\_solver }!{\tt (function algebra\_system, vector y\_guess, vector theta, real[] x\_r, int[] x\_i, real rel\_tol, real f\_tol, int max\_steps): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{algebra\_solver}}\texttt{(function\ algebra\_system,\ vector\ y\_guess,\ vector\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i,\ real\ rel\_tol,\ real\ f\_tol,\ int\ max\_steps)}\newline
Solves the algebraic system, given an initial guess, using the Powell
hybrid algorithm with additional control parameters for the solver.

\emph{Note:} In future releases, the function \texttt{algebra\_solver} will be deprecated
and replaced with \texttt{algebra\_solver\_powell}.

\index{{\tt \bfseries algebra\_solver\_newton }!{\tt (function algebra\_system, vector y\_guess, vector theta, real[] x\_r, int[] x\_i): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{algebra\_solver\_newton}}\texttt{(function\ algebra\_system,\ vector\ y\_guess,\ vector\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i)}\newline
Solves the algebraic system, given an initial guess, using Newton's method.

\index{{\tt \bfseries algebra\_solver\_newton }!{\tt (function algebra\_system, vector y\_guess, vector theta, real[] x\_r, int[] x\_i, real rel\_tol, real f\_tol, int max\_steps): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{algebra\_solver\_newton}}\texttt{(function\ algebra\_system,\ vector\ y\_guess,\ vector\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i,\ real\ rel\_tol,\ real\ f\_tol,\ int\ max\_steps)}\newline
Solves the algebraic system, given an initial guess, using Newton's method
with additional control parameters for the solver.

\hypertarget{arguments-to-the-algebraic-solver}{%
\subsubsection{Arguments to the algebraic solver}\label{arguments-to-the-algebraic-solver}}

The arguments to the algebraic solvers are as follows:

\begin{itemize}
\item
  \emph{\texttt{algebra\_system}}: function literal referring to a function
  specifying the system of algebraic equations with signature
  \texttt{(vector,\ vector,\ real{[}{]},\ int{[}{]}):vector}. The arguments represent (1)
  unknowns, (2) parameters, (3) real data, and (4) integer data,
  and the return value contains the value of the algebraic function,
  which goes to 0 when we plug in the solution to the algebraic system,
\item
  \emph{\texttt{y\_guess}}: initial guess for the solution, type \texttt{vector},
\item
  \emph{\texttt{theta}}: parameters only, type \texttt{vector},
\item
  \emph{\texttt{x\_r}}: real data only, type \texttt{real{[}{]}}, and
\item
  \emph{\texttt{x\_i}}: integer data only, type \texttt{int{[}{]}}.
\end{itemize}

For more fine-grained control of the algebraic solver, these
parameters can also be provided:

\begin{itemize}
\item
  \emph{\texttt{rel\_tol}}: relative tolerance for the algebraic solver, type
  \texttt{real}, data only,
\item
  \emph{\texttt{function\_tol}}: function tolerance for the algebraic solver,
  type \texttt{real}, data only,
\item
  \emph{\texttt{max\_num\_steps}}: maximum number of steps to take in the
  algebraic solver, type \texttt{int}, data only.
\end{itemize}

\hypertarget{return-value}{%
\subsubsection{Return value}\label{return-value}}

The return value for the algebraic solver is an object of type
\texttt{vector}, with values which, when plugged in as \texttt{y} make the algebraic
function go to 0.

\hypertarget{sizes-and-parallel-arrays}{%
\subsubsection{Sizes and parallel arrays}\label{sizes-and-parallel-arrays}}

Certain sizes have to be consistent. The initial guess, return value
of the solver, and return value of the algebraic function must all be
the same size.

The parameters, real data, and integer data will be passed from the
solver directly to the system function.

\hypertarget{algorithmic-details}{%
\subsubsection{Algorithmic details}\label{algorithmic-details}}

Stan offers two algebraic solvers: \texttt{algebra\_solver} and \texttt{algebra\_solver\_newton}.
\texttt{algebra\_solver} is baed on the Powell hybrid method (\protect\hyperlink{ref-Powell:1970}{Powell 1970}),
which in turn uses first-order derivatives. The Stan code builds on
the implementation of the hybrid solver in the unsupported module for
nonlinear optimization problems of the Eigen library (\protect\hyperlink{ref-Eigen:2013}{Guennebaud, Jacob, and others 2010}).
This solver is in turn based on the algorithm developed for the
package MINPACK-1 (\protect\hyperlink{ref-minpack:1980}{Jorge J. More 1980}).

\texttt{algebra\_solver\_newton}, uses Newton's method,
also a first-order derivative based numerical solver.
The Stan code builds on the implementation in KINSOL
from the SUNDIALS suite (\protect\hyperlink{ref-Hindmarsh:2005}{Hindmarsh et al. 2005}).
For many problems, we find that \texttt{algebra\_solver\_newton} is faster
than Powell's method.
If however Newton's method performs poorly, either failing to or requiring an excessively
long time to converge, the user should be prepared to switch
to \texttt{algebra\_solver}.

For both solvers, the Jacobian of the solution
with respect to auxiliary parameters is
computed using the implicit function theorem. Intermediate Jacobians
(of the algebraic function's output with respect to the unknowns y
and with respect to the auxiliary parameters theta) are computed using
Stan's automatic differentiation.

\hypertarget{functions-ode-solver}{%
\section{Ordinary differential equation (ODE) solvers}\label{functions-ode-solver}}

Stan provides several higher order functions for solving initial value
problems specified as Ordinary Differential Equations (ODEs).

Solving an initial value ODE means given a set of differential equations
\(y'(t, \theta) = f(t, y, \theta)\) and initial conditions \(y(t_0, \theta)\),
solving for \(y\) at a sequence of times \(t_0 < t_1 \leq t_2, \cdots \leq t_n\).
\(f(t, y, \theta)\) is referred to here as the ODE system function.

\(f(t, y, \theta)\) will be defined as a function with a certain signature
and provided along with the initial conditions and output times to one of the
ODE solver functions.

To make it easier to write ODEs, the solve functions take extra arguments
that are passed along unmodified to the user-supplied system function.
Because there can be any number of these arguments and they can be of different types,
they are denoted below as \texttt{...}. The types of the arguments represented by \texttt{...}
in the ODE solve function call must match the types of the arguments represented by
\texttt{...} in the user-supplied system function.

\hypertarget{non-stiff-solver}{%
\subsection{Non-stiff solver}\label{non-stiff-solver}}

\index{{\tt \bfseries ode\_rk45 }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, ...): vector[]}|hyperpage}

\texttt{vector{[}{]}} \textbf{\texttt{ode\_rk45}}\texttt{(function\ ode,\ vector\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ ...)}\newline
Solves the ODE system for the times provided using the Dormand-Prince
algorithm, a 4th/5th order Runge-Kutta method.

\index{{\tt \bfseries ode\_rk45\_tol }!{\tt (function ode, vector initial\_state, real initial\_time, real[] times, real rel\_tol, real abs\_tol, int max\_num\_steps, ...): vector[]}|hyperpage}

\texttt{vector{[}{]}} \textbf{\texttt{ode\_rk45\_tol}}\texttt{(function\ ode,\ vector\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real\ rel\_tol,\ real\ abs\_tol,\ int\ max\_num\_steps,\ ...)}\newline
Solves the ODE system for the times provided using the Dormand-Prince
algorithm, a 4th/5th order Runge-Kutta method with additional control
parameters for the solver.

\index{{\tt \bfseries ode\_adams }!{\tt (function ode, vector initial\_state, real initial\_time, real[] times, ...): vector[]}|hyperpage}

\texttt{vector{[}{]}} \textbf{\texttt{ode\_adams}}\texttt{(function\ ode,\ vector\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ ...)}\newline
Solves the ODE system for the times provided using the Adams-Moulton method.

\index{{\tt \bfseries ode\_adams\_tol }!{\tt (function ode, vector initial\_state, real initial\_time, real[] times, data real rel\_tol, data real abs\_tol, data int max\_num\_steps, ...): vector[]}|hyperpage}

\texttt{vector{[}{]}} \textbf{\texttt{ode\_adams\_tol}}\texttt{(function\ ode,\ vector\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ data\ real\ rel\_tol,\ data\ real\ abs\_tol,\ data\ int\ max\_num\_steps,\ ...)}\newline
Solves the ODE system for the times provided using the Adams-Moulton
method with additional control parameters for the solver.

\hypertarget{stiff-solver}{%
\subsection{Stiff solver}\label{stiff-solver}}

\index{{\tt \bfseries ode\_bdf }!{\tt (function ode, vector initial\_state, real initial\_time, real[] times, ...): vector[]}|hyperpage}

\texttt{vector{[}{]}} \textbf{\texttt{ode\_bdf}}\texttt{(function\ ode,\ vector\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ ...)}\newline
Solves the ODE system for the times provided using the backward differentiation
formula (BDF) method.

\index{{\tt \bfseries ode\_bdf\_tol }!{\tt (function ode, vector initial\_state, real initial\_time, real[] times, data real rel\_tol, data real abs\_tol, data int max\_num\_steps, ...): vector[]}|hyperpage}

\texttt{vector{[}{]}} \textbf{\texttt{ode\_bdf\_tol}}\texttt{(function\ ode,\ vector\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ data\ real\ rel\_tol,\ data\ real\ abs\_tol,\ data\ int\ max\_num\_steps,\ ...)}\newline
Solves the ODE system for the times provided using the backward differentiation
formula (BDF) method with additional control parameters for the solver.

\hypertarget{ode-system-function}{%
\subsection{ODE system function}\label{ode-system-function}}

The first argument to one of the ODE solvers is always the ODE system
function. The ODE system function must have a \texttt{vector} return type, and the
first two arguments must be a \texttt{real} and \texttt{vector} in that order. These two
arguments are followed by the variadic arguments that are passed through from
the ODE solve function call:

\begin{verbatim}
 vector ode(real time, vector state, ...)
\end{verbatim}

The ODE system function should return the derivative of the state with
respect to time at the time and state provided. The length of the returned
vector must match the length of the state input into the function.

The arguments to this function are:

\begin{itemize}
\item
  \emph{\texttt{time}}, the time to evaluate the ODE system
\item
  \emph{\texttt{state}}, the state of the ODE system at the time specified
\item
  \emph{\texttt{...}}, sequence of arguments passed unmodified from the ODE solve
  function call. The types here must match the types in the \texttt{...} arguments of the
  ODE solve function call.
\end{itemize}

\hypertarget{arguments-to-the-ode-solvers}{%
\subsection{Arguments to the ODE solvers}\label{arguments-to-the-ode-solvers}}

The arguments to the ODE solvers in both the stiff and non-stiff solvers are the
same.

\begin{itemize}
\item
  \emph{\texttt{ode}}: ODE system function,
\item
  \emph{\texttt{initial\_state}}: initial state, type \texttt{vector},
\item
  \emph{\texttt{initial\_time}}: initial time, type \texttt{int} or \texttt{real},
\item
  \emph{\texttt{times}}: solution times, type \texttt{real{[}{]}},
\item
  \emph{\texttt{...}}: sequence of arguments that will be passed through unmodified
  to the ODE system function. The types here must match the types in the \texttt{...}
  arguments of the ODE system function.
\end{itemize}

For the versions of the ode solver functions ending in \texttt{\_tol}, these three
parameters must be provided after \texttt{times} and before the \texttt{...} arguments:

\begin{itemize}
\item
  \texttt{data} \emph{\texttt{rel\_tol}}: relative tolerance for the ODE solver, type \texttt{real},
  data only,
\item
  \texttt{data} \emph{\texttt{abs\_tol}}: absolute tolerance for the ODE solver, type \texttt{real},
  data only, and
\item
  \texttt{data} \emph{\texttt{max\_num\_steps}}: maximum number of steps to take between output
  times in the ODE solver, type \texttt{int}, data only.
\end{itemize}

Because these are all \texttt{data} arguments, they must be defined in either the data
or transformed data blocks. They cannot be parameters, transformed parameters
or functions of parameters or transformed parameters.

\hypertarget{return-values}{%
\subsubsection{Return values}\label{return-values}}

The return value for the ODE solvers is an array of vectors (type \texttt{vector{[}{]}}),
one vector representing the state of the system at every time in specified in
the \texttt{times} argument.

\hypertarget{array-and-vector-sizes}{%
\subsubsection{Array and vector sizes}\label{array-and-vector-sizes}}

The sizes must match, and in particular, the following groups are of
the same size:

\begin{itemize}
\item
  state variables passed into the system function, derivatives
  returned by the system function, initial state passed into the
  solver, and length of each vector in the output,
\item
  number of solution times and number of vectors in the output,
\end{itemize}

\hypertarget{functions-1d-integrator}{%
\section{1D integrator}\label{functions-1d-integrator}}

Stan provides a built-in mechanism to perform 1D integration of a function via quadrature methods.

It operates similarly to the \protect\hyperlink{functions-algebraic-solver}{algebraic solver} and
the \protect\hyperlink{functions-ode-solver}{ordinary differential equations solver} in that it allows as an argument a function.

Like both of those utilities, some of the arguments are limited
to data only expressions. These expressions must not contain variables
other than those declared in the data or transformed data blocks.

\hypertarget{specifying-an-integrand-as-a-function}{%
\subsection{Specifying an integrand as a function}\label{specifying-an-integrand-as-a-function}}

Performing a 1D integration requires the integrand to be specified somehow.
This is done by defining a function in the Stan functions block with the special signature:

\begin{verbatim}
real integrand(real x, real xc, real[] theta,
                      real[] x_r, int[] x_i)
\end{verbatim}

The function should return the value of the integrand evaluated at
the point x.

The argument of this function are:

\begin{itemize}
\item
  \emph{\texttt{x}}, the independent variable being integrated over
\item
  \emph{\texttt{xc}}, a high precision version of the distance from x to the nearest endpoint in a definite integral (for more into see section \protect\hyperlink{precision-loss}{Precision Loss}).
\item
  \emph{\texttt{theta}}, parameter values used to evaluate the integral
\item
  \emph{\texttt{x\_r}}, data values used to evaluate the integral
\item
  \emph{\texttt{x\_i}}, integer data used to evaluate the integral
\end{itemize}

Like algebraic solver and the differential equations solver, the 1D
integrator separates parameter values, \texttt{theta}, from data values, \texttt{x\_r}.

\hypertarget{call-to-the-1d-integrator}{%
\subsection{Call to the 1D integrator}\label{call-to-the-1d-integrator}}

\index{{\tt \bfseries integrate\_1d }!{\tt (function integrand, real a, real b, real[] theta, real[] x\_r, int[] x\_i): real}|hyperpage}

\texttt{real} \textbf{\texttt{integrate\_1d}} \texttt{(function\ integrand,\ real\ a,\ real\ b,\ real{[}{]}\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i)}\newline
Integrates the integrand from a to b.

\index{{\tt \bfseries integrate\_1d }!{\tt (function integrand, real a, real b, real[] theta, real[] x\_r, int[] x\_i, real relative\_tolerance): real}|hyperpage}

\texttt{real} \textbf{\texttt{integrate\_1d}} \texttt{(function\ integrand,\ real\ a,\ real\ b,\ real{[}{]}\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i,\ real\ relative\_tolerance)}\newline
Integrates the integrand from a to b with the given relative tolerance.

\hypertarget{arguments-to-the-1d-integrator}{%
\subsubsection{Arguments to the 1D integrator}\label{arguments-to-the-1d-integrator}}

The arguments to the 1D integrator are as follows:

\begin{itemize}
\tightlist
\item
  \emph{\texttt{integrand}}: function literal referring to a function specifying the integrand with signature \texttt{(real,\ real,\ real{[}{]},\ real{[}{]},\ int{[}{]}):real}
  The arguments represent

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumi{(\arabic{enumi})}
    \tightlist
    \item
      where integrand is evaluated,
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\arabic{enumi})}
    \setcounter{enumi}{1}
    \tightlist
    \item
      distance from evaluation point to integration limit for definite integrals,
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\arabic{enumi})}
    \setcounter{enumi}{2}
    \tightlist
    \item
      parameters,
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\arabic{enumi})}
    \setcounter{enumi}{3}
    \tightlist
    \item
      real data
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\arabic{enumi})}
    \setcounter{enumi}{4}
    \tightlist
    \item
      integer data, and the return value is the integrand evaluated at the given point,
    \end{enumerate}
  \end{itemize}
\item
  \emph{\texttt{a}}: left limit of integration, may be negative infinity, type \texttt{real},
\item
  \emph{\texttt{b}}: right limit of integration, may be positive infinity, type \texttt{real},
\item
  \emph{\texttt{theta}}: parameters only, type \texttt{real{[}{]}},
\item
  \emph{\texttt{x\_r}}: real data only, type \texttt{real{[}{]}},
\item
  \emph{\texttt{x\_i}}: integer data only, type \texttt{int{[}{]}}.
\end{itemize}

A \texttt{relative\_tolerance} argument can optionally be provided for more control over the algorithm:

\begin{itemize}
\tightlist
\item
  \emph{\texttt{relative\_tolerance}}: relative tolerance for the 1d integrator, type \texttt{real}, data only.
\end{itemize}

\hypertarget{return-value-1}{%
\subsubsection{Return value}\label{return-value-1}}

The return value for the 1D integrator is a \texttt{real}, the value of the integral.

\hypertarget{zero-crossing}{%
\subsubsection{Zero-crossing integrals}\label{zero-crossing}}

For numeric stability, integrals on the (possibly infinite) interval \((a, b)\) that cross zero are split into two integrals, one from \((a, 0)\) and one from \((0, b)\). Each integral is separately integrated to the given \texttt{relative\_tolerance}.

\hypertarget{precision-loss}{%
\subsubsection{Precision loss near limits of integration in definite integrals}\label{precision-loss}}

When integrating certain definite integrals, there can be significant precision loss in evaluating the integrand near the endpoints. This has to do with the breakdown in precision of double precision floating point values when adding or subtracting a small number from a number much larger than it in magnitude (for instance, \texttt{1.0\ -\ x}). \texttt{xc} (as passed to the integrand) is a high-precision version of the distance between \texttt{x} and the definite integral endpoints and can be used to address this issue. More information (and an example where this is useful) is given in the User's Guide. For zero crossing integrals, \texttt{xc} will be a high precision version of the distance to the endpoints of the two smaller integrals. For any integral with an endpoint at negative infinity or positive infinity, \texttt{xc} is set to \texttt{NaN}.

\hypertarget{algorithmic-details-1}{%
\subsubsection{Algorithmic details}\label{algorithmic-details-1}}

Internally the 1D integrator uses the double-exponential methods in the Boost 1D quadrature library. Boost in turn makes use of quadrature methods developed in (\protect\hyperlink{ref-Takahasi:1974}{Takahasi and Mori 1974}), (\protect\hyperlink{ref-Mori:1978}{Mori 1978}), (\protect\hyperlink{ref-Bailey:2005}{Bailey, Jeyabalan, and Li 2005}), and (\protect\hyperlink{ref-Tanaka:2009}{Tanaka et al. 2009}).

The gradients of the integral are computed in accordance with the Leibniz integral rule. Gradients of the integrand are computed internally with Stan's automatic differentiation.

\hypertarget{functions-reduce}{%
\section{Reduce-sum function}\label{functions-reduce}}

Stan provides a higher-order reduce function for summation. A function
which returns a scalar \texttt{g:\ U\ -\textgreater{}\ real} is mapped to every element of a
list of type \texttt{U{[}{]}}, \texttt{\{\ x1,\ x2,\ ...\ \}} and all the results are
accumulated,

\texttt{g(x1)\ +\ g(x2)\ +\ ...}

For efficiency reasons the reduce function doesn't work with the
element-wise evaluated function \texttt{g} itself, but instead works through
evaluating partial sums, \texttt{f:\ U{[}{]}\ -\textgreater{}\ real}, where:

\begin{verbatim}
f({ x1 }) = g(x1)
f({ x1, x2 }) = g(x1) + g(x2)
f({ x1, x2, ... }) = g(x1) + g(x2) + ...
\end{verbatim}

Mathematically the summation reduction is associative and forming
arbitrary partial sums in an aribitrary order will not change the
result. However, floating point numerics on computers only have
a limited precision such that associativity does not hold
exactly. This implies that the order of summation determines the exact
numerical result. For this reason, the higher-order reduce function is
available in two variants:

\begin{itemize}
\tightlist
\item
  \texttt{reduce\_sum}: Automatically choose partial sums partitioning based on a dynamic
  scheduling algorithm.
\item
  \texttt{reduce\_sum\_static}: Compute the same sum as \texttt{reduce\_sum}, but partition
  the input in the same way for given data set (in \texttt{reduce\_sum} this partitioning
  might change depending on computer load). This should result in stable
  numerical evaluations.
\end{itemize}

\hypertarget{specifying-the-reduce-sum-function}{%
\subsection{Specifying the reduce-sum function}\label{specifying-the-reduce-sum-function}}

The higher-order reduce function takes a partial sum function \texttt{f}, an array argument \texttt{x}
(with one array element for each term in the sum), a recommended
\texttt{grainsize}, and a set of shared arguments. This representation allows
parallelization of the resultant sum.

\index{{\tt \bfseries reduce\_sum }!{\tt (F f, T[] x, int grainsize, T1 s1, T2 s2, ...): real}|hyperpage}

\texttt{real} \textbf{\texttt{reduce\_sum}}\texttt{(F\ f,\ T{[}{]}\ x,\ int\ grainsize,\ T1\ s1,\ T2\ s2,\ ...)}\newline
\texttt{real} \textbf{\texttt{reduce\_sum\_static}}\texttt{(F\ f,\ T{[}{]}\ x,\ int\ grainsize,\ T1\ s1,\ T2\ s2,\ ...)}\newline

Returns the equivalent of \texttt{f(x,\ 1,\ size(x),\ s1,\ s2,\ ...)}, but computes
the result in parallel by breaking the array \texttt{x} into independent
partial sums. \texttt{s1,\ s2,\ ...} are shared between all terms in the sum.

\begin{itemize}
\tightlist
\item
  \emph{\texttt{f}}: function literal referring to a function specifying the
  partial sum operation. Refer to the \protect\hyperlink{functions-partial-sum}{partial sum function}.
\item
  \emph{\texttt{x}}: array of \texttt{T}, one for each term of the reduction, \texttt{T} can be any type,
\item
  \emph{\texttt{grainsize}}: For \texttt{reduce\_sum}, \texttt{grainsize} is the recommended size of the partial sum (\texttt{grainsize\ =\ 1} means pick totally automatically). For \texttt{reduce\_sum\_static}, \texttt{grainsize} determines the maximum size of the partial sums, type \texttt{int},
\item
  \emph{\texttt{s1}}: first (optional) shared argument, type \texttt{T1}, where \texttt{T1} can be any type
\item
  \emph{\texttt{s2}}: second (optional) shared argument, type \texttt{T2}, where \texttt{T2} can be any type,
\item
  \emph{\texttt{...}}: remainder of shared arguments, each of which can be any type.
\end{itemize}

\hypertarget{functions-partial-sum}{%
\subsection{The partial sum function}\label{functions-partial-sum}}

The partial sum function must have the following signature where the type \texttt{T}, and the
types of all the shared arguments (\texttt{T1}, \texttt{T2}, \ldots) match those of the original
\texttt{reduce\_sum} (\texttt{reduce\_sum\_static}) call.

\begin{verbatim}
(T[] x_subset, int start, int end, T1 s1, T2 s2, ...):real
\end{verbatim}

The partial sum function returns the sum of the \texttt{start} to \texttt{end} terms (inclusive) of the overall
calculations. The arguments to the partial sum function are:

\begin{itemize}
\item
  \emph{\texttt{x\_subset}}, the subset of \texttt{x} a given partial sum is responsible for computing, type \texttt{T{[}{]}}, where \texttt{T} matches the type of \texttt{x} in \texttt{reduce\_sum} (\texttt{reduce\_sum\_static})
\item
  \emph{\texttt{start}}, the index of the first term of the partial sum, type \texttt{int}
\item
  \emph{\texttt{end}}, the index of the last term of the partial sum (inclusive), type \texttt{int}
\item
  \emph{\texttt{s1}}, first shared argument, type \texttt{T1}, matching type of \texttt{s1} in \texttt{reduce\_sum} (\texttt{reduce\_sum\_static})
\item
  \emph{\texttt{s2}}, second shared argument, type \texttt{T2}, matching type of \texttt{s2} in \texttt{reduce\_sum} (\texttt{reduce\_sum\_static})
\item
  \emph{\texttt{...}}, remainder of shared arguments, with types matching those in \texttt{reduce\_sum} (\texttt{reduce\_sum\_static})
\end{itemize}

\hypertarget{functions-map}{%
\section{Map-rect function}\label{functions-map}}

Stan provides a higher-order map function. This allows map-reduce
functionality to be coded in Stan as described in the user's guide.

\hypertarget{specifying-the-mapped-function}{%
\subsection{Specifying the mapped function}\label{specifying-the-mapped-function}}

The function being mapped must have a signature identical to that of
the function \texttt{f} in the following declaration.

\begin{verbatim}
 vector f(vector phi, vector theta,
          data real[] x_r, data int[] x_i);
\end{verbatim}

The map function returns the sequence of results for the particular
shard being evaluated. The arguments to the mapped function are:

\begin{itemize}
\item
  \emph{\texttt{phi}}, the sequence of parameters shared across shards
\item
  \emph{\texttt{theta}}, the sequence of parameters specific to this shard
\item
  \emph{\texttt{x\_r}}, sequence of real-valued data
\item
  \emph{\texttt{x\_i}}, sequence of integer data
\end{itemize}

All input for the mapped function must be packed into these sequences
and all output from the mapped function must be packed into a single
vector. The vector of output from each mapped function is
concatenated into the final result.

\hypertarget{rectangular-map}{%
\subsection{Rectangular map}\label{rectangular-map}}

The rectangular map function operates on rectangular (not ragged) data
structures, with parallel data structures for job-specific parameters,
job-specific real data, and job-specific integer data.

\index{{\tt \bfseries map\_rect }!{\tt (F f, vector phi, vector[] theta, data real[,] x\_r, data int[,] x\_i): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{map\_rect}}\texttt{(F\ f,\ vector\ phi,\ vector{[}{]}\ theta,\ data\ real{[},{]}\ x\_r,\ data\ int{[},{]}\ x\_i)}\newline
Return the concatenation of the results of applying the function f, of
type \texttt{(vector,\ vector,\ real{[}{]},\ int{[}{]}):vector} elementwise, i.e.,
\texttt{f(phi,\ theta{[}n{]},\ x\_r{[}n{]},\ x\_i{[}n{]})} for each \texttt{n} in \texttt{1:N}, where \texttt{N} is
the size of the parallel arrays of job-specific/local parameters
\texttt{theta}, real data \texttt{x\_r}, and integer data \texttt{x\_r}. The shared/global
parameters \texttt{phi} are passed to each invocation of \texttt{f}.

\hypertarget{deprecated-functions}{%
\chapter{Deprecated Functions}\label{deprecated-functions}}

This appendix lists currently deprecated functionality along with how to
replace it. These deprecated features are likely to be removed in the future.

\hypertarget{functions-old-ode-solver}{%
\section{integrate\_ode\_rk45, integrate\_ode\_adams, integrate\_ode\_bdf ODE integrators}\label{functions-old-ode-solver}}

These ODE integrator functions have been replaced by those described in:

\hypertarget{specifying-an-ordinary-differential-equation-as-a-function}{%
\subsection{Specifying an ordinary differential equation as a function}\label{specifying-an-ordinary-differential-equation-as-a-function}}

A system of ODEs is specified as an ordinary function in Stan within
the functions block. The ODE system function must have this function
signature:

\begin{verbatim}
real[] ode(real time, real[] state, real[] theta,
         real[] x_r, int[] x_i)
\end{verbatim}

The ODE system function should return the derivative of the state with
respect to time at the time provided. The length of the returned real
array must match the length of the state input into the function.

The arguments to this function are:

\begin{itemize}
\item
  \emph{\texttt{time}}, the time to evaluate the ODE system
\item
  \emph{\texttt{state}}, the state of the ODE system at the time specified
\item
  \emph{\texttt{theta}}, parameter values used to evaluate the ODE system
\item
  \emph{\texttt{x\_r}}, data values used to evaluate the ODE system
\item
  \emph{\texttt{x\_i}}, integer data values used to evaluate the ODE system.
\end{itemize}

The ODE system function separates parameter values, \emph{\texttt{theta}}, from
data values, \emph{\texttt{x\_r}}, for efficiency in computing the gradients of the
ODE.

\hypertarget{non-stiff-solver-1}{%
\subsection{Non-stiff solver}\label{non-stiff-solver-1}}

\index{{\tt \bfseries integrate\_ode\_rk45 }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, real[] x\_r, int[] x\_i): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode\_rk45}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i)}\newline
Solves the ODE system for the times provided using the Dormand-Prince
algorithm, a 4th/5th order Runge-Kutta method.

\index{{\tt \bfseries integrate\_ode\_rk45 }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, real[] x\_r, int[] x\_i, real rel\_tol, real abs\_tol, int max\_num\_steps): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode\_rk45}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i,\ real\ rel\_tol,\ real\ abs\_tol,\ int\ max\_num\_steps)}\newline
Solves the ODE system for the times provided using the Dormand-Prince
algorithm, a 4th/5th order Runge-Kutta method with additional control
parameters for the solver.

\index{{\tt \bfseries integrate\_ode }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, real[] x\_r, int[] x\_i): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ real{[}{]}\ x\_r,\ int{[}{]}\ x\_i)}\newline
Solves the ODE system for the times provided using the Dormand-Prince
algorithm, a 4th/5th order Runge-Kutta method.

\index{{\tt \bfseries integrate\_ode\_adams }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, data real[] x\_r, data int[] x\_i): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode\_adams}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ data\ real{[}{]}\ x\_r,\ data\ int{[}{]}\ x\_i)}\newline
Solves the ODE system for the times provided using the Adams-Moulton method.

\index{{\tt \bfseries integrate\_ode\_adams }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, data real[] x\_r, data int[] x\_i, data real rel\_tol, data real abs\_tol, data int max\_num\_steps): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode\_adams}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ data\ real{[}{]}\ x\_r,\ data\ int{[}{]}\ x\_i,\ data\ real\ rel\_tol,\ data\ real\ abs\_tol,\ data\ int\ max\_num\_steps)}\newline
Solves the ODE system for the times provided using the Adams-Moulton
method with additional control parameters for the solver.

\hypertarget{stiff-solver-1}{%
\subsection{Stiff solver}\label{stiff-solver-1}}

\index{{\tt \bfseries integrate\_ode\_bdf }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, data real[] x\_r, data int[] x\_i): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode\_bdf}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ data\ real{[}{]}\ x\_r,\ data\ int{[}{]}\ x\_i)}\newline
Solves the ODE system for the times provided using the backward differentiation
formula (BDF) method.

\index{{\tt \bfseries integrate\_ode\_bdf }!{\tt (function ode, real[] initial\_state, real initial\_time, real[] times, real[] theta, data real[] x\_r, data int[] x\_i, data real rel\_tol, data real abs\_tol, data int max\_num\_steps): real[ , ]}|hyperpage}

\texttt{real{[}\ ,\ {]}} \textbf{\texttt{integrate\_ode\_bdf}}\texttt{(function\ ode,\ real{[}{]}\ initial\_state,\ real\ initial\_time,\ real{[}{]}\ times,\ real{[}{]}\ theta,\ data\ real{[}{]}\ x\_r,\ data\ int{[}{]}\ x\_i,\ data\ real\ rel\_tol,\ data\ real\ abs\_tol,\ data\ int\ max\_num\_steps)}\newline
Solves the ODE system for the times provided using the backward differentiation
formula (BDF) method with additional control parameters for the solver.

\hypertarget{arguments-to-the-ode-solvers-1}{%
\subsection{Arguments to the ODE solvers}\label{arguments-to-the-ode-solvers-1}}

The arguments to the ODE solvers in both the stiff and non-stiff cases
are as follows.

\begin{itemize}
\tightlist
\item
  \emph{\texttt{ode}}: function literal referring to a function specifying the
  system of differential equations with signature:
\end{itemize}

\begin{verbatim}
(real, real[], real[], data real[], data int[]):real[]
\end{verbatim}

The arguments represent (1) time, (2) system state, (3) parameters,
(4) real data, and (5) integer data, and the return value contains the
derivatives with respect to time of the state,

\begin{itemize}
\item
  \emph{\texttt{initial\_state}}: initial state, type \texttt{real{[}{]}},
\item
  \emph{\texttt{initial\_time}}: initial time, type \texttt{int} or \texttt{real},
\item
  \emph{\texttt{times}}: solution times, type \texttt{real{[}{]}},
\item
  \emph{\texttt{theta}}: parameters, type \texttt{real{[}{]}},
\item
  \texttt{data} \emph{\texttt{x\_r}}: real data, type \texttt{real{[}{]}}, data only, and
\item
  \texttt{data} \emph{\texttt{x\_i}}: integer data, type \texttt{int{[}{]}}, data only.
\end{itemize}

For more fine-grained control of the ODE solvers, these parameters can
also be provided:

\begin{itemize}
\item
  \texttt{data} \emph{\texttt{rel\_tol}}: relative tolerance for the ODE solver, type
  \texttt{real}, data only,
\item
  \texttt{data} \emph{\texttt{abs\_tol}}: absolute tolerance for the ODE solver, type
  \texttt{real}, data only, and
\item
  \texttt{data} \emph{\texttt{max\_num\_steps}}: maximum number of steps to take in the
  ODE solver, type \texttt{int}, data only.
\end{itemize}

\hypertarget{return-values-1}{%
\subsubsection{Return values}\label{return-values-1}}

The return value for the ODE solvers is an array of type \texttt{real{[},{]}},
with values consisting of solutions at the specified times.

\hypertarget{sizes-and-parallel-arrays-1}{%
\subsubsection{Sizes and parallel arrays}\label{sizes-and-parallel-arrays-1}}

The sizes must match, and in particular, the following groups are of
the same size:

\begin{itemize}
\item
  state variables passed into the system function, derivatives
  returned by the system function, initial state passed into the
  solver, and rows of the return value of the solver,
\item
  solution times and number of rows of the return value of the
  solver,
\item
  parameters, real data and integer data passed to the solver will
  be passed to the system function
\end{itemize}

\hypertarget{conventions-for-probability-functions}{%
\chapter{Conventions for Probability Functions}\label{conventions-for-probability-functions}}

Functions associated with distributions are set up to follow the same
naming conventions for both built-in distributions and for
user-defined distributions.

\hypertarget{suffix-marks-type-of-function}{%
\section{Suffix marks type of function}\label{suffix-marks-type-of-function}}

The suffix is determined by the type of function according to the
following table.

\begin{longtable}[]{@{}lll@{}}
\toprule
function & outcome & suffix \\ \addlinespace
\midrule
\endhead
log probability mass function & discrete & \texttt{\_lpmf} \\ \addlinespace
log probability density function & continuous & \texttt{\_lpdf} \\ \addlinespace
log cumulative distribution function & any & \texttt{\_lcdf} \\ \addlinespace
log complementary cumulative distribution function & any & \texttt{\_lccdf} \\ \addlinespace
random number generator & any & \texttt{\_rng} \\ \addlinespace
\bottomrule
\end{longtable}

For example, \texttt{normal\_lpdf} is the log of the normal probability
density function (pdf) and \texttt{bernoulli\_lpmf} is the log of the
bernoulli probability mass function (pmf). The log of the
corresponding cumulative distribution functions (cdf) use the same
suffix, \texttt{normal\_lcdf} and \texttt{bernoulli\_lcdf}.

\hypertarget{argument-order-and-the-vertical-bar}{%
\section{Argument order and the vertical bar}\label{argument-order-and-the-vertical-bar}}

Each probability function has a specific outcome value and a number of
parameters. Following conditional probability notation, probability
density and mass functions use a vertical bar to separate the outcome
from the parameters of the distribution. For example, \texttt{normal\_lpdf(y\ \textbar{}\ mu,\ sigma)} returns the value of mathematical formula \(\log \text{Normal}(y \, | \, \mu, \sigma)\). Cumulative distribution
functions separate the outcome from the parameters in the same way
(e.g., \texttt{normal\_lcdf(y\_low\ \textbar{}\ mu,\ sigma)}

\hypertarget{sampling-notation}{%
\section{Sampling notation}\label{sampling-notation}}

The notation

\begin{verbatim}
 y ~ normal(mu, sigma);
\end{verbatim}

provides the same (proportional) contribution to the model log density
as the explicit target density increment,

\begin{verbatim}
 target += normal_lpdf(y | mu, sigma);
\end{verbatim}

In both cases, the effect is to add terms to the target log density.
The only difference is that the example with the sampling (\texttt{\textasciitilde{}})
notation drops all additive constants in the log density; the
constants are not necessary for any of Stan's sampling, approximation,
or optimization algorithms.

\hypertarget{finite-inputs}{%
\section{Finite inputs}\label{finite-inputs}}

All of the distribution functions are configured to throw exceptions
(effectively rejecting samples or optimization steps) when they are
supplied with non-finite arguments. The two cases of non-finite
arguments are the infinite values and not-a-number value---these are
standard in floating-point arithmetic.

\hypertarget{boundary-conditions}{%
\section{Boundary conditions}\label{boundary-conditions}}

Many distributions are defined with support or constraints on
parameters forming an open interval. For example, the normal density
function accepts a scale parameter \(\sigma > 0\). If \(\sigma = 0\), the
probability function will throw an exception.

This is true even for (complementary) cumulative distribution
functions, which will throw exceptions when given input that is out of
the support.

\hypertarget{distributions-prng}{%
\section{Pseudorandom number generators}\label{distributions-prng}}

For most of the probability functions, there is a matching
pseudorandom number generator (PRNG) with the suffix \texttt{\_rng}. For
example, the function \texttt{normal\_rng(real,\ real)} accepts two real
arguments, an unconstrained location \(\mu\) and positive scale \(\sigma > 0\), and returns an unconstrained pseudorandom value drawn from
\(\text{Normal}(\mu,\sigma)\). There are also vectorized forms of
random number generators which return more than one random variate at
a time.

\hypertarget{restricted-to-transformed-data-and-generated-quantities}{%
\subsection{Restricted to transformed data and generated quantities}\label{restricted-to-transformed-data-and-generated-quantities}}

Unlike regular functions, the PRNG functions may only be used in the
transformed data or generated quantities blocks.

\hypertarget{limited-vectorization}{%
\subsection{Limited vectorization}\label{limited-vectorization}}

Unlike the probability functions, only some of the PRNG functions are
vectorized.

\hypertarget{cumulative-distribution-functions}{%
\section{Cumulative distribution functions}\label{cumulative-distribution-functions}}

For most of the univariate probability functions, there is a
corresponding cumulative distribution function, log cumulative
distribution function, and log complementary cumulative distribution
function.

For a univariate random variable \(Y\) with probability function \(p_Y(y \, | \, \theta)\), the cumulative distribution function (CDF) \(F_Y\) is
defined by \[ F_Y(y) \ = \ \text{Pr}[Y \le y] \ = \ \int_{-\infty}^y p(y
\, | \, \theta) \ \text{d}y. \] The complementary cumulative
distribution function (CCDF) is defined as \[ \text{Pr}[Y > y] \ =
\ 1 - F_Y(y). \] The reason to use CCDFs instead of CDFs in
floating-point arithmetic is that it is possible to represent numbers
very close to 0 (the closest you can get is roughly \(10^{-300}\)), but
not numbers very close to 1 (the closest you can get is roughly \(1 - 10^{-15}\)).

In Stan, there is a cumulative distribution function for each
probability function. For instance, \texttt{normal\_cdf(y,\ mu,\ sigma)} is
defined by \[ \int_{-\infty}^y \text{Normal}(y \, | \, \mu, \sigma) \
\text{d}y. \] There are also log forms of the CDF and CCDF for most
univariate distributions. For example, \texttt{normal\_lcdf(y\ \textbar{}\ mu,\ sigma)}
is defined by \[ \log \left( \int_{-\infty}^y \text{Normal}(y \, | \,
\mu, \sigma) \   \text{d}y \right) \] and \texttt{normal\_lccdf(y\ \textbar{}\ mu,\ sigma)} is defined by \[ \log \left( 1 - \int_{-\infty}^y
\text{Normal}(y \, | \, \mu, \sigma) \   \text{d}y \right). \]

\hypertarget{vectorization}{%
\section{Vectorization}\label{vectorization}}

Stan's univariate log probability functions, including the log density
functions, log mass functions, log CDFs, and log CCDFs, all support
vectorized function application, with results defined to be the sum of
the elementwise application of the function. Some of the PRNG
functions support vectorization, see section \protect\hyperlink{prng-vectorization}{vectorized PRNG functions}
for more details.

In all cases, matrix operations are at least as fast and usually
faster than loops and vectorized log probability functions are faster
than their equivalent form defined with loops. This isn't because
loops are slow in Stan, but because more efficient automatic
differentiation can be used. The efficiency comes from the fact that
a vectorized log probability function only introduces one new node into
the expression graph, thus reducing the number of virtual function
calls required to compute gradients in C++, as well as from allowing
caching of repeated computations.

Stan also overloads the multivariate normal distribution, including
the Cholesky-factor form, allowing arrays of row vectors or vectors
for the variate and location parameter. This is a huge savings in
speed because the work required to solve the linear system for the
covariance matrix is only done once.

Stan also overloads some scalar functions, such as \texttt{log} and \texttt{exp}, to
apply to vectors (arrays) and return vectors (arrays). These
vectorizations are defined elementwise and unlike the probability
functions, provide only minimal efficiency speedups over repeated
application and assignment in a loop.

\hypertarget{prob-vectorization}{%
\subsection{Vectorized function signatures}\label{prob-vectorization}}

\hypertarget{vectorized-scalar-arguments}{%
\subsubsection{Vectorized scalar arguments}\label{vectorized-scalar-arguments}}

The normal probability function is specified with the signature

\begin{verbatim}
 normal_lpdf(reals | reals, reals);
\end{verbatim}

The pseudotype \texttt{reals} is used to indicate that an argument position
may be vectorized. Argument positions declared as \texttt{reals} may be
filled with a real, a one-dimensional array, a vector, or a
row-vector. If there is more than one array or vector argument, their
types can be anything but their size must match. For instance, it is
legal to use \texttt{normal\_lpdf(row\_vector\ \textbar{}\ vector,\ real)} as long as the
vector and row vector have the same size.

\hypertarget{vectorized-vector-and-row-vector-arguments}{%
\subsubsection{Vectorized vector and row vector arguments}\label{vectorized-vector-and-row-vector-arguments}}

The multivariate normal distribution accepting vector or array of
vector arguments is written as

\begin{verbatim}
 multi_normal_lpdf(vectors | vectors, matrix);
\end{verbatim}

These arguments may be row vectors, column vectors, or arrays of row
vectors or column vectors.

\hypertarget{vectorized-integer-arguments}{%
\subsubsection{Vectorized integer arguments}\label{vectorized-integer-arguments}}

The pseudotype \texttt{ints} is used for vectorized integer arguments. Where
it appears either an integer or array of integers may be used.

\hypertarget{evaluating-vectorized-log-probability-functions}{%
\subsection{Evaluating vectorized log probability functions}\label{evaluating-vectorized-log-probability-functions}}

The result of a vectorized log probability function is equivalent to
the sum of the evaluations on each element. Any non-vector argument,
namely \texttt{real} or \texttt{int}, is repeated. For instance, if \texttt{y} is a vector
of size \texttt{N}, \texttt{mu} is a vector of size \texttt{N}, and \texttt{sigma} is a scalar,
then

\begin{verbatim}
 ll = normal_lpdf(y | mu, sigma);
\end{verbatim}

is just a more efficient way to write

\begin{verbatim}
 ll = 0;
 for (n in 1:N)
   ll = ll + normal_lpdf(y[n] | mu[n], sigma);
\end{verbatim}

With the same arguments, the vectorized sampling statement

\begin{verbatim}
 y ~ normal(mu, sigma);
\end{verbatim}

has the same effect on the total log probability as

\begin{verbatim}
 for (n in 1:N)
   y[n] ~ normal(mu[n], sigma);
\end{verbatim}

\hypertarget{prng-vectorization}{%
\subsection{Evaluating vectorized PRNG functions}\label{prng-vectorization}}

Some PRNG functions accept sequences as well as scalars as arguments.
Such functions are indicated by argument pseudotypes \texttt{reals} or
\texttt{ints}. In cases of sequence arguments, the output will also be a
sequence. For example, the following is allowed in the transformed data and
generated quantities blocks.

\begin{verbatim}
 vector[3] mu = ...;
 real x[3] = normal_rng(mu, 3);
\end{verbatim}

\hypertarget{argument-types}{%
\subsubsection{Argument types}\label{argument-types}}

In the case of PRNG functions, arguments marked \texttt{ints} may be integers
or integer arrays, whereas arguments marked \texttt{reals} may be integers or
reals, integer or real arrays, vectors, or row vectors.

\begin{longtable}[]{@{}ll@{}}
\toprule
pseudotype & allowable PRNG arguments \\ \addlinespace
\midrule
\endhead
\texttt{ints} & \texttt{int}, \texttt{int{[}{]}} \\ \addlinespace
\texttt{reals} & \texttt{int}, \texttt{int{[}{]}}, \texttt{real}, \texttt{real{[}{]}}, \texttt{vector}, \texttt{row\_vector} \\ \addlinespace
\bottomrule
\end{longtable}

\hypertarget{dimension-matching}{%
\subsubsection{Dimension matching}\label{dimension-matching}}

In general, if there are multiple non-scalar arguments, they must all
have the same dimensions, but need not have the same type. For
example, the \texttt{normal\_rng} function may be called with one vector
argument and one real array argument as long as they have the same
number of elements.

\begin{verbatim}
 vector[3] mu = ...;
 real sigma[3] = ...;
 real x[3] = normal_rng(mu, sigma);
\end{verbatim}

\hypertarget{return-type}{%
\subsubsection{Return type}\label{return-type}}

The result of a vectorized PRNG function depends on the size of the
arguments and the distribution's support. If all arguments are
scalars, then the return type is a scalar. For a continuous
distribution, if there are any non-scalar arguments, the return type
is a real array (\texttt{real{[}{]}}) matching the size of any of the non-scalar
arguments, as all non-scalar arguments must have matching size.
Discrete distributions return \texttt{ints} and continuous distributions
return \texttt{reals}, each of appropriate size. The symbol \texttt{R} denotes such
a return type.

\hypertarget{discrete-distributions}{%
\chapter*{Discrete Distributions}\label{discrete-distributions}}
\addcontentsline{toc}{chapter}{Discrete Distributions}

\hypertarget{binary-distributions}{%
\chapter{Binary Distributions}\label{binary-distributions}}

Binary probability distributions have support on \(\{0,1\}\), where 1
represents the value true and 0 the value false.

\hypertarget{bernoulli-distribution}{%
\section{Bernoulli distribution}\label{bernoulli-distribution}}

\hypertarget{probability-mass-function}{%
\subsection{Probability mass function}\label{probability-mass-function}}

If \(\theta \in [0,1]\), then for \(y \in \{0,1\}\), \[
\text{Bernoulli}(y~|~\theta) = \left\{ \begin{array}{ll} \theta &
\text{if } y = 1, \text{ and} \\ 1 - \theta & \text{if } y = 0.
\end{array} \right. \]

\hypertarget{sampling-statement}{%
\subsection{Sampling statement}\label{sampling-statement}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{bernoulli}}\texttt{(theta)}

Increment target log probability density with \texttt{bernoulli\_lupmf(y\ \textbar{}\ theta)}.
\index{{\tt \bfseries bernoulli }!sampling statement|hyperpage}

\hypertarget{stan-functions}{%
\subsection{Stan Functions}\label{stan-functions}}

\index{{\tt \bfseries bernoulli\_lpmf  }!{\tt (ints y \textbar\ reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_lpmf}}\texttt{(ints\ y\ \textbar{}\ reals\ theta)}\newline
The log Bernoulli probability mass of y given chance of success \texttt{theta}

\index{{\tt \bfseries bernoulli\_lupmf  }!{\tt (ints y \textbar\ reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_lupmf}}\texttt{(ints\ y\ \textbar{}\ reals\ theta)}\newline
The log Bernoulli probability mass of y given chance of success theta
dropping constant additive terms

\index{{\tt \bfseries bernoulli\_cdf  }!{\tt (ints y, reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_cdf}}\texttt{(ints\ y,\ reals\ theta)}\newline
The Bernoulli cumulative distribution function of y given chance of
success \texttt{theta}

\index{{\tt \bfseries bernoulli\_lcdf  }!{\tt (ints y \textbar\ reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_lcdf}}\texttt{(ints\ y\ \textbar{}\ reals\ theta)}\newline
The log of the Bernoulli cumulative distribution function of y given
chance of success \texttt{theta}

\index{{\tt \bfseries bernoulli\_lccdf  }!{\tt (ints y \textbar\ reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_lccdf}}\texttt{(ints\ y\ \textbar{}\ reals\ theta)}\newline
The log of the Bernoulli complementary cumulative distribution
function of y given chance of success \texttt{theta}

\index{{\tt \bfseries bernoulli\_rng  }!{\tt (reals theta): R}|hyperpage}

\texttt{R} \textbf{\texttt{bernoulli\_rng}}\texttt{(reals\ theta)}\newline
Generate a Bernoulli variate with chance of success \texttt{theta}; may only be
used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{bernoulli-logit-distribution}{%
\section{Bernoulli distribution, logit parameterization}\label{bernoulli-logit-distribution}}

Stan also supplies a direct parameterization in terms of a
logit-transformed chance-of-success parameter. This parameterization
is more numerically stable if the chance-of-success parameter is on
the logit scale, as with the linear predictor in a logistic
regression.

\hypertarget{probability-mass-function-1}{%
\subsection{Probability mass function}\label{probability-mass-function-1}}

If \(\alpha \in \mathbb{R}\), then for \(y \in \{0,1\}\), \[
\text{BernoulliLogit}(y~|~\alpha) = \text{Bernoulli}(y |
\text{logit}^{-1}(\alpha)) = \left\{ \begin{array}{ll}
\text{logit}^{-1}(\alpha) & \text{if } y = 1, \text{ and} \\ 1 -
\text{logit}^{-1}(\alpha) &  \text{if } y = 0. \end{array} \right. \]

\hypertarget{sampling-statement-1}{%
\subsection{Sampling statement}\label{sampling-statement-1}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{bernoulli\_logit}}\texttt{(alpha)}

Increment target log probability density with \texttt{bernoulli\_logit\_lupmf(y\ \textbar{}\ alpha)}.
\index{{\tt \bfseries bernoulli\_logit }!sampling statement|hyperpage}

\hypertarget{stan-functions-1}{%
\subsection{Stan Functions}\label{stan-functions-1}}

\index{{\tt \bfseries bernoulli\_logit\_lpmf  }!{\tt (ints y \textbar\ reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_lpmf}}\texttt{(ints\ y\ \textbar{}\ reals\ alpha)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha)}

\index{{\tt \bfseries bernoulli\_logit\_lupmf  }!{\tt (ints y \textbar\ reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_lupmf}}\texttt{(ints\ y\ \textbar{}\ reals\ alpha)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha)} dropping constant additive terms

\index{{\tt \bfseries bernoulli\_logit\_rng  }!{\tt (reals alpha): R}|hyperpage}

\texttt{R} \textbf{\texttt{bernoulli\_logit\_rng}}\texttt{(reals\ alpha)}\newline
Generate a Bernoulli variate with chance of success
\(\text{logit}^{-1}(\alpha)\); may only be used in transformed data and generated
quantities blocks. For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{bernoulli-logit-glm}{%
\section{Bernoulli-logit generalized linear model (Logistic Regression)}\label{bernoulli-logit-glm}}

Stan also supplies a single function for a generalized linear model
with Bernoulli likelihood and logit link function, i.e.~a function
for a logistic regression. This provides a more efficient
implementation of logistic regression than a manually written
regression in terms of a Bernoulli likelihood and matrix
multiplication.

\hypertarget{probability-mass-function-2}{%
\subsection{Probability mass function}\label{probability-mass-function-2}}

If \(x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m\), then for \(y \in {\{0,1\}}^n\), \begin{align*}
&\text{BernoulliLogitGLM}(y~|~x, \alpha, \beta) = \prod_{1\leq i \leq
n}\text{Bernoulli}(y_i~|~\text{logit}^{-1}(\alpha_i + x_i\cdot
\beta))\\ &= \prod_{1\leq i \leq n} \left\{ \begin{array}{ll}
\text{logit}^{-1}(\alpha_i + \sum_{1\leq j\leq m}x_{ij}\cdot \beta_j)
& \text{if } y_i = 1, \text{ and} \\ 1 - \text{logit}^{-1}(\alpha_i +
\sum_{1\leq j\leq m}x_{ij}\cdot \beta_j) & \text{if } y_i = 0.
\end{array} \right. \end{align*}

\hypertarget{sampling-statement-2}{%
\subsection{Sampling statement}\label{sampling-statement-2}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{bernoulli\_logit\_glm}}\texttt{(x,\ alpha,\ beta)}

Increment target log probability density with \texttt{bernoulli\_logit\_glm\_lupmf(y\ \textbar{}\ x,\ alpha,\ beta)}.
\index{{\tt \bfseries bernoulli\_logit\_glm }!sampling statement|hyperpage}

\hypertarget{stan-functions-2}{%
\subsection{Stan Functions}\label{stan-functions-2}}

\index{{\tt \bfseries bernoulli\_logit\_glm\_lpmf  }!{\tt (int y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)}.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lupmf  }!{\tt (int y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)} dropping constant additive terms.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lpmf  }!{\tt (int y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)}.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lupmf  }!{\tt (int y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)} dropping constant additive terms.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lpmf  }!{\tt (int[] y \textbar\ row\_vector x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)}.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lupmf  }!{\tt (int[] y \textbar\ row\_vector x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)} dropping constant additive terms.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lpmf  }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)}.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lupmf  }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)} dropping constant additive terms.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lpmf  }!{\tt (int[] y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)}.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lupmf  }!{\tt (int[] y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)} dropping constant additive terms.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lpmf  }!{\tt (int[] y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)}.

\index{{\tt \bfseries bernoulli\_logit\_glm\_lupmf  }!{\tt (int[] y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{bernoulli\_logit\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Bernoulli probability mass of y given chance of success
\texttt{inv\_logit(alpha\ +\ x\ *\ beta)} dropping constant additive terms.

\hypertarget{bounded-discrete-distributions}{%
\chapter{Bounded Discrete Distributions}\label{bounded-discrete-distributions}}

Bounded discrete probability functions have support on \(\{ 0, \ldots, N \}\) for some upper bound \(N\).

\hypertarget{binomial-distribution}{%
\section{Binomial distribution}\label{binomial-distribution}}

\hypertarget{probability-mass-function-3}{%
\subsection{Probability mass function}\label{probability-mass-function-3}}

Suppose \(N \in \mathbb{N}\) and \(\theta \in [0,1]\), and \(n \in \{0,\ldots,N\}\). \[ \text{Binomial}(n~|~N,\theta) = \binom{N}{n}
\theta^n (1 - \theta)^{N - n}. \]

\hypertarget{log-probability-mass-function}{%
\subsection{Log probability mass function}\label{log-probability-mass-function}}

\begin{eqnarray*} \log \text{Binomial}(n~|~N,\theta) & = & \log
\Gamma(N+1) - \log \Gamma(n + 1) - \log \Gamma(N- n + 1) \\[4pt] & & {
} + n \log \theta + (N - n) \log (1 - \theta), \end{eqnarray*}

\hypertarget{gradient-of-log-probability-mass-function}{%
\subsection{Gradient of log probability mass function}\label{gradient-of-log-probability-mass-function}}

\[ \frac{\partial}{\partial \theta} \log \text{Binomial}(n~|~N,\theta)
= \frac{n}{\theta} - \frac{N - n}{1 - \theta} \]

\hypertarget{sampling-statement-3}{%
\subsection{Sampling statement}\label{sampling-statement-3}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{binomial}}\texttt{(N,\ theta)}

Increment target log probability density with \texttt{binomial\_lupmf(n\ \textbar{}\ N,\ theta)}.
\index{{\tt \bfseries binomial }!sampling statement|hyperpage}

\hypertarget{stan-functions-3}{%
\subsection{Stan functions}\label{stan-functions-3}}

\index{{\tt \bfseries binomia\_lpmf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_lpmf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ theta)}\newline
The log binomial probability mass of n successes in N trials given
chance of success theta

\index{{\tt \bfseries binomia\_lupmf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_lupmf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ theta)}\newline
The log binomial probability mass of n successes in N trials given
chance of success theta dropping constant additive terms

\index{{\tt \bfseries binomia\_cdf }!{\tt (ints n, ints N, reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_cdf}}\texttt{(ints\ n,\ ints\ N,\ reals\ theta)}\newline
The binomial cumulative distribution function of n successes in N
trials given chance of success theta

\index{{\tt \bfseries binomia\_lcdf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_lcdf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ theta)}\newline
The log of the binomial cumulative distribution function of n
successes in N trials given chance of success theta

\index{{\tt \bfseries binomia\_lccdf }!{\tt (ints n \textbar\ ints N, reals theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_lccdf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ theta)}\newline
The log of the binomial complementary cumulative distribution function
of n successes in N trials given chance of success theta

\index{{\tt \bfseries binomial\_rng }!{\tt (ints N, reals theta): R}|hyperpage}

\texttt{R} \textbf{\texttt{binomial\_rng}}\texttt{(ints\ N,\ reals\ theta)}\newline
Generate a binomial variate with N trials and chance of success theta;
may only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{binomial-distribution-logit-parameterization}{%
\section{Binomial distribution, logit parameterization}\label{binomial-distribution-logit-parameterization}}

Stan also provides a version of the binomial probability mass function
distribution with the chance of success parameterized on the
unconstrained logistic scale.

\hypertarget{probability-mass-function-4}{%
\subsection{Probability mass function}\label{probability-mass-function-4}}

Suppose \(N \in \mathbb{N}\), \(\alpha \in \mathbb{R}\), and \(n \in \{0,\ldots,N\}\). Then \begin{eqnarray*}
\text{BinomialLogit}(n~|~N,\alpha) & = &
\text{Binomial}(n~|~N,\text{logit}^{-1}(\alpha)) \\[6pt] & = &
\binom{N}{n} \left( \text{logit}^{-1}(\alpha) \right)^{n}  \left( 1 -
\text{logit}^{-1}(\alpha) \right)^{N - n}.  \end{eqnarray*}

\hypertarget{log-probability-mass-function-1}{%
\subsection{Log probability mass function}\label{log-probability-mass-function-1}}

\begin{eqnarray*} \log \text{BinomialLogit}(n~|~N,\alpha) & = & \log
\Gamma(N+1) - \log \Gamma(n + 1) - \log \Gamma(N- n + 1) \\[4pt]   & &
{ } + n \log \text{logit}^{-1}(\alpha) + (N - n) \log \left( 1 -
\text{logit}^{-1}(\alpha) \right), \end{eqnarray*}

\hypertarget{gradient-of-log-probability-mass-function-1}{%
\subsection{Gradient of log probability mass function}\label{gradient-of-log-probability-mass-function-1}}

\[ \frac{\partial}{\partial \alpha} \log
\text{BinomialLogit}(n~|~N,\alpha) =
\frac{n}{\text{logit}^{-1}(-\alpha)} - \frac{N -
n}{\text{logit}^{-1}(\alpha)} \]

\hypertarget{sampling-statement-4}{%
\subsection{Sampling statement}\label{sampling-statement-4}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{binomial\_logit}}\texttt{(N,\ alpha)}

Increment target log probability density with \texttt{binomial\_logit\_lupmf(n\ \textbar{}\ N,\ alpha)}.
\index{{\tt \bfseries binomial\_logit }!sampling statement|hyperpage}

\hypertarget{stan-functions-4}{%
\subsection{Stan functions}\label{stan-functions-4}}

\index{{\tt \bfseries binomial\_logit\_lpmf }!{\tt (ints n \textbar\ ints N, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_logit\_lpmf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ alpha)}\newline
The log binomial probability mass of n successes in N trials given
logit-scaled chance of success alpha

\index{{\tt \bfseries binomial\_logit\_lupmf }!{\tt (ints n \textbar\ ints N, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{binomial\_logit\_lupmf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ alpha)}\newline
The log binomial probability mass of n successes in N trials given
logit-scaled chance of success alpha dropping constant additive terms

\hypertarget{beta-binomial-distribution}{%
\section{Beta-binomial distribution}\label{beta-binomial-distribution}}

\hypertarget{probability-mass-function-5}{%
\subsection{Probability mass function}\label{probability-mass-function-5}}

If \(N \in \mathbb{N}\), \(\alpha \in \mathbb{R}^+\), and \(\beta \in \mathbb{R}^+\), then for \(n \in {0,\ldots,N}\), \[
\text{BetaBinomial}(n~|~N,\alpha,\beta) = \binom{N}{n}
\frac{\mathrm{B}(n+\alpha, N -n +   \beta)}{\mathrm{B}(\alpha,\beta)},
\] where the beta function \(\mathrm{B}(u,v)\) is defined for \(u \in \mathbb{R}^+\) and \(v \in \mathbb{R}^+\) by \[ \mathrm{B}(u,v) =
\frac{\Gamma(u) \ \Gamma(v)}{\Gamma(u + v)}. \]

\hypertarget{sampling-statement-5}{%
\subsection{Sampling statement}\label{sampling-statement-5}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{beta\_binomial}}\texttt{(N,\ alpha,\ beta)}

Increment target log probability density with \texttt{beta\_binomial\_lupmf(n\ \textbar{}\ N,\ alpha,\ beta)}.
\index{{\tt \bfseries beta\_binomial }!sampling statement|hyperpage}

\hypertarget{stan-functions-5}{%
\subsection{Stan functions}\label{stan-functions-5}}

\index{{\tt \bfseries beta\_binomial\_lpmf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_binomial\_lpmf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ alpha,\ reals\ beta)}\newline
The log beta-binomial probability mass of n successes in N trials
given prior success count (plus one) of alpha and prior failure count
(plus one) of beta

\index{{\tt \bfseries beta\_binomial\_lupmf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_binomial\_lupmf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ alpha,\ reals\ beta)}\newline
The log beta-binomial probability mass of n successes in N trials
given prior success count (plus one) of alpha and prior failure count
(plus one) of beta dropping constant additive terms

\index{{\tt \bfseries beta\_binomial\_cdf }!{\tt (ints n, ints N, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_binomial\_cdf}}\texttt{(ints\ n,\ ints\ N,\ reals\ alpha,\ reals\ beta)}\newline
The beta-binomial cumulative distribution function of n successes in N
trials given prior success count (plus one) of alpha and prior failure
count (plus one) of beta

\index{{\tt \bfseries beta\_binomial\_lcdf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_binomial\_lcdf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ alpha,\ reals\ beta)}\newline
The log of the beta-binomial cumulative distribution function of n
successes in N trials given prior success count (plus one) of alpha
and prior failure count (plus one) of beta

\index{{\tt \bfseries beta\_binomial\_lccdf }!{\tt (ints n \textbar\ ints N, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_binomial\_lccdf}}\texttt{(ints\ n\ \textbar{}\ ints\ N,\ reals\ alpha,\ reals\ beta)}\newline
The log of the beta-binomial complementary cumulative distribution
function of n successes in N trials given prior success count (plus
one) of alpha and prior failure count (plus one) of beta

\index{{\tt \bfseries beta\_binomial\_rng }!{\tt (ints N, reals alpha, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{beta\_binomial\_rng}}\texttt{(ints\ N,\ reals\ alpha,\ reals\ beta)}\newline
Generate a beta-binomial variate with N trials, prior success count
(plus one) of alpha, and prior failure count (plus one) of beta; may
only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{hypergeometric-distribution}{%
\section{Hypergeometric distribution}\label{hypergeometric-distribution}}

\hypertarget{probability-mass-function-6}{%
\subsection{Probability mass function}\label{probability-mass-function-6}}

If \(a \in \mathbb{N}\), \(b \in \mathbb{N}\), and \(N \in \{0,\ldots,a+b\}\), then for \(n \in \{\max(0,N-b),\ldots,\min(a,N)\}\),
\[ \text{Hypergeometric}(n~|~N,a,b) = \frac{\normalsize{\binom{a}{n}
\binom{b}{N - n}}}      {\normalsize{\binom{a + b}{N}}}. \]

\hypertarget{sampling-statement-6}{%
\subsection{Sampling statement}\label{sampling-statement-6}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{hypergeometric}}\texttt{(N,\ a,\ b)}

Increment target log probability density with \texttt{hypergeometric\_lupmf(n\ \textbar{}\ N,\ a,\ b)}.
\index{{\tt \bfseries hypergeometric }!sampling statement|hyperpage}

\hypertarget{stan-functions-6}{%
\subsection{Stan functions}\label{stan-functions-6}}

\index{{\tt \bfseries hypergeometric\_lpmf }!{\tt (int n | int N, int a, int b): real}|hyperpage}

\texttt{real} \textbf{\texttt{hypergeometric\_lpmf}}\texttt{(int\ n\ \textbar{}\ int\ N,\ int\ a,\ int\ b)}\newline
The log hypergeometric probability mass of n successes in N trials
given total success count of a and total failure count of b

\index{{\tt \bfseries hypergeometric\_lupmf }!{\tt (int n | int N, int a, int b): real}|hyperpage}

\texttt{real} \textbf{\texttt{hypergeometric\_lupmf}}\texttt{(int\ n\ \textbar{}\ int\ N,\ int\ a,\ int\ b)}\newline
The log hypergeometric probability mass of n successes in N trials
given total success count of a and total failure count of b dropping constant
additive terms

\index{{\tt \bfseries hypergeometric\_rng }!{\tt (int N, int a, int2 b): int}|hyperpage}

\texttt{int} \textbf{\texttt{hypergeometric\_rng}}\texttt{(int\ N,\ int\ a,\ int\ b)}\newline
Generate a hypergeometric variate with N trials, total success count
of a, and total failure count of b; may only be used in transformed data and
generated quantities blocks

\hypertarget{categorical-distribution}{%
\section{Categorical distribution}\label{categorical-distribution}}

\hypertarget{probability-mass-functions}{%
\subsection{Probability mass functions}\label{probability-mass-functions}}

If \(N \in \mathbb{N}\), \(N > 0\), and if \(\theta \in \mathbb{R}^N\) forms
an \(N\)-simplex (i.e., has nonnegative entries summing to one), then
for \(y \in \{1,\ldots,N\}\), \[ \text{Categorical}(y~|~\theta) =
\theta_y. \] In addition, Stan provides a log-odds scaled categorical
distribution, \[ \text{CategoricalLogit}(y~|~\beta) =
\text{Categorical}(y~|~\text{softmax}(\beta)). \]
See \protect\hyperlink{softmax}{the definition of softmax} for the definition of the softmax function.

\hypertarget{sampling-statement-7}{%
\subsection{Sampling statement}\label{sampling-statement-7}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{categorical}}\texttt{(theta)}

Increment target log probability density with \texttt{categorical\_lupmf(y\ \textbar{}\ theta)}
dropping constant additive terms.
\index{{\tt \bfseries categorical }!sampling statement|hyperpage}

\hypertarget{sampling-statement-8}{%
\subsection{Sampling statement}\label{sampling-statement-8}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{categorical\_logit}}\texttt{(beta)}

Increment target log probability density with \texttt{categorical\_logit\_lupmf(y\ \textbar{}\ beta)}.
\index{{\tt \bfseries categorical\_logit }!sampling statement|hyperpage}

\hypertarget{stan-functions-7}{%
\subsection{Stan functions}\label{stan-functions-7}}

All of the categorical distributions are vectorized so that the
outcome y can be a single integer (type \texttt{int}) or an array of integers
(type \texttt{int{[}{]}}).

\index{{\tt \bfseries categorical\_lpmf }!{\tt (ints y \textbar\ vector theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_lpmf}}\texttt{(ints\ y\ \textbar{}\ vector\ theta)}\newline
The log categorical probability mass function with outcome(s) y in
\(1:N\) given \(N\)-vector of outcome probabilities theta. The parameter
theta must have non-negative entries that sum to one, but it need not
be a variable declared as a simplex.

\index{{\tt \bfseries categorical\_lupmf }!{\tt (ints y \textbar\ vector theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_lupmf}}\texttt{(ints\ y\ \textbar{}\ vector\ theta)}\newline
The log categorical probability mass function with outcome(s) y in
\(1:N\) given \(N\)-vector of outcome probabilities theta dropping constant
additive terms. The parameter theta must have non-negative entries that sum
to one, but it need not be a variable declared as a simplex.

\index{{\tt \bfseries categorical\_logit\_lpmf }!{\tt (ints y \textbar\ vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_lpmf}}\texttt{(ints\ y\ \textbar{}\ vector\ beta)}\newline
The log categorical probability mass function with outcome(s) y in
\(1:N\) given log-odds of outcomes beta.

\index{{\tt \bfseries categorical\_logit\_lupmf }!{\tt (ints y \textbar\ vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_lupmf}}\texttt{(ints\ y\ \textbar{}\ vector\ beta)}\newline
The log categorical probability mass function with outcome(s) y in
\(1:N\) given log-odds of outcomes beta dropping constant additive terms.

\index{{\tt \bfseries categorical\_rng }!{\tt (vector theta): int}|hyperpage}

\texttt{int} \textbf{\texttt{categorical\_rng}}\texttt{(vector\ theta)}\newline
Generate a categorical variate with \(N\)-simplex distribution parameter
theta; may only be used in transformed data and generated quantities blocks

\index{{\tt \bfseries categorical\_logit\_rng }!{\tt (vector beta): int}|hyperpage}

\texttt{int} \textbf{\texttt{categorical\_logit\_rng}}\texttt{(vector\ beta)}\newline
Generate a categorical variate with outcome in range \(1:N\) from
log-odds vector beta; may only be used in transformed data and generated
quantities blocks

\hypertarget{categorical-logit-glm}{%
\section{Categorical logit generalized linear model (softmax regression)}\label{categorical-logit-glm}}

Stan also supplies a single function for a generalized linear model
with categorical likelihood and logit link function, i.e.~a function
for a softmax regression. This provides a more efficient
implementation of softmax regression than a manually written
regression in terms of a Categorical likelihood and matrix
multiplication.

Note that the implementation does not put any restrictions on the coefficient
matrix \(\beta\). It is up to the user to use a reference category, a suitable
prior or some other means of identifiability. See Multi-logit in the
\href{https://mc-stan.org/users/documentation/}{Stan User's Guide}.

\hypertarget{probability-mass-functions-1}{%
\subsection{Probability mass functions}\label{probability-mass-functions-1}}

If \(N,M,K \in \mathbb{N}\), \(N,M,K > 0\), and if \(x\in \mathbb{R}^{M\cdot K}, \alpha \in \mathbb{R}^N, \beta\in \mathbb{R}^{K\cdot N}\), then for \(y \in \{1,\ldots,N\}^M\),
\[ \text{CategoricalLogitGLM}(y~|~x,\alpha,\beta) = \\[5pt]
\prod_{1\leq i \leq M}\text{CategoricalLogit}(y_i~|~\alpha+x_i\cdot\beta) = \\[15pt]
\prod_{1\leq i \leq M}\text{Categorical}(y_i~|~softmax(\alpha+x_i\cdot\beta)). \]
See \protect\hyperlink{softmax}{the definition of softmax} for the definition of the softmax function.

\hypertarget{sampling-statement-9}{%
\subsection{Sampling statement}\label{sampling-statement-9}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{categorical\_logit\_glm}}\texttt{(x,\ alpha,\ beta)}

Increment target log probability density with \texttt{categorical\_logit\_glm\_lupmf(y\ \textbar{}\ x,\ alpha,\ beta)}.
\index{{\tt \bfseries categorical\_logit\_glm }!sampling statement|hyperpage}

\hypertarget{stan-functions-8}{%
\subsection{Stan functions}\label{stan-functions-8}}

\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int y \textbar\ row\_vector x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcome \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries categorical\_logit\_glm\_lupmf }!{\tt (int y \textbar\ row\_vector x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcome \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcomes \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries categorical\_logit\_glm\_lupmf }!{\tt (int y \textbar\ matrix x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcomes \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcomes \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries categorical\_logit\_glm\_lupmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcomes \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries categorical\_logit\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcomes \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries categorical\_logit\_glm\_lupmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, matrix beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{categorical\_logit\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ matrix\ beta)}\newline
The log categorical probability mass function with outcomes \texttt{y} in
\(1:N\) given \(N\)-vector of log-odds of outcomes \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\hypertarget{ordered-logistic-distribution}{%
\section{Ordered logistic distribution}\label{ordered-logistic-distribution}}

\hypertarget{probability-mass-function-7}{%
\subsection{Probability mass function}\label{probability-mass-function-7}}

If \(K \in \mathbb{N}\) with \(K > 2\), \(c \in \mathbb{R}^{K-1}\) such that
\(c_k < c_{k+1}\) for \(k \in \{1,\ldots,K-2\}\), and \(\eta \in \mathbb{R}\), then for \(k \in \{1,\ldots,K\}\), \[
\text{OrderedLogistic}(k~|~\eta,c) = \left\{ \begin{array}{ll} 1 -
\text{logit}^{-1}(\eta - c_1)  &  \text{if } k = 1, \\[4pt]
\text{logit}^{-1}(\eta - c_{k-1}) - \text{logit}^{-1}(\eta - c_{k})  &
\text{if } 1 < k < K, \text{and} \\[4pt] \text{logit}^{-1}(\eta -
c_{K-1}) - 0  &  \text{if } k = K. \end{array} \right. \] The \(k=K\)
case is written with the redundant subtraction of zero to illustrate
the parallelism of the cases; the \(k=1\) and \(k=K\) edge cases can be
subsumed into the general definition by setting \(c_0 = -\infty\) and
\(c_K = +\infty\) with \(\text{logit}^{-1}(-\infty) = 0\) and
\(\text{logit}^{-1}(\infty) = 1\).

\hypertarget{sampling-statement-10}{%
\subsection{Sampling statement}\label{sampling-statement-10}}

\texttt{k\ \textasciitilde{}} \textbf{\texttt{ordered\_logistic}}\texttt{(eta,\ c)}

Increment target log probability density with \texttt{ordered\_logistic\_lupmf(k\ \textbar{}\ eta,\ c)}.
\index{{\tt \bfseries ordered\_logistic }!sampling statement|hyperpage}

\hypertarget{stan-functions-9}{%
\subsection{Stan functions}\label{stan-functions-9}}

\index{{\tt \bfseries ordered\_logistic\_lpmf }!{\tt (ints k \textbar\ vector eta, vectors c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_lpmf}}\texttt{(ints\ k\ \textbar{}\ vector\ eta,\ vectors\ c)}\newline
The log ordered logistic probability mass of k given linear predictors
\texttt{eta}, and cutpoints \texttt{c}.

\index{{\tt \bfseries ordered\_logistic\_lupmf }!{\tt (ints k \textbar\ vector eta, vectors c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_lupmf}}\texttt{(ints\ k\ \textbar{}\ vector\ eta,\ vectors\ c)}\newline
The log ordered logistic probability mass of k given linear predictors
\texttt{eta}, and cutpoints \texttt{c} dropping constant additive terms.

\index{{\tt \bfseries ordered\_logistic\_rng }!{\tt (real eta, vector c): int}|hyperpage}

\texttt{int} \textbf{\texttt{ordered\_logistic\_rng}}\texttt{(real\ eta,\ vector\ c)}\newline
Generate an ordered logistic variate with linear predictor \texttt{eta} and
cutpoints \texttt{c}; may only be used in transformed data and generated quantities blocks

\hypertarget{ordered-logistic-generalized-linear-model-ordinal-regression}{%
\section{Ordered logistic generalized linear model (ordinal regression)}\label{ordered-logistic-generalized-linear-model-ordinal-regression}}

\hypertarget{probability-mass-function-8}{%
\subsection{Probability mass function}\label{probability-mass-function-8}}

If \(N,M,K \in \mathbb{N}\) with \(N, M > 0\), \(K > 2\), \(c \in \mathbb{R}^{K-1}\) such that
\(c_k < c_{k+1}\) for \(k \in \{1,\ldots,K-2\}\), and \(x\in \mathbb{R}^{N\cdot M}, \beta\in \mathbb{R}^M\), then for \(y \in \{1,\ldots,K\}^N\),
\[\text{OrderedLogisticGLM}(y~|~x,\beta,c) = \\[4pt]
\prod_{1\leq i \leq N}\text{OrderedLogistic}(y_i~|~x_i\cdot \beta,c) = \\[17pt]
\prod_{1\leq i \leq N}\left\{ \begin{array}{ll}
1 - \text{logit}^{-1}(x_i\cdot \beta - c_1)  &  \text{if } y = 1, \\[4pt]
\text{logit}^{-1}(x_i\cdot \beta - c_{y-1}) - \text{logit}^{-1}(x_i\cdot \beta - c_{y}) & \text{if } 1 < y < K, \text{and} \\[4pt]
\text{logit}^{-1}(x_i\cdot \beta - c_{K-1}) - 0  &  \text{if } y = K.
\end{array} \right. \] The \(k=K\)
case is written with the redundant subtraction of zero to illustrate
the parallelism of the cases; the \(y=1\) and \(y=K\) edge cases can be
subsumed into the general definition by setting \(c_0 = -\infty\) and
\(c_K = +\infty\) with \(\text{logit}^{-1}(-\infty) = 0\) and
\(\text{logit}^{-1}(\infty) = 1\).

\hypertarget{sampling-statement-11}{%
\subsection{Sampling statement}\label{sampling-statement-11}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{ordered\_logistic\_glm}}\texttt{(x,\ beta,\ c)}

Increment target log probability density with \texttt{ordered\_logistic\_lupmf(y\ \textbar{}\ x,\ beta,\ c)}.
\index{{\tt \bfseries ordered\_logistic\_glm }!sampling statement|hyperpage}

\hypertarget{stan-functions-10}{%
\subsection{Stan functions}\label{stan-functions-10}}

\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int y \textbar\ row\_vector x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ row\_vector\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors \texttt{x\ *\ beta}, and cutpoints c.
The cutpoints \texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lupmf }!{\tt (int y \textbar\ row\_vector x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ row\_vector\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors
\texttt{x\ *\ beta}, and cutpoints c dropping constant additive terms. The cutpoints
\texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors \texttt{x\ *\ beta}, and cutpoints c.
The cutpoints \texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lupmf }!{\tt (int y \textbar\ matrix x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors
\texttt{x\ *\ beta}, and cutpoints c dropping constant additive terms. The cutpoints
\texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors \texttt{x\ *\ beta}, and cutpoints c.
The cutpoints \texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lupmf }!{\tt (int[] y \textbar\ row\_vector x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors
\texttt{x\ *\ beta}, and cutpoints c dropping constant additive terms. The cutpoints
\texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors
\texttt{x\ *\ beta}, and cutpoints c.~The cutpoints \texttt{c} must be ordered.

\index{{\tt \bfseries ordered\_logistic\_glm\_lupmf }!{\tt (int[] y \textbar\ matrix x, vector beta, vector c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_logistic\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ beta,\ vector\ c)}\newline
The log ordered logistic probability mass of y, given linear predictors
\texttt{x\ *\ beta}, and cutpoints c dropping constant additive terms. The cutpoints \texttt{c}
must be ordered.

\hypertarget{ordered-probit-distribution}{%
\section{Ordered probit distribution}\label{ordered-probit-distribution}}

\hypertarget{probability-mass-function-9}{%
\subsection{Probability mass function}\label{probability-mass-function-9}}

If \(K \in \mathbb{N}\) with \(K > 2\), \(c \in \mathbb{R}^{K-1}\) such that
\(c_k < c_{k+1}\) for \(k \in \{1,\ldots,K-2\}\), and \(\eta \in \mathbb{R}\), then for \(k \in \{1,\ldots,K\}\), \[
\text{OrderedProbit}(k~|~\eta,c) = \left\{ \begin{array}{ll} 1 -
\Phi(\eta - c_1) & \text{if } k = 1, \\[4pt] \Phi(\eta - c_{k-1}) -
\Phi(\eta - c_{k})  & \text{if } 1 < k < K, \text{and} \\[4pt]
\Phi(\eta - c_{K-1}) - 0 & \text{if } k = K. \end{array} \right. \]
The \(k=K\) case is written with the redundant subtraction of zero to
illustrate the parallelism of the cases; the \(k=1\) and \(k=K\) edge
cases can be subsumed into the general definition by setting \(c_0 = -\infty\) and \(c_K = +\infty\) with \(\Phi(-\infty) = 0\) and
\(\Phi(\infty) = 1\).

\hypertarget{sampling-statement-12}{%
\subsection{Sampling statement}\label{sampling-statement-12}}

\texttt{k\ \textasciitilde{}} \textbf{\texttt{ordered\_probit}}\texttt{(eta,\ c)}

Increment target log probability density with \texttt{ordered\_probit\_lupmf(k\ \textbar{}\ eta,\ c)}.
\index{{\tt \bfseries ordered\_probit }!sampling statement|hyperpage}

\hypertarget{stan-functions-11}{%
\subsection{Stan functions}\label{stan-functions-11}}

\index{{\tt \bfseries ordered\_probit\_lpmf }!{\tt (ints k \textbar\ vector eta, vectors c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_probit\_lpmf}}\texttt{(ints\ k\ \textbar{}\ vector\ eta,\ vectors\ c)}\newline
The log ordered probit probability mass of k given linear predictors
eta, and cutpoints c.

\index{{\tt \bfseries ordered\_probit\_lupmf }!{\tt (ints k \textbar\ vector eta, vectors c): real}|hyperpage}

\texttt{real} \textbf{\texttt{ordered\_probit\_lupmf}}\texttt{(ints\ k\ \textbar{}\ vector\ eta,\ vectors\ c)}\newline
The log ordered probit probability mass of k given linear predictors
eta, and cutpoints c dropping constant additive terms.

\index{{\tt \bfseries ordered\_probit\_rng }!{\tt (real eta, vector c): int}|hyperpage}

\texttt{int} \textbf{\texttt{ordered\_probit\_rng}}\texttt{(real\ eta,\ vector\ c)}\newline
Generate an ordered probit variate with linear predictor eta and
cutpoints c; may only be used in transformed data and generated quantities blocks

\hypertarget{unbounded-discrete-distributions}{%
\chapter{Unbounded Discrete Distributions}\label{unbounded-discrete-distributions}}

The unbounded discrete distributions have support over the natural
numbers (i.e., the non-negative integers).

\hypertarget{negative-binomial-distribution}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution}}

For the negative binomial distribution Stan uses the parameterization
described in \protect\hyperlink{ref-GelmanEtAl:2013}{Gelman et al.} (\protect\hyperlink{ref-GelmanEtAl:2013}{2013}). For alternative parameterizations, see
section \protect\hyperlink{neg-binom-2-log}{negative binomial glm}.

\hypertarget{probability-mass-function-10}{%
\subsection{Probability mass function}\label{probability-mass-function-10}}

If \(\alpha \in \mathbb{R}^+\) and \(\beta \in \mathbb{R}^+\), then for \(n \in \mathbb{N}\), \[ \text{NegBinomial}(n~|~\alpha,\beta) = \binom{n +
\alpha - 1}{\alpha - 1} \, \left( \frac{\beta}{\beta+1}
\right)^{\!\alpha} \, \left( \frac{1}{\beta + 1} \right)^{\!n} \!. \]

The mean and variance of a random variable \(n \sim \text{NegBinomial}(\alpha,\beta)\) are given by \[ \mathbb{E}[n] =
\frac{\alpha}{\beta} \ \ \text{ and } \ \ \text{Var}[n] =
\frac{\alpha}{\beta^2} (\beta + 1). \]

\hypertarget{sampling-statement-13}{%
\subsection{Sampling statement}\label{sampling-statement-13}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{neg\_binomial}}\texttt{(alpha,\ beta)}

Increment target log probability density with \texttt{neg\_binomial\_lupmf(n\ \textbar{}\ alpha,\ beta)}.
\index{{\tt \bfseries neg\_binomial }!sampling statement|hyperpage}

\hypertarget{stan-functions-12}{%
\subsection{Stan functions}\label{stan-functions-12}}

\index{{\tt \bfseries neg\_binomial\_lpmf }!{\tt (ints n \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_lpmf}}\texttt{(ints\ n\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log negative binomial probability mass of \texttt{n} given shape \texttt{alpha} and
inverse scale \texttt{beta}

\index{{\tt \bfseries neg\_binomial\_lupmf }!{\tt (ints n \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_lupmf}}\texttt{(ints\ n\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log negative binomial probability mass of \texttt{n} given shape \texttt{alpha} and
inverse scale \texttt{beta} dropping constant additive terms

\index{{\tt \bfseries neg\_binomial\_cdf }!{\tt (ints n, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_cdf}}\texttt{(ints\ n,\ reals\ alpha,\ reals\ beta)}\newline
The negative binomial cumulative distribution function of \texttt{n} given
shape \texttt{alpha} and inverse scale \texttt{beta}

\index{{\tt \bfseries neg\_binomial\_lcdf }!{\tt (ints n \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_lcdf}}\texttt{(ints\ n\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the negative binomial cumulative distribution function of \texttt{n}
given shape \texttt{alpha} and inverse scale \texttt{beta}

\index{{\tt \bfseries neg\_binomial\_lccdf }!{\tt (ints n \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_lccdf}}\texttt{(ints\ n\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the negative binomial complementary cumulative distribution
function of \texttt{n} given shape \texttt{alpha} and inverse scale \texttt{beta}

\index{{\tt \bfseries neg\_binomial\_rng }!{\tt (reals alpha, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{neg\_binomial\_rng}}\texttt{(reals\ alpha,\ reals\ beta)}\newline
Generate a negative binomial variate with shape \texttt{alpha} and inverse
scale \texttt{beta}; may only be used in transformed data and generated quantities blocks.
\texttt{alpha} \(/\) \texttt{beta} must be less than \(2 ^ {29}\). For a description of argument and
return types, see section \protect\hyperlink{prob-vectorization}{vectorized function signatures}.

\hypertarget{nbalt}{%
\section{Negative binomial distribution (alternative parameterization)}\label{nbalt}}

Stan also provides an alternative parameterization of the negative
binomial distribution directly using a mean (i.e., location) parameter
and a parameter that controls overdispersion relative to the square of
the mean. Section \protect\hyperlink{betafun}{combinatorial functions}, below, provides a second
alternative parameterization directly in terms of the log mean.

\hypertarget{probability-mass-function-11}{%
\subsection{Probability mass function}\label{probability-mass-function-11}}

The first parameterization is for \(\mu \in \mathbb{R}^+\) and \(\phi \in \mathbb{R}^+\), which for \(n \in \mathbb{N}\) is defined as \[
\text{NegBinomial2}(n \, | \, \mu, \phi)  = \binom{n + \phi - 1}{n} \,
\left( \frac{\mu}{\mu+\phi} \right)^{\!n} \, \left(
\frac{\phi}{\mu+\phi} \right)^{\!\phi} \!. \]

The mean and variance of a random variable \(n \sim \text{NegBinomial2}(n~|~\mu,\phi)\) are \[ \mathbb{E}[n] = \mu \ \ \
\text{ and } \ \ \ \text{Var}[n] = \mu + \frac{\mu^2}{\phi}. \] Recall
that \(\text{Poisson}(\mu)\) has variance \(\mu\), so \(\mu^2 / \phi > 0\)
is the additional variance of the negative binomial above that of the
Poisson with mean \(\mu\). So the inverse of parameter \(\phi\) controls
the overdispersion, scaled by the square of the mean, \(\mu^2\).

\hypertarget{sampling-statement-14}{%
\subsection{Sampling statement}\label{sampling-statement-14}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{neg\_binomial\_2}}\texttt{(mu,\ phi)}

Increment target log probability density with \texttt{neg\_binomial\_2\_lupmf(n\ \textbar{}\ mu,\ phi)}.
\index{{\tt \bfseries neg\_binomial\_2 }!sampling statement|hyperpage}

\hypertarget{stan-functions-13}{%
\subsection{Stan functions}\label{stan-functions-13}}

\index{{\tt \bfseries neg\_binomial\_2\_lpmf }!{\tt (ints n \textbar\ reals mu, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_lpmf}}\texttt{(ints\ n\ \textbar{}\ reals\ mu,\ reals\ phi)}\newline
The negative binomial probability mass of \texttt{n} given location \texttt{mu} and
precision \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_lupmf }!{\tt (ints n \textbar\ reals mu, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_lupmf}}\texttt{(ints\ n\ \textbar{}\ reals\ mu,\ reals\ phi)}\newline
The negative binomial probability mass of \texttt{n} given location \texttt{mu} and
precision \texttt{phi} dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_cdf }!{\tt (ints n, reals mu, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_cdf}}\texttt{(ints\ n,\ reals\ mu,\ reals\ phi)}\newline
The negative binomial cumulative distribution function of \texttt{n} given
location \texttt{mu} and precision \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_lcdf }!{\tt (ints n \textbar\ reals mu, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_lcdf}}\texttt{(ints\ n\ \textbar{}\ reals\ mu,\ reals\ phi)}\newline
The log of the negative binomial cumulative distribution function of \texttt{n}
given location \texttt{mu} and precision \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_lccdf }!{\tt (ints n \textbar\ reals mu, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_lccdf}}\texttt{(ints\ n\ \textbar{}\ reals\ mu,\ reals\ phi)}\newline
The log of the negative binomial complementary cumulative distribution
function of \texttt{n} given location \texttt{mu} and precision \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_rng }!{\tt (reals mu, reals phi): R}|hyperpage}

\texttt{R} \textbf{\texttt{neg\_binomial\_2\_rng}}\texttt{(reals\ mu,\ reals\ phi)}\newline
Generate a negative binomial variate with location \texttt{mu} and precision
\texttt{phi}; may only be used in transformed data and generated quantities blocks. \texttt{mu}
must be less than \(2 ^ {29}\). For a description of argument and return types, see
section \protect\hyperlink{prob-vectorization}{vectorized function signatures}.

\hypertarget{neg-binom-2-log}{%
\section{Negative binomial distribution (log alternative parameterization)}\label{neg-binom-2-log}}

Related to the parameterization in section \protect\hyperlink{nbalt}{negative binomial, alternative parameterization}, the following
parameterization uses a log mean parameter \(\eta = \log(\mu)\), defined
for \(\eta \in \mathbb{R}\), \(\phi \in \mathbb{R}^+\), so that for \(n \in \mathbb{N}\), \[ \text{NegBinomial2Log}(n \, | \, \eta, \phi) =
\text{NegBinomial2}(n | \exp(\eta), \phi). \] This alternative may be
used for sampling, as a function, and for random number generation,
but as of yet, there are no CDFs implemented for it. This is especially useful
for log-linear negative binomial regressions.

\hypertarget{sampling-statement-15}{%
\subsection{Sampling statement}\label{sampling-statement-15}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{neg\_binomial\_2\_log}}\texttt{(eta,\ phi)}

Increment target log probability density with \texttt{neg\_binomial\_2\_log\_lupmf(n\ \textbar{}\ eta,\ phi)}.
\index{{\tt \bfseries neg\_binomial\_2\_log }!sampling statement|hyperpage}

\hypertarget{stan-functions-14}{%
\subsection{Stan functions}\label{stan-functions-14}}

\index{{\tt \bfseries neg\_binomial\_2\_log\_lpmf }!{\tt (ints n \textbar\ reals eta, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_lpmf}}\texttt{(ints\ n\ \textbar{}\ reals\ eta,\ reals\ phi)}\newline
The log negative binomial probability mass of \texttt{n} given log-location \texttt{eta}
and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_lupmf }!{\tt (ints n \textbar\ reals eta, reals phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_lupmf}}\texttt{(ints\ n\ \textbar{}\ reals\ eta,\ reals\ phi)}\newline
The log negative binomial probability mass of \texttt{n} given log-location \texttt{eta}
and inverse overdispersion parameter \texttt{phi} dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_log\_rng }!{\tt (reals eta, reals phi): R}|hyperpage}

\texttt{R} \textbf{\texttt{neg\_binomial\_2\_log\_rng}}\texttt{(reals\ eta,\ reals\ phi)}\newline
Generate a negative binomial variate with log-location \texttt{eta} and inverse
overdispersion control \texttt{phi}; may only be used in transformed data and generated
quantities blocks. \texttt{eta} must be less than \(29 \log 2\). For a description of
argument and return types, see section \protect\hyperlink{prob-vectorization}{vectorized function signatures}.

\hypertarget{neg-binom-2-log-glm}{%
\section{Negative-binomial-2-log generalized linear model (negative binomial regression)}\label{neg-binom-2-log-glm}}

Stan also supplies a single function for a generalized linear model
with negative binomial likelihood and log link function, i.e.~a
function for a negative binomial regression. This provides a
more efficient implementation of negative binomial regression than a
manually written regression in terms of a negative binomial likelihood
and matrix multiplication.

\hypertarget{probability-mass-function-12}{%
\subsection{Probability mass function}\label{probability-mass-function-12}}

If \(x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m, \phi\in \mathbb{R}^+\), then for \(y \in \mathbb{N}^n\), \[
\text{NegBinomial2LogGLM}(y~|~x, \alpha, \beta, \phi) = \prod_{1\leq i
\leq n}\text{NegBinomial2}(y_i~|~\exp(\alpha_i + x_i\cdot \beta),
\phi). \]

\hypertarget{sampling-statement-16}{%
\subsection{Sampling statement}\label{sampling-statement-16}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{neg\_binomial\_2\_log\_glm}}\texttt{(x,\ alpha,\ beta,\ phi)}

Increment target log probability density with \texttt{neg\_binomial\_2\_log\_glm\_lupmf(y\ \textbar{}\ x,\ alpha,\ beta,\ phi)}.
\index{{\tt \bfseries neg\_binomial\_2\_log\_glm }!sampling statement|hyperpage}

\hypertarget{stan-functions-15}{%
\subsection{Stan functions}\label{stan-functions-15}}

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, real alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lupmf }!{\tt (int y \textbar\ matrix x, real alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}
dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, vector alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lupmf }!{\tt (int y \textbar\ matrix x, vector alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}
dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, real alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ row\_vector x, real alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}
dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}
dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, real alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ matrix x, real alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}
dropping constant additive terms.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}.

\index{{\tt \bfseries neg\_binomial\_2\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, vector beta, real phi): real}|hyperpage}

\texttt{real} \textbf{\texttt{neg\_binomial\_2\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ phi)}\newline
The log negative binomial probability mass of \texttt{y} given log-location
\texttt{alpha\ +\ x\ *\ beta} and inverse overdispersion parameter \texttt{phi}
dropping constant additive terms.

\hypertarget{poisson}{%
\section{Poisson distribution}\label{poisson}}

\hypertarget{probability-mass-function-13}{%
\subsection{Probability mass function}\label{probability-mass-function-13}}

If \(\lambda \in \mathbb{R}^+\), then for \(n \in \mathbb{N}\), \[
\text{Poisson}(n|\lambda) = \frac{1}{n!} \, \lambda^n \,
\exp(-\lambda). \]

\hypertarget{sampling-statement-17}{%
\subsection{Sampling statement}\label{sampling-statement-17}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{poisson}}\texttt{(lambda)}

Increment target log probability density with \texttt{poisson\_lupmf(n\ \textbar{}\ lambda)}.
\index{{\tt \bfseries poisson }!sampling statement|hyperpage}

\hypertarget{stan-functions-16}{%
\subsection{Stan functions}\label{stan-functions-16}}

\index{{\tt \bfseries poisson\_lpmf }!{\tt (ints n | reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_lpmf}}\texttt{(ints\ n\ \textbar{}\ reals\ lambda)}\newline
The log Poisson probability mass of n given rate lambda

\index{{\tt \bfseries poisson\_lupmf }!{\tt (ints n | reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_lupmf}}\texttt{(ints\ n\ \textbar{}\ reals\ lambda)}\newline
The log Poisson probability mass of n given rate lambda dropping constant
additive terms

\index{{\tt \bfseries poisson\_cdf }!{\tt (ints n, reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_cdf}}\texttt{(ints\ n,\ reals\ lambda)}\newline
The Poisson cumulative distribution function of n given rate lambda

\index{{\tt \bfseries poisson\_lcdf }!{\tt (ints n \textbar\ reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_lcdf}}\texttt{(ints\ n\ \textbar{}\ reals\ lambda)}\newline
The log of the Poisson cumulative distribution function of n given
rate lambda

\index{{\tt \bfseries poisson\_lccdf }!{\tt (ints n \textbar\ reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_lccdf}}\texttt{(ints\ n\ \textbar{}\ reals\ lambda)}\newline
The log of the Poisson complementary cumulative distribution function
of n given rate lambda

\index{{\tt \bfseries poisson\_rng }!{\tt (reals lambda): R}|hyperpage}

\texttt{R} \textbf{\texttt{poisson\_rng}}\texttt{(reals\ lambda)}\newline
Generate a Poisson variate with rate lambda; may only be used in
transformed data and generated quantities blocks. lambda must be less than
\(2^{30}\). For a description of argument and return types, see section
\protect\hyperlink{prob-vectorization}{vectorized function signatures}.

\hypertarget{poisson-distribution-log-parameterization}{%
\section{Poisson distribution, log parameterization}\label{poisson-distribution-log-parameterization}}

Stan also provides a parameterization of the Poisson using the log
rate \(\alpha = \log \lambda\) as a parameter. This is useful for
log-linear Poisson regressions so that the predictor does not need to
be exponentiated and passed into the standard Poisson probability
function.

\hypertarget{probability-mass-function-14}{%
\subsection{Probability mass function}\label{probability-mass-function-14}}

If \(\alpha \in \mathbb{R}\), then for \(n \in \mathbb{N}\), \[
\text{PoissonLog}(n|\alpha) = \frac{1}{n!} \, \exp \left(n\alpha -
\exp(\alpha) \right). \]

\hypertarget{sampling-statement-18}{%
\subsection{Sampling statement}\label{sampling-statement-18}}

\texttt{n\ \textasciitilde{}} \textbf{\texttt{poisson\_log}}\texttt{(alpha)}

Increment target log probability density with \texttt{poisson\_log\_lupmf(n\ \textbar{}\ alpha)}.
\index{{\tt \bfseries poisson\_log }!sampling statement|hyperpage}

\hypertarget{stan-functions-17}{%
\subsection{Stan functions}\label{stan-functions-17}}

\index{{\tt \bfseries poisson\_log\_lpmf }!{\tt (ints n \textbar\ reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_lpmf}}\texttt{(ints\ n\ \textbar{}\ reals\ alpha)}\newline
The log Poisson probability mass of n given log rate alpha

\index{{\tt \bfseries poisson\_log\_lupmf }!{\tt (ints n \textbar\ reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_lupmf}}\texttt{(ints\ n\ \textbar{}\ reals\ alpha)}\newline
The log Poisson probability mass of n given log rate alpha dropping constant
additive terms

\index{{\tt \bfseries poisson\_log\_rng }!{\tt (reals alpha): R}|hyperpage}

\texttt{R} \textbf{\texttt{poisson\_log\_rng}}\texttt{(reals\ alpha)}\newline
Generate a Poisson variate with log rate alpha; may only be used in
transformed data and generated quantities blocks. alpha must be less than
\(30 \log 2\). For a description of argument and return types, see section
\protect\hyperlink{prob-vectorization}{vectorized function signatures}.

\hypertarget{poisson-log-glm}{%
\section{Poisson-log generalized linear model (Poisson regression)}\label{poisson-log-glm}}

Stan also supplies a single function for a generalized linear model
with Poisson likelihood and log link function, i.e.~a function for a
Poisson regression. This provides a more efficient
implementation of Poisson regression than a manually written
regression in terms of a Poisson likelihood and matrix multiplication.

\hypertarget{probability-mass-function-15}{%
\subsection{Probability mass function}\label{probability-mass-function-15}}

If \(x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m\), then for \(y \in \mathbb{N}^n\), \[
\text{PoisonLogGLM}(y|x, \alpha, \beta) = \prod_{1\leq i \leq
n}\text{Poisson}(y_i|\exp(\alpha_i + x_i\cdot \beta)). \]

\hypertarget{sampling-statement-19}{%
\subsection{Sampling statement}\label{sampling-statement-19}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{poisson\_log\_glm}}\texttt{(x,\ alpha,\ beta)}

Increment target log probability density with \texttt{poisson\_log\_glm\_lupmf(y\ \textbar{}\ x,\ alpha,\ beta)}.
\index{{\tt \bfseries poisson\_log\_glm }!sampling statement|hyperpage}

\hypertarget{stan-functions-18}{%
\subsection{Stan functions}\label{stan-functions-18}}

\index{{\tt \bfseries poisson\_log\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries poisson\_log\_glm\_lupmf }!{\tt (int y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries poisson\_log\_glm\_lpmf }!{\tt (int y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lpmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries poisson\_log\_glm\_lupmf }!{\tt (int y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lupmf}}\texttt{(int\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries poisson\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries poisson\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ row\_vector x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries poisson\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries poisson\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ row\_vector x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries poisson\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries poisson\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ matrix x, real alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\index{{\tt \bfseries poisson\_log\_glm\_lpmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}.

\index{{\tt \bfseries poisson\_log\_glm\_lupmf }!{\tt (int[] y \textbar\ matrix x, vector alpha, vector beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{poisson\_log\_glm\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta)}\newline
The log Poisson probability mass of \texttt{y} given the log-rate \texttt{alpha\ +\ x\ *\ beta}
dropping constant additive terms.

\hypertarget{multivariate-discrete-distributions}{%
\chapter{Multivariate Discrete Distributions}\label{multivariate-discrete-distributions}}

The multivariate discrete distributions are over multiple integer
values, which are expressed in Stan as arrays.

\hypertarget{multinomial-distribution}{%
\section{Multinomial distribution}\label{multinomial-distribution}}

\hypertarget{probability-mass-function-16}{%
\subsection{Probability mass function}\label{probability-mass-function-16}}

If \(K \in \mathbb{N}\), \(N \in \mathbb{N}\), and \(\theta \in \text{$K$-simplex}\), then for \(y \in \mathbb{N}^K\) such that
\(\sum_{k=1}^K y_k = N\), \[ \text{Multinomial}(y|\theta) =
\binom{N}{y_1,\ldots,y_K} \prod_{k=1}^K \theta_k^{y_k}, \] where the
multinomial coefficient is defined by \[ \binom{N}{y_1,\ldots,y_k} =
\frac{N!}{\prod_{k=1}^K y_k!}. \]

\hypertarget{sampling-statement-20}{%
\subsection{Sampling statement}\label{sampling-statement-20}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multinomial}}\texttt{(theta)}

Increment target log probability density with \texttt{multinomial\_lupmf(y\ \textbar{}\ theta)}.
\index{{\tt \bfseries multinomial }!sampling statement|hyperpage}

\hypertarget{stan-functions-19}{%
\subsection{Stan functions}\label{stan-functions-19}}

\index{{\tt \bfseries multinomial\_lpmf }!{\tt (int[] y \textbar\ vector theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{multinomial\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ vector\ theta)}\newline
The log multinomial probability mass function with outcome array \texttt{y}
of size \(K\) given the \(K\)-simplex distribution parameter theta and
(implicit) total count \texttt{N\ =\ sum(y)}

\index{{\tt \bfseries multinomial\_lupmf }!{\tt (int[] y \textbar\ vector theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{multinomial\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ vector\ theta)}\newline
The log multinomial probability mass function with outcome array \texttt{y}
of size \(K\) given the \(K\)-simplex distribution parameter theta and
(implicit) total count \texttt{N\ =\ sum(y)} dropping constant additive terms

\index{{\tt \bfseries multinomial\_rng }!{\tt (vector theta, int N): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{multinomial\_rng}}\texttt{(vector\ theta,\ int\ N)}\newline
Generate a multinomial variate with simplex distribution parameter
theta and total count \(N\); may only be used in transformed data and
generated quantities blocks

\hypertarget{multinomial-distribution-logit-parameterization}{%
\section{Multinomial distribution, logit parameterization}\label{multinomial-distribution-logit-parameterization}}

Stan also provides a version of the multinomial probability mass
function distribution with the \(\text{$K$-simplex}\) for the event
count probabilities per category given on the unconstrained logistic
scale.

\hypertarget{probability-mass-function-17}{%
\subsection{Probability mass function}\label{probability-mass-function-17}}

If \(K \in \mathbb{N}\), \(N \in \mathbb{N}\), and \(\text{softmax}^{-1}(\theta) \in \text{$K$-simplex}\), then for \(y \in \mathbb{N}^K\) such that
\(\sum_{k=1}^K y_k = N\), \[
\text{MultinomialLogit}(y|\theta) = \text{Multinomial}(y|\text{softmax}^{-1}(\theta)) =
\binom{N}{y_1,\ldots,y_K} \prod_{k=1}^K [\text{softmax}^{-1}(\theta)_k]^{y_k}, \] where the
multinomial coefficient is defined by \[ \binom{N}{y_1,\ldots,y_k} =
\frac{N!}{\prod_{k=1}^K y_k!}. \]

\hypertarget{sampling-statement-21}{%
\subsection{Sampling statement}\label{sampling-statement-21}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multinomial\_logit}}\texttt{(theta)}

Increment target log probability density with \texttt{multinomial\_logit\_lupmf(y\ \textbar{}\ theta)}.
\index{{\tt \bfseries multinomial\_logit }!sampling statement|hyperpage}

\hypertarget{stan-functions-20}{%
\subsection{Stan functions}\label{stan-functions-20}}

\index{{\tt \bfseries multinomial\_logit\_lpmf }!{\tt (int[] y \textbar\ vector theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{multinomial\_logit\_lpmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ vector\ theta)}\newline
The log multinomial probability mass function with outcome array \texttt{y}
of size \(K\) given the \(K\)-simplex distribution parameter \(\text{softmax}^{-1}(\theta)\) and
(implicit) total count \texttt{N\ =\ sum(y)}

\index{{\tt \bfseries multinomial\_logit\_lupmf }!{\tt (int[] y \textbar\ vector theta): real}|hyperpage}

\texttt{real} \textbf{\texttt{multinomial\_logit\_lupmf}}\texttt{(int{[}{]}\ y\ \textbar{}\ vector\ theta)}\newline
The log multinomial probability mass function with outcome array \texttt{y}
of size \(K\) given the \(K\)-simplex distribution parameter \(\text{softmax}^{-1}(\theta)\) and (implicit) total count \texttt{N\ =\ sum(y)} dropping constant additive
terms

\index{{\tt \bfseries multinomial\_logit\_rng }!{\tt (vector theta, int N): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{multinomial\_logit\_rng}}\texttt{(vector\ theta,\ int\ N)}\newline
Generate a multinomial variate with simplex distribution parameter
\(\text{softmax}^{-1}(\theta)\) and total count \(N\); may only be used in transformed data and
generated quantities blocks

\hypertarget{continuous-distributions}{%
\chapter*{Continuous Distributions}\label{continuous-distributions}}
\addcontentsline{toc}{chapter}{Continuous Distributions}

\hypertarget{unbounded-continuous-distributions}{%
\chapter{Unbounded Continuous Distributions}\label{unbounded-continuous-distributions}}

The unbounded univariate continuous probability distributions have
support on all real numbers.

\hypertarget{normal-distribution}{%
\section{Normal distribution}\label{normal-distribution}}

\hypertarget{probability-density-function}{%
\subsection{Probability density function}\label{probability-density-function}}

If \(\mu \in \mathbb{R}\) and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[ \text{Normal}(y|\mu,\sigma) = \frac{1}{\sqrt{2 \pi} \
\sigma} \exp\left( - \, \frac{1}{2}            \left(  \frac{y -
\mu}{\sigma} \right)^2     \right) \!. \]

\hypertarget{sampling-statement-22}{%
\subsection{Sampling statement}\label{sampling-statement-22}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{normal}}\texttt{(mu,\ sigma)}

Increment target log probability density with \texttt{normal\_lupdf(y\ \textbar{}\ mu,\ sigma)}.
\index{{\tt \bfseries normal }!sampling statement|hyperpage}

\hypertarget{stan-functions-21}{%
\subsection{Stan functions}\label{stan-functions-21}}

\index{{\tt \bfseries normal\_lpdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the normal density of y given location mu and scale sigma

\index{{\tt \bfseries normal\_lupdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the normal density of y given location mu and scale sigma dropping
constant additive terms.

\index{{\tt \bfseries normal\_cdf }!{\tt (reals y, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ sigma)}\newline
The cumulative normal distribution of y given location mu and scale
sigma; normal\_cdf will underflow to 0 for \(\frac{{y}-{\mu}}{{\sigma}}\)
below -37.5 and overflow to 1 for \(\frac{{y}-{\mu}}{{\sigma}}\) above
8.25; the function \texttt{Phi\_approx} is more robust in the tails, but must
be scaled and translated for anything other than a standard normal.

\index{{\tt \bfseries normal\_lcdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the cumulative normal distribution of y given location mu
and scale sigma; normal\_lcdf will underflow to \(-\infty\) for
\(\frac{{y}-{\mu}}{{\sigma}}\) below -37.5 and overflow to 0 for
\(\frac{{y}-{\mu}}{{\sigma}}\) above 8.25; \texttt{log(Phi\_approx(...))} is more
robust in the tails, but must be scaled and translated for anything other
than a standard normal.

\index{{\tt \bfseries normal\_lccdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the complementary cumulative normal distribution of y given
location mu and scale sigma; normal\_lccdf will overflow to 0 for
\(\frac{{y}-{\mu}}{{\sigma}}\) below -37.5 and underflow to \(-\infty\)
for \(\frac{{y}-{\mu}}{{\sigma}}\) above 8.25; \texttt{log1m(Phi\_approx(...))} is
more robust in the tails, but must be scaled and translated for anything
other than a standard normal.

\index{{\tt \bfseries normal\_rng }!{\tt (reals mu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{normal\_rng}}\texttt{(reals\ mu,\ reals\ sigma)}\newline
Generate a normal variate with location mu and scale sigma; may only
be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{standard-normal-distribution}{%
\subsection{Standard normal distribution}\label{standard-normal-distribution}}

The standard normal distribution is so-called because its parameters
are the units for their respective operations---the location (mean) is
zero and the scale (standard deviation) one. The standard normal is
parameter-free, and the unit parameters allow considerable
simplification of the expression for the density. \[
\text{StdNormal}(y) \ = \ \text{Normal}(y \mid 0, 1) \ = \
\frac{1}{\sqrt{2 \pi}} \, \exp \left( \frac{-y^2}{2} \right)\!. \] Up
to a proportion on the log scale, where Stan computes, \[ \log
\text{Normal}(y \mid 0, 1) \ = \ \frac{-y^2}{2} + \text{const}. \]
With no logarithm, no subtraction, and no division by a parameter, the
standard normal log density is much more efficient to compute than the
normal log density with constant location \(0\) and scale \(1\).

\hypertarget{sampling-statement-23}{%
\subsection{Sampling statement}\label{sampling-statement-23}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{std\_normal}}\texttt{()}

Increment target log probability density with \texttt{std\_normal\_lupdf(y)}.
\index{{\tt \bfseries std\_normal }!sampling statement|hyperpage}

\hypertarget{stan-functions-22}{%
\subsection{Stan functions}\label{stan-functions-22}}

\index{{\tt \bfseries std\_normal\_lpdf }!{\tt (reals y): real}|hyperpage}

\texttt{real} \textbf{\texttt{std\_normal\_lpdf}}\texttt{(reals\ y)}\newline
The standard normal (location zero, scale one) log probability density
of y.

\index{{\tt \bfseries std\_normal\_lupdf }!{\tt (reals y): real}|hyperpage}

\texttt{real} \textbf{\texttt{std\_normal\_lupdf}}\texttt{(reals\ y)}\newline
The standard normal (location zero, scale one) log probability density
of y dropping constant additive terms.

\index{{\tt \bfseries std\_normal\_cdf }!{\tt (reals y): real}|hyperpage}

\texttt{real} \textbf{\texttt{std\_normal\_cdf}}\texttt{(reals\ y)}\newline
The cumulative standard normal distribution of y; std\_normal\_cdf will
underflow to 0 for \(y\) below -37.5 and overflow to 1 for \(y\) above 8.25;
the function \texttt{Phi\_approx} is more robust in the tails.

\index{{\tt \bfseries std\_normal\_lcdf }!{\tt (reals y): real}|hyperpage}

\texttt{real} \textbf{\texttt{std\_normal\_lcdf}}\texttt{(reals\ y)}\newline
The log of the cumulative standard normal distribution of y; std\_normal\_lcdf
will underflow to \(-\infty\) for \(y\) below -37.5 and overflow to 0 for \(y\)
above 8.25; \texttt{log(Phi\_approx(...))} is more robust in the tails.

\index{{\tt \bfseries std\_normal\_lccdf }!{\tt (reals y): real}|hyperpage}

\texttt{real} \textbf{\texttt{std\_normal\_lccdf}}\texttt{(reals\ y)}\newline
The log of the complementary cumulative standard normal distribution of y;
std\_normal\_lccdf will overflow to 0 for \(y\) below -37.5 and underflow to
\(-\infty\) for \(y\) above 8.25; \texttt{log1m(Phi\_approx(...))} is more robust in the
tails.

\index{{\tt \bfseries std\_normal\_rng }!{\tt (): real}|hyperpage}

\texttt{real} \textbf{\texttt{std\_normal\_rng}}\texttt{()}\newline
Generate a normal variate with location zero and scale one; may only
be used in transformed data and generated quantities blocks.

\hypertarget{normal-id-glm}{%
\section{Normal-id generalized linear model (linear regression)}\label{normal-id-glm}}

Stan also supplies a single function for a generalized linear lodel
with normal likelihood and identity link function, i.e.~a function
for a linear regression. This provides a more efficient
implementation of linear regression than a manually written regression
in terms of a normal likelihood and matrix multiplication.

\hypertarget{probability-distribution-function}{%
\subsection{Probability distribution function}\label{probability-distribution-function}}

If \(x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m, \sigma\in \mathbb{R}^+\), then for \(y \in \mathbb{R}^n\),
\[ \text{NormalIdGLM}(y|x, \alpha, \beta, \sigma) = \prod_{1\leq i
\leq n}\text{Normal}(y_i|\alpha_i + x_i\cdot \beta, \sigma). \]

\hypertarget{sampling-statement-24}{%
\subsection{Sampling statement}\label{sampling-statement-24}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{normal\_id\_glm}}\texttt{(x,\ alpha,\ beta,\ sigma)}

Increment target log probability density with \texttt{normal\_id\_glm\_lupdf(y\ \textbar{}\ x,\ alpha,\ beta,\ sigma)}.
\index{{\tt \bfseries normal\_id\_glm }!sampling statement|hyperpage}

\hypertarget{stan-functions-23}{%
\subsection{Stan functions}\label{stan-functions-23}}

\index{{\tt \bfseries normal\_id\_glm\_lpdf }!{\tt (real y \textbar\ matrix x, real alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lpdf}}\texttt{(real\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma}.

\index{{\tt \bfseries normal\_id\_glm\_lupdf }!{\tt (real y \textbar\ matrix x, real alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lupdf}}\texttt{(real\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma} dropping constant additive terms.

\index{{\tt \bfseries normal\_id\_glm\_lpdf }!{\tt (real y \textbar\ matrix x, vector alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lpdf}}\texttt{(real\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma}.

\index{{\tt \bfseries normal\_id\_glm\_lupdf }!{\tt (real y \textbar\ matrix x, vector alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lupdf}}\texttt{(real\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma} dropping constant additive terms.

\index{{\tt \bfseries normal\_id\_glm\_lpdf }!{\tt (vector y \textbar\ row\_vector x, real alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lpdf}}\texttt{(vector\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma}.

\index{{\tt \bfseries normal\_id\_glm\_lupdf }!{\tt (vector y \textbar\ row\_vector x, real alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lupdf}}\texttt{(vector\ y\ \textbar{}\ row\_vector\ x,\ real\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma} dropping constant additive terms.

\index{{\tt \bfseries normal\_id\_glm\_lpdf }!{\tt (vector y \textbar\ row\_vector x, vector alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lpdf}}\texttt{(vector\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma}.

\index{{\tt \bfseries normal\_id\_glm\_lupdf }!{\tt (vector y \textbar\ row\_vector x, vector alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lupdf}}\texttt{(vector\ y\ \textbar{}\ row\_vector\ x,\ vector\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma} dropping constant additive terms.

\index{{\tt \bfseries normal\_id\_glm\_lpdf }!{\tt (vector y \textbar\ matrix x, real alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lpdf}}\texttt{(vector\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma}.

\index{{\tt \bfseries normal\_id\_glm\_lupdf }!{\tt (vector y \textbar\ matrix x, real alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lupdf}}\texttt{(vector\ y\ \textbar{}\ matrix\ x,\ real\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma} dropping constant additive terms.

\index{{\tt \bfseries normal\_id\_glm\_lpdf }!{\tt (vector y \textbar\ matrix x, vector alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lpdf}}\texttt{(vector\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma}.

\index{{\tt \bfseries normal\_id\_glm\_lupdf }!{\tt (vector y \textbar\ matrix x, vector alpha, vector beta, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{normal\_id\_glm\_lupdf}}\texttt{(vector\ y\ \textbar{}\ matrix\ x,\ vector\ alpha,\ vector\ beta,\ real\ sigma)}\newline
The log normal probability density of \texttt{y} given location \texttt{alpha\ +\ x\ *\ beta}
and scale \texttt{sigma} dropping constant additive terms.

\hypertarget{exponentially-modified-normal-distribution}{%
\section{Exponentially modified normal distribution}\label{exponentially-modified-normal-distribution}}

\hypertarget{probability-density-function-1}{%
\subsection{Probability density function}\label{probability-density-function-1}}

If \(\mu \in \mathbb{R}\), \(\sigma \in \mathbb{R}^+\), and \(\lambda \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[
\text{ExpModNormal}(y|\mu,\sigma,\lambda) = \frac{\lambda}{2} \ \exp
\left(\frac{\lambda}{2} \left(2\mu + \lambda \sigma^2 -
2y\right)\right) \text{erfc}\left(\frac{\mu + \lambda\sigma^2 -
y}{\sqrt{2}\sigma}\right) . \]

\hypertarget{sampling-statement-25}{%
\subsection{Sampling statement}\label{sampling-statement-25}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{exp\_mod\_normal}}\texttt{(mu,\ sigma,\ lambda)}

Increment target log probability density with \texttt{exp\_mod\_normal\_lupdf(y\ \textbar{}\ mu,\ sigma,\ lambda)}.
\index{{\tt \bfseries exp\_mod\_normal }!sampling statement|hyperpage}

\hypertarget{stan-functions-24}{%
\subsection{Stan functions}\label{stan-functions-24}}

\index{{\tt \bfseries exp\_mod\_normal\_lpdf }!{\tt (reals y | reals mu, reals sigma, reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{exp\_mod\_normal\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma,\ reals\ lambda)}\newline
The log of the exponentially modified normal density of y given
location mu, scale sigma, and shape lambda

\index{{\tt \bfseries exp\_mod\_normal\_lupdf }!{\tt (reals y | reals mu, reals sigma, reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{exp\_mod\_normal\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma,\ reals\ lambda)}\newline
The log of the exponentially modified normal density of y given
location mu, scale sigma, and shape lambda dropping constant additive terms

\index{{\tt \bfseries exp\_mod\_normal\_cdf }!{\tt (reals y, reals mu, reals sigma, reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{exp\_mod\_normal\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ sigma,\ reals\ lambda)}\newline
The exponentially modified normal cumulative distribution function of
y given location mu, scale sigma, and shape lambda

\index{{\tt \bfseries exp\_mod\_normal\_lcdf }!{\tt (reals y | reals mu, reals sigma, reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{exp\_mod\_normal\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma,\ reals\ lambda)}\newline
The log of the exponentially modified normal cumulative distribution
function of y given location mu, scale sigma, and shape lambda

\index{{\tt \bfseries exp\_mod\_normal\_lccdf }!{\tt (reals y \textbar\ reals mu, reals sigma, reals lambda): real}|hyperpage}

\texttt{real} \textbf{\texttt{exp\_mod\_normal\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma,\ reals\ lambda)}\newline
The log of the exponentially modified normal complementary cumulative
distribution function of y given location mu, scale sigma, and shape
lambda

\index{{\tt \bfseries exp\_mod\_normal\_rng }!{\tt (reals mu, reals sigma, reals lambda): R}|hyperpage}

\texttt{R} \textbf{\texttt{exp\_mod\_normal\_rng}}\texttt{(reals\ mu,\ reals\ sigma,\ reals\ lambda)}\newline
Generate a exponentially modified normal variate with location mu,
scale sigma, and shape lambda; may only be used in transformed data and generated
quantities blocks. For a description of argument and return types, see
section \protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{skew-normal-distribution}{%
\section{Skew normal distribution}\label{skew-normal-distribution}}

\hypertarget{probability-density-function-2}{%
\subsection{Probability density function}\label{probability-density-function-2}}

If \(\xi \in \mathbb{R}\), \(\omega \in \mathbb{R}^+\), and \(\alpha \in \mathbb{R}\), then for \(y \in \mathbb{R}\), \[ \text{SkewNormal}(y \mid
\xi, \omega, \alpha) = \frac{1}{\omega\sqrt{2\pi}} \ \exp\left( - \,
\frac{1}{2}            \left(  \frac{y - \xi}{\omega} \right)^2
\right) \ \left(1 + \text{erf}\left( \alpha\left(\frac{y -
\xi}{\omega\sqrt{2}}\right)\right)\right) . \]

\hypertarget{sampling-statement-26}{%
\subsection{Sampling statement}\label{sampling-statement-26}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{skew\_normal}}\texttt{(xi,\ omega,\ alpha)}

Increment target log probability density with \texttt{skew\_normal\_lupdf(y\ \textbar{}\ xi,\ omega,\ alpha)}.
\index{{\tt \bfseries skew\_normal }!sampling statement|hyperpage}

\hypertarget{stan-functions-25}{%
\subsection{Stan functions}\label{stan-functions-25}}

\index{{\tt \bfseries skew\_normal\_lpdf }!{\tt (reals y \textbar\ reals xi, reals omega, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{skew\_normal\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ xi,\ reals\ omega,\ reals\ alpha)}\newline
The log of the skew normal density of y given location xi, scale
omega, and shape alpha

\index{{\tt \bfseries skew\_normal\_lupdf }!{\tt (reals y \textbar\ reals xi, reals omega, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{skew\_normal\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ xi,\ reals\ omega,\ reals\ alpha)}\newline
The log of the skew normal density of y given location xi, scale
omega, and shape alpha dropping constant additive terms

\index{{\tt \bfseries skew\_normal\_cdf }!{\tt (reals y, reals xi, reals omega, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{skew\_normal\_cdf}}\texttt{(reals\ y,\ reals\ xi,\ reals\ omega,\ reals\ alpha)}\newline
The skew normal distribution function of y given location xi, scale
omega, and shape alpha

\index{{\tt \bfseries skew\_normal\_lcdf }!{\tt (reals y \textbar\ reals xi, reals omega, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{skew\_normal\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ xi,\ reals\ omega,\ reals\ alpha)}\newline
The log of the skew normal cumulative distribution function of y given
location xi, scale omega, and shape alpha

\index{{\tt \bfseries skew\_normal\_lccdf }!{\tt (reals y \textbar\ reals xi, reals omega, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{skew\_normal\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ xi,\ reals\ omega,\ reals\ alpha)}\newline
The log of the skew normal complementary cumulative distribution
function of y given location xi, scale omega, and shape alpha

\index{{\tt \bfseries skew\_normal\_rng }!{\tt (reals xi, reals omega, real alpha): R}|hyperpage}

\texttt{R} \textbf{\texttt{skew\_normal\_rng}}\texttt{(reals\ xi,\ reals\ omega,\ real\ alpha)}\newline
Generate a skew normal variate with location xi, scale omega, and
shape alpha; may only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{student-t-distribution}{%
\section{Student-t distribution}\label{student-t-distribution}}

\hypertarget{probability-density-function-3}{%
\subsection{Probability density function}\label{probability-density-function-3}}

If \(\nu \in \mathbb{R}^+\), \(\mu \in \mathbb{R}\), and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[
\text{StudentT}(y|\nu,\mu,\sigma) = \frac{\Gamma\left((\nu +
1)/2\right)}      {\Gamma(\nu/2)} \ \frac{1}{\sqrt{\nu \pi} \ \sigma}
\ \left( 1 + \frac{1}{\nu} \left(\frac{y - \mu}{\sigma}\right)^2
\right)^{-(\nu + 1)/2} \! . \]

\hypertarget{sampling-statement-27}{%
\subsection{Sampling statement}\label{sampling-statement-27}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{student\_t}}\texttt{(nu,\ mu,\ sigma)}

Increment target log probability density with \texttt{student\_t\_lupdf(y\ \textbar{}\ nu,\ mu,\ sigma)}.
\index{{\tt \bfseries student\_t }!sampling statement|hyperpage}

\hypertarget{stan-functions-26}{%
\subsection{Stan functions}\label{stan-functions-26}}

\index{{\tt \bfseries student\_t\_lpdf }!{\tt (reals y \textbar\ reals nu, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{student\_t\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ mu,\ reals\ sigma)}\newline
The log of the Student-\(t\) density of y given degrees of freedom nu,
location mu, and scale sigma

\index{{\tt \bfseries student\_t\_lupdf }!{\tt (reals y \textbar\ reals nu, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{student\_t\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ mu,\ reals\ sigma)}\newline
The log of the Student-\(t\) density of y given degrees of freedom nu,
location mu, and scale sigma dropping constant additive terms

\index{{\tt \bfseries student\_t\_cdf }!{\tt (reals y, reals nu, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{student\_t\_cdf}}\texttt{(reals\ y,\ reals\ nu,\ reals\ mu,\ reals\ sigma)}\newline
The Student-\(t\) cumulative distribution function of y given degrees of
freedom nu, location mu, and scale sigma

\index{{\tt \bfseries student\_t\_lcdf }!{\tt (reals y \textbar\ reals nu, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{student\_t\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ mu,\ reals\ sigma)}\newline
The log of the Student-\(t\) cumulative distribution function of y given
degrees of freedom nu, location mu, and scale sigma

\index{{\tt \bfseries student\_t\_lccdf }!{\tt (reals y \textbar\ reals nu, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{student\_t\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ mu,\ reals\ sigma)}\newline
The log of the Student-\(t\) complementary cumulative distribution
function of y given degrees of freedom nu, location mu, and scale
sigma

\index{{\tt \bfseries student\_t\_rng }!{\tt (reals nu, reals mu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{student\_t\_rng}}\texttt{(reals\ nu,\ reals\ mu,\ reals\ sigma)}\newline
Generate a Student-\(t\) variate with degrees of freedom nu, location
mu, and scale sigma; may only be used in transformed data and generated
quantities blocks. For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{cauchy-distribution}{%
\section{Cauchy distribution}\label{cauchy-distribution}}

\hypertarget{probability-density-function-4}{%
\subsection{Probability density function}\label{probability-density-function-4}}

If \(\mu \in \mathbb{R}\) and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[ \text{Cauchy}(y|\mu,\sigma) = \frac{1}{\pi \sigma} \
\frac{1}{1 + \left((y - \mu)/\sigma\right)^2} . \]

\hypertarget{sampling-statement-28}{%
\subsection{Sampling statement}\label{sampling-statement-28}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{cauchy}}\texttt{(mu,\ sigma)}

Increment target log probability density with \texttt{cauchy\_lupdf(y\ \textbar{}\ mu,\ sigma)}.
\index{{\tt \bfseries cauchy }!sampling statement|hyperpage}

\hypertarget{stan-functions-27}{%
\subsection{Stan functions}\label{stan-functions-27}}

\index{{\tt \bfseries cauchy\_lpdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{cauchy\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the Cauchy density of y given location mu and scale sigma

\index{{\tt \bfseries cauchy\_lupdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{cauchy\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the Cauchy density of y given location mu and scale sigma
dropping constant additive terms

\index{{\tt \bfseries cauchy\_cdf }!{\tt (reals y, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{cauchy\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ sigma)}\newline
The Cauchy cumulative distribution function of y given location mu and
scale sigma

\index{{\tt \bfseries cauchy\_lcdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{cauchy\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the Cauchy cumulative distribution function of y given
location mu and scale sigma

\index{{\tt \bfseries cauchy\_lccdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{cauchy\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the Cauchy complementary cumulative distribution function
of y given location mu and scale sigma

\index{{\tt \bfseries cauchy\_rng }!{\tt (reals mu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{cauchy\_rng}}\texttt{(reals\ mu,\ reals\ sigma)}\newline
Generate a Cauchy variate with location mu and scale sigma; may only
be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{double-exponential-laplace-distribution}{%
\section{Double exponential (Laplace) distribution}\label{double-exponential-laplace-distribution}}

\hypertarget{probability-density-function-5}{%
\subsection{Probability density function}\label{probability-density-function-5}}

If \(\mu \in \mathbb{R}\) and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[ \text{DoubleExponential}(y|\mu,\sigma) =
\frac{1}{2\sigma}   \exp \left( - \, \frac{|y - \mu|}{\sigma} \right)
. \] Note that the double exponential distribution is parameterized in
terms of the scale, in contrast to the exponential distribution (see
section \protect\hyperlink{exponential-distribution}{exponential distribution}), which is
parameterized in terms of inverse scale.

The double-exponential distribution can be defined as a compound
exponential-normal distribution (\protect\hyperlink{ref-Ding:18}{Ding and Blitzstein 2018}). Using the inverse scale
parameterization for the exponential distribution, and the standard deviation
parameterization for the normal distribution, one can write \[ \alpha \sim
\mathsf{Exponential}\left( \frac{1}{2 \sigma^2} \right) \] and \[ \beta \mid
\alpha \sim \mathsf{Normal}(\mu, \sqrt{\alpha}), \] then \[ \beta \sim
\mathsf{DoubleExponential}(\mu, \sigma ). \] This may be used to code
a non-centered parameterization by taking \[ \beta^{\text{raw}} \sim
\mathsf{Normal}(0, 1) \] and defining \[ \beta = \mu + \alpha \,
\beta^{\text{raw}}. \]

\hypertarget{sampling-statement-29}{%
\subsection{Sampling statement}\label{sampling-statement-29}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{double\_exponential}}\texttt{(mu,\ sigma)}

Increment target log probability density with \texttt{double\_exponential\_lupdf(y\ \textbar{}\ mu,\ sigma)}.
\index{{\tt \bfseries double\_exponential }!sampling statement|hyperpage}

\hypertarget{stan-functions-28}{%
\subsection{Stan functions}\label{stan-functions-28}}

\index{{\tt \bfseries double\_exponential\_lpdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{double\_exponential\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the double exponential density of y given location mu and
scale sigma

\index{{\tt \bfseries double\_exponential\_lupdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{double\_exponential\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the double exponential density of y given location mu and
scale sigma dropping constant additive terms

\index{{\tt \bfseries double\_exponential\_cdf }!{\tt (reals y, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{double\_exponential\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ sigma)}\newline
The double exponential cumulative distribution function of y given
location mu and scale sigma

\index{{\tt \bfseries double\_exponential\_lcdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{double\_exponential\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the double exponential cumulative distribution function of
y given location mu and scale sigma

\index{{\tt \bfseries double\_exponential\_lccdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{double\_exponential\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the double exponential complementary cumulative
distribution function of y given location mu and scale sigma

\index{{\tt \bfseries double\_exponential\_rng }!{\tt (reals mu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{double\_exponential\_rng}}\texttt{(reals\ mu,\ reals\ sigma)}\newline
Generate a double exponential variate with location mu and scale
sigma; may only be used in transformed data and generated quantities blocks. For a
description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{logistic-distribution}{%
\section{Logistic distribution}\label{logistic-distribution}}

\hypertarget{probability-density-function-6}{%
\subsection{Probability density function}\label{probability-density-function-6}}

If \(\mu \in \mathbb{R}\) and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[ \text{Logistic}(y|\mu,\sigma) = \frac{1}{\sigma} \
\exp\!\left( - \, \frac{y - \mu}{\sigma} \right) \ \left(1 + \exp
\!\left( - \, \frac{y - \mu}{\sigma} \right) \right)^{\!-2} \! . \]

\hypertarget{sampling-statement-30}{%
\subsection{Sampling statement}\label{sampling-statement-30}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{logistic}}\texttt{(mu,\ sigma)}

Increment target log probability density with \texttt{logistic\_lupdf(y\ \textbar{}\ mu,\ sigma)}.
\index{{\tt \bfseries logistic }!sampling statement|hyperpage}

\hypertarget{stan-functions-29}{%
\subsection{Stan functions}\label{stan-functions-29}}

\index{{\tt \bfseries logistic\_lpdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{logistic\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the logistic density of y given location mu and scale sigma

\index{{\tt \bfseries logistic\_lupdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{logistic\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the logistic density of y given location mu and scale sigma
dropping constant additive terms

\index{{\tt \bfseries logistic\_cdf }!{\tt (reals y, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{logistic\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ sigma)}\newline
The logistic cumulative distribution function of y given location mu
and scale sigma

\index{{\tt \bfseries logistic\_lcdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{logistic\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the logistic cumulative distribution function of y given
location mu and scale sigma

\index{{\tt \bfseries logistic\_lccdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{logistic\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the logistic complementary cumulative distribution function
of y given location mu and scale sigma

\index{{\tt \bfseries logistic\_rng}!{\tt (reals mu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{logistic\_rng}}\texttt{(reals\ mu,\ reals\ sigma)}\newline
Generate a logistic variate with location mu and scale sigma; may only
be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{gumbel-distribution}{%
\section{Gumbel distribution}\label{gumbel-distribution}}

\hypertarget{probability-density-function-7}{%
\subsection{Probability density function}\label{probability-density-function-7}}

If \(\mu \in \mathbb{R}\) and \(\beta \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[ \text{Gumbel}(y|\mu,\beta) = \frac{1}{\beta} \
\exp\left(-\frac{y-\mu}{\beta}-\exp\left(-\frac{y-\mu}{\beta}\right)\right)
. \]

\hypertarget{sampling-statement-31}{%
\subsection{Sampling statement}\label{sampling-statement-31}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{gumbel}}\texttt{(mu,\ beta)}

Increment target log probability density with \texttt{gumbel\_lupdf(y\ \textbar{}\ mu,\ beta)}.
\index{{\tt \bfseries gumbel }!sampling statement|hyperpage}

\hypertarget{stan-functions-30}{%
\subsection{Stan functions}\label{stan-functions-30}}

\index{{\tt \bfseries gumbel\_lpdf }!{\tt (reals y \textbar\ reals mu, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gumbel\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ beta)}\newline
The log of the gumbel density of y given location mu and scale beta

\index{{\tt \bfseries gumbel\_lupdf }!{\tt (reals y \textbar\ reals mu, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gumbel\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ beta)}\newline
The log of the gumbel density of y given location mu and scale beta
dropping constant additive terms

\index{{\tt \bfseries gumbel\_cdf }!{\tt (reals y, reals mu, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gumbel\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ beta)}\newline
The gumbel cumulative distribution function of y given location mu and
scale beta

\index{{\tt \bfseries gumbel\_lcdf }!{\tt (reals y \textbar\ reals mu, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gumbel\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ beta)}\newline
The log of the gumbel cumulative distribution function of y given
location mu and scale beta

\index{{\tt \bfseries gumbel\_lccdf }!{\tt (reals y \textbar\ reals mu, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gumbel\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ beta)}\newline
The log of the gumbel complementary cumulative distribution function
of y given location mu and scale beta

\index{{\tt \bfseries gumbel\_rng }!{\tt (reals mu, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{gumbel\_rng}}\texttt{(reals\ mu,\ reals\ beta)}\newline
Generate a gumbel variate with location mu and scale beta; may only be
used in transformed data and generated quantities blocks. For a description
of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{positive-continuous-distributions}{%
\chapter{Positive Continuous Distributions}\label{positive-continuous-distributions}}

The positive continuous probability functions have support on the
positive real numbers.

\hypertarget{lognormal}{%
\section{Lognormal distribution}\label{lognormal}}

\hypertarget{probability-density-function-8}{%
\subsection{Probability density function}\label{probability-density-function-8}}

If \(\mu \in \mathbb{R}\) and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[ \text{LogNormal}(y|\mu,\sigma) = \frac{1}{\sqrt{2
\pi} \ \sigma} \, \frac{1}{y} \ \exp \! \left(        - \, \frac{1}{2}
\, \left( \frac{\log y - \mu}{\sigma} \right)^2      \right) . \]

\hypertarget{sampling-statement-32}{%
\subsection{Sampling statement}\label{sampling-statement-32}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{lognormal}}\texttt{(mu,\ sigma)}

Increment target log probability density with \texttt{lognormal\_lupdf(y\ \textbar{}\ mu,\ sigma)}.
\index{{\tt \bfseries lognormal }!sampling statement|hyperpage}

\hypertarget{stan-functions-31}{%
\subsection{Stan functions}\label{stan-functions-31}}

\index{{\tt \bfseries lognormal\_lpdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{lognormal\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the lognormal density of y given location mu and scale
sigma

\index{{\tt \bfseries lognormal\_lupdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{lognormal\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the lognormal density of y given location mu and scale
sigma dropping constant additive terms

\index{{\tt \bfseries lognormal\_cdf }!{\tt (reals y, reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{lognormal\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ sigma)}\newline
The cumulative lognormal distribution function of y given location mu
and scale sigma

\index{{\tt \bfseries lognormal\_lcdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{lognormal\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the lognormal cumulative distribution function of y given
location mu and scale sigma

\index{{\tt \bfseries lognormal\_lccdf }!{\tt (reals y \textbar\ reals mu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{lognormal\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ sigma)}\newline
The log of the lognormal complementary cumulative distribution
function of y given location mu and scale sigma

\index{{\tt \bfseries lognormal\_rng }!{\tt (reals mu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{lognormal\_rng}}\texttt{(reals\ mu,\ reals\ sigma)}\newline
Generate a lognormal variate with location mu and scale sigma; may
only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{chi-square-distribution}{%
\section{Chi-square distribution}\label{chi-square-distribution}}

\hypertarget{probability-density-function-9}{%
\subsection{Probability density function}\label{probability-density-function-9}}

If \(\nu \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[
\text{ChiSquare}(y|\nu) = \frac{2^{-\nu/2}}     {\Gamma(\nu / 2)} \,
y^{\nu/2 - 1} \, \exp \! \left( -\, \frac{1}{2} \, y \right) . \]

\hypertarget{sampling-statement-33}{%
\subsection{Sampling statement}\label{sampling-statement-33}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{chi\_square}}\texttt{(nu)}

Increment target log probability density with \texttt{chi\_square\_lupdf(y\ \textbar{}\ nu)}.
\index{{\tt \bfseries chi\_square }!sampling statement|hyperpage}

\hypertarget{stan-functions-32}{%
\subsection{Stan functions}\label{stan-functions-32}}

\index{{\tt \bfseries chi\_square\_lpdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{chi\_square\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the Chi-square density of y given degrees of freedom nu

\index{{\tt \bfseries chi\_square\_lupdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{chi\_square\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the Chi-square density of y given degrees of freedom nu
dropping constant additive terms

\index{{\tt \bfseries chi\_square\_cdf }!{\tt (reals y, reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{chi\_square\_cdf}}\texttt{(reals\ y,\ reals\ nu)}\newline
The Chi-square cumulative distribution function of y given degrees of
freedom nu

\index{{\tt \bfseries chi\_square\_lcdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{chi\_square\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the Chi-square cumulative distribution function of y given
degrees of freedom nu

\index{{\tt \bfseries chi\_square\_lccdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{chi\_square\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the complementary Chi-square cumulative distribution
function of y given degrees of freedom nu

\index{{\tt \bfseries chi\_square\_rng }!{\tt (reals nu): R}|hyperpage}

\texttt{R} \textbf{\texttt{chi\_square\_rng}}\texttt{(reals\ nu)}\newline
Generate a Chi-square variate with degrees of freedom nu; may only be
used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{inverse-chi-square-distribution}{%
\section{Inverse chi-square distribution}\label{inverse-chi-square-distribution}}

\hypertarget{probability-density-function-10}{%
\subsection{Probability density function}\label{probability-density-function-10}}

If \(\nu \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[
\text{InvChiSquare}(y \, | \, \nu) = \frac{2^{-\nu/2}}    {\Gamma(\nu
/ 2)} \, y^{-\nu/2 - 1} \, \exp\! \left( \! - \, \frac{1}{2} \,
\frac{1}{y} \right) . \]

\hypertarget{sampling-statement-34}{%
\subsection{Sampling statement}\label{sampling-statement-34}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{inv\_chi\_square}}\texttt{(nu)}

Increment target log probability density with \texttt{inv\_chi\_square\_lupdf(y\ \textbar{}\ nu)}.
\index{{\tt \bfseries inv\_chi\_square }!sampling statement|hyperpage}

\hypertarget{stan-functions-33}{%
\subsection{Stan functions}\label{stan-functions-33}}

\index{{\tt \bfseries inv\_chi\_square\_lpdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_chi\_square\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the inverse Chi-square density of y given degrees of
freedom nu

\index{{\tt \bfseries inv\_chi\_square\_lupdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_chi\_square\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the inverse Chi-square density of y given degrees of
freedom nu dropping constant additive terms

\index{{\tt \bfseries inv\_chi\_square\_cdf }!{\tt (reals y, reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_chi\_square\_cdf}}\texttt{(reals\ y,\ reals\ nu)}\newline
The inverse Chi-squared cumulative distribution function of y given
degrees of freedom nu

\index{{\tt \bfseries inv\_chi\_square\_lcdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_chi\_square\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the inverse Chi-squared cumulative distribution function of
y given degrees of freedom nu

\index{{\tt \bfseries inv\_chi\_square\_lccdf }!{\tt (reals y \textbar\ reals nu): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_chi\_square\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu)}\newline
The log of the inverse Chi-squared complementary cumulative
distribution function of y given degrees of freedom nu

\index{{\tt \bfseries inv\_chi\_square\_rng }!{\tt (reals nu): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_chi\_square\_rng}}\texttt{(reals\ nu)}\newline
Generate an inverse Chi-squared variate with degrees of freedom nu;
may only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{scaled-inverse-chi-square-distribution}{%
\section{Scaled inverse chi-square distribution}\label{scaled-inverse-chi-square-distribution}}

\hypertarget{probability-density-function-11}{%
\subsection{Probability density function}\label{probability-density-function-11}}

If \(\nu \in \mathbb{R}^+\) and \(\sigma \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[ \text{ScaledInvChiSquare}(y|\nu,\sigma) =
\frac{(\nu / 2)^{\nu/2}}      {\Gamma(\nu / 2)} \, \sigma^{\nu} \,
y^{-(\nu/2 + 1)} \, \exp \! \left( \!    - \, \frac{1}{2} \, \nu \,
\sigma^2 \, \frac{1}{y} \right) . \]

\hypertarget{sampling-statement-35}{%
\subsection{Sampling statement}\label{sampling-statement-35}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{scaled\_inv\_chi\_square}}\texttt{(nu,\ sigma)}

Increment target log probability density with \texttt{scaled\_inv\_chi\_square\_lupdf(y\ \textbar{}\ nu,\ sigma)}.
\index{{\tt \bfseries scaled\_inv\_chi\_square }!sampling statement|hyperpage}

\hypertarget{stan-functions-34}{%
\subsection{Stan functions}\label{stan-functions-34}}

\index{{\tt \bfseries scaled\_inv\_chi\_square\_lpdf }!{\tt (reals y \textbar\ reals nu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{scaled\_inv\_chi\_square\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ sigma)}\newline
The log of the scaled inverse Chi-square density of y given degrees of
freedom nu and scale sigma

\index{{\tt \bfseries scaled\_inv\_chi\_square\_lupdf }!{\tt (reals y \textbar\ reals nu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{scaled\_inv\_chi\_square\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ sigma)}\newline
The log of the scaled inverse Chi-square density of y given degrees of
freedom nu and scale sigma dropping constant additive terms

\index{{\tt \bfseries scaled\_inv\_chi\_square\_cdf }!{\tt (reals y, reals nu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{scaled\_inv\_chi\_square\_cdf}}\texttt{(reals\ y,\ reals\ nu,\ reals\ sigma)}\newline
The scaled inverse Chi-square cumulative distribution function of y
given degrees of freedom nu and scale sigma

\index{{\tt \bfseries scaled\_inv\_chi\_square\_lcdf }!{\tt (reals y \textbar\ reals nu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{scaled\_inv\_chi\_square\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ sigma)}\newline
The log of the scaled inverse Chi-square cumulative distribution
function of y given degrees of freedom nu and scale sigma

\index{{\tt \bfseries scaled\_inv\_chi\_square\_lccdf }!{\tt (reals y \textbar\ reals nu, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{scaled\_inv\_chi\_square\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ nu,\ reals\ sigma)}\newline
The log of the scaled inverse Chi-square complementary cumulative
distribution function of y given degrees of freedom nu and scale sigma

\index{{\tt \bfseries scaled\_inv\_chi\_square\_rng }!{\tt (reals nu, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{scaled\_inv\_chi\_square\_rng}}\texttt{(reals\ nu,\ reals\ sigma)}\newline
Generate a scaled inverse Chi-squared variate with degrees of freedom
nu and scale sigma; may only be used in transformed data and generated
quantities blocks. For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{exponential-distribution}{%
\section{Exponential distribution}\label{exponential-distribution}}

\hypertarget{probability-density-function-12}{%
\subsection{Probability density function}\label{probability-density-function-12}}

If \(\beta \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[
\text{Exponential}(y|\beta) = \beta \, \exp ( - \beta \, y ) . \]

\hypertarget{sampling-statement-36}{%
\subsection{Sampling statement}\label{sampling-statement-36}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{exponential}}\texttt{(beta)}

Increment target log probability density with \texttt{exponential\_lupdf(y\ \textbar{}\ beta)}.
\index{{\tt \bfseries exponential }!sampling statement|hyperpage}

\hypertarget{stan-functions-35}{%
\subsection{Stan functions}\label{stan-functions-35}}

\index{{\tt \bfseries exponential\_lpdf }!{\tt (reals y \textbar\ reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{exponential\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ beta)}\newline
The log of the exponential density of y given inverse scale beta

\index{{\tt \bfseries exponential\_lupdf }!{\tt (reals y \textbar\ reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{exponential\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ beta)}\newline
The log of the exponential density of y given inverse scale beta
dropping constant additive terms

\index{{\tt \bfseries exponential\_cdf }!{\tt (reals y, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{exponential\_cdf}}\texttt{(reals\ y,\ reals\ beta)}\newline
The exponential cumulative distribution function of y given inverse
scale beta

\index{{\tt \bfseries exponential\_lcdf }!{\tt (reals y \textbar\ reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{exponential\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ beta)}\newline
The log of the exponential cumulative distribution function of y given
inverse scale beta

\index{{\tt \bfseries exponential\_lccdf }!{\tt (reals y \textbar\ reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{exponential\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ beta)}\newline
The log of the exponential complementary cumulative distribution
function of y given inverse scale beta

\index{{\tt \bfseries exponential\_rng }!{\tt (reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{exponential\_rng}}\texttt{(reals\ beta)}\newline
Generate an exponential variate with inverse scale beta; may only be
used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{gamma-distribution}{%
\section{Gamma distribution}\label{gamma-distribution}}

\hypertarget{probability-density-function-13}{%
\subsection{Probability density function}\label{probability-density-function-13}}

If \(\alpha \in \mathbb{R}^+\) and \(\beta \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[ \text{Gamma}(y|\alpha,\beta) =
\frac{\beta^{\alpha}}      {\Gamma(\alpha)} \, y^{\alpha - 1}
\exp(-\beta \, y) . \]

\hypertarget{sampling-statement-37}{%
\subsection{Sampling statement}\label{sampling-statement-37}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{gamma}}\texttt{(alpha,\ beta)}

Increment target log probability density with \texttt{gamma\_lupdf(y\ \textbar{}\ alpha,\ beta)}.
\index{{\tt \bfseries gamma }!sampling statement|hyperpage}

\hypertarget{stan-functions-36}{%
\subsection{Stan functions}\label{stan-functions-36}}

\index{{\tt \bfseries gamma\_lpdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the gamma density of y given shape alpha and inverse scale
beta

\index{{\tt \bfseries gamma\_lupdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the gamma density of y given shape alpha and inverse scale
beta dropping constant additive terms

\index{{\tt \bfseries gamma\_cdf }!{\tt (reals y, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_cdf}}\texttt{(reals\ y,\ reals\ alpha,\ reals\ beta)}\newline
The cumulative gamma distribution function of y given shape alpha and
inverse scale beta

\index{{\tt \bfseries gamma\_lcdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the cumulative gamma distribution function of y given shape
alpha and inverse scale beta

\index{{\tt \bfseries gamma\_lccdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{gamma\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the complementary cumulative gamma distribution function of
y given shape alpha and inverse scale beta

\index{{\tt \bfseries gamma\_rng }!{\tt (reals alpha, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{gamma\_rng}}\texttt{(reals\ alpha,\ reals\ beta)}\newline
Generate a gamma variate with shape alpha and inverse scale beta; may
only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{inverse-gamma-distribution}{%
\section{Inverse gamma Distribution}\label{inverse-gamma-distribution}}

\hypertarget{probability-density-function-14}{%
\subsection{Probability density function}\label{probability-density-function-14}}

If \(\alpha \in \mathbb{R}^+\) and \(\beta \in \mathbb{R}^+\), then for \(y \in \mathbb{R}^+\), \[ \text{InvGamma}(y|\alpha,\beta) =
\frac{\beta^{\alpha}}      {\Gamma(\alpha)} \ y^{-(\alpha + 1)} \,
\exp \! \left( \! - \beta \, \frac{1}{y} \right) . \]

\hypertarget{sampling-statement-38}{%
\subsection{Sampling statement}\label{sampling-statement-38}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{inv\_gamma}}\texttt{(alpha,\ beta)}

Increment target log probability density with \texttt{inv\_gamma\_lupdf(y\ \textbar{}\ alpha,\ beta)}.
\index{{\tt \bfseries inv\_gamma }!sampling statement|hyperpage}

\hypertarget{stan-functions-37}{%
\subsection{Stan functions}\label{stan-functions-37}}

\index{{\tt \bfseries inv\_gamma\_lpdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_gamma\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the inverse gamma density of y given shape alpha and scale
beta

\index{{\tt \bfseries inv\_gamma\_lupdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_gamma\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the inverse gamma density of y given shape alpha and scale
beta dropping constant additive terms

\index{{\tt \bfseries inv\_gamma\_cdf }!{\tt (reals y, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_gamma\_cdf}}\texttt{(reals\ y,\ reals\ alpha,\ reals\ beta)}\newline
The inverse gamma cumulative distribution function of y given shape
alpha and scale beta

\index{{\tt \bfseries inv\_gamma\_lcdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_gamma\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the inverse gamma cumulative distribution function of y
given shape alpha and scale beta

\index{{\tt \bfseries inv\_gamma\_lccdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_gamma\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the inverse gamma complementary cumulative distribution
function of y given shape alpha and scale beta

\index{{\tt \bfseries inv\_gamma\_rng }!{\tt (reals alpha, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{inv\_gamma\_rng}}\texttt{(reals\ alpha,\ reals\ beta)}\newline
Generate an inverse gamma variate with shape alpha and scale beta; may
only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{weibull-distribution}{%
\section{Weibull distribution}\label{weibull-distribution}}

\hypertarget{probability-density-function-15}{%
\subsection{Probability density function}\label{probability-density-function-15}}

If \(\alpha \in \mathbb{R}^+\) and \(\sigma \in \mathbb{R}^+\), then for
\(y \in [0,\infty)\), \[ \text{Weibull}(y|\alpha,\sigma) =
\frac{\alpha}{\sigma} \, \left( \frac{y}{\sigma} \right)^{\alpha - 1}
\, \exp \! \left( \! - \left( \frac{y}{\sigma} \right)^{\alpha}
\right) . \]

Note that if \(Y \propto \text{Weibull}(\alpha,\sigma)\), then \(Y^{-1} \propto \text{Frechet}(\alpha,\sigma^{-1})\).

\hypertarget{sampling-statement-39}{%
\subsection{Sampling statement}\label{sampling-statement-39}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{weibull}}\texttt{(alpha,\ sigma)}

Increment target log probability density with \texttt{weibull\_lupdf(y\ \textbar{}\ alpha,\ sigma)}.
\index{{\tt \bfseries weibull }!sampling statement|hyperpage}

\hypertarget{stan-functions-38}{%
\subsection{Stan functions}\label{stan-functions-38}}

\index{{\tt \bfseries weibull\_lpdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{weibull\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Weibull density of y given shape alpha and scale sigma

\index{{\tt \bfseries weibull\_lupdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{weibull\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Weibull density of y given shape alpha and scale sigma
dropping constant additive terms

\index{{\tt \bfseries weibull\_cdf }!{\tt (reals y, reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{weibull\_cdf}}\texttt{(reals\ y,\ reals\ alpha,\ reals\ sigma)}\newline
The Weibull cumulative distribution function of y given shape alpha
and scale sigma

\index{{\tt \bfseries weibull\_lcdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{weibull\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Weibull cumulative distribution function of y given
shape alpha and scale sigma

\index{{\tt \bfseries weibull\_lccdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{weibull\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Weibull complementary cumulative distribution function
of y given shape alpha and scale sigma

\index{{\tt \bfseries weibull\_rng }!{\tt (reals alpha, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{weibull\_rng}}\texttt{(reals\ alpha,\ reals\ sigma)}\newline
Generate a weibull variate with shape alpha and scale sigma; may only
be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{frechet-distribution}{%
\section{Frechet distribution}\label{frechet-distribution}}

\hypertarget{probability-density-function-16}{%
\subsection{Probability density function}\label{probability-density-function-16}}

If \(\alpha \in \mathbb{R}^+\) and \(\sigma \in \mathbb{R}^+\), then for
\(y \in \mathbb{R}^+\), \[ \text{Frechet}(y|\alpha,\sigma) =
\frac{\alpha}{\sigma} \, \left( \frac{y}{\sigma} \right)^{-\alpha - 1}
\, \exp \! \left( \! - \left( \frac{y}{\sigma} \right)^{-\alpha}
\right) . \]

Note that if \(Y \propto \text{Frechet}(\alpha,\sigma)\), then \(Y^{-1} \propto \text{Weibull}(\alpha,\sigma^{-1})\).

\hypertarget{sampling-statement-40}{%
\subsection{Sampling statement}\label{sampling-statement-40}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{frechet}}\texttt{(alpha,\ sigma)}

Increment target log probability density with \texttt{frechet\_lupdf(y\ \textbar{}\ alpha,\ sigma)}.
\index{{\tt \bfseries frechet }!sampling statement|hyperpage}

\hypertarget{stan-functions-39}{%
\subsection{Stan functions}\label{stan-functions-39}}

\index{{\tt \bfseries frechet\_lpdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{frechet\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Frechet density of y given shape alpha and scale sigma

\index{{\tt \bfseries frechet\_lupdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{frechet\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Frechet density of y given shape alpha and scale sigma
dropping constant additive terms

\index{{\tt \bfseries frechet\_cdf }!{\tt (reals y, reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{frechet\_cdf}}\texttt{(reals\ y,\ reals\ alpha,\ reals\ sigma)}\newline
The Frechet cumulative distribution function of y given shape alpha
and scale sigma

\index{{\tt \bfseries frechet\_lcdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{frechet\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Frechet cumulative distribution function of y given
shape alpha and scale sigma

\index{{\tt \bfseries frechet\_lccdf }!{\tt (reals y \textbar\ reals alpha, reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{frechet\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ sigma)}\newline
The log of the Frechet complementary cumulative distribution function
of y given shape alpha and scale sigma

\index{{\tt \bfseries frechet\_rng }!{\tt (reals alpha, reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{frechet\_rng}}\texttt{(reals\ alpha,\ reals\ sigma)}\newline
Generate a Frechet variate with shape alpha and scale sigma; may only
be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{rayleigh-distribution}{%
\section{Rayleigh distribution}\label{rayleigh-distribution}}

\hypertarget{probability-density-function-17}{%
\subsection{Probability density function}\label{probability-density-function-17}}

If \(\sigma \in \mathbb{R}^+\), then for \(y \in [0,\infty)\), \[
\text{Rayleigh}(y|\sigma) = \frac{y}{\sigma^2} \exp(-y^2 / 2\sigma^2)
\!. \]

\hypertarget{sampling-statement-41}{%
\subsection{Sampling statement}\label{sampling-statement-41}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{rayleigh}}\texttt{(sigma)}

Increment target log probability density with \texttt{rayleigh\_lupdf(y\ \textbar{}\ sigma)}.
\index{{\tt \bfseries rayleigh }!sampling statement|hyperpage}

\hypertarget{stan-functions-40}{%
\subsection{Stan functions}\label{stan-functions-40}}

\index{{\tt \bfseries rayleigh\_lpdf }!{\tt (reals y \textbar\ reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{rayleigh\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ sigma)}\newline
The log of the Rayleigh density of y given scale sigma

\index{{\tt \bfseries rayleigh\_lupdf }!{\tt (reals y \textbar\ reals sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{rayleigh\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ sigma)}\newline
The log of the Rayleigh density of y given scale sigma
dropping constant additive terms

\index{{\tt \bfseries rayleigh\_cdf }!{\tt (real y, real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{rayleigh\_cdf}}\texttt{(real\ y,\ real\ sigma)}\newline
The Rayleigh cumulative distribution of y given scale sigma

\index{{\tt \bfseries rayleigh\_lcdf }!{\tt (real y \textbar\ real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{rayleigh\_lcdf}}\texttt{(real\ y\ \textbar{}\ real\ sigma)}\newline
The log of the Rayleigh cumulative distribution of y given scale sigma

\index{{\tt \bfseries rayleigh\_lccdf }!{\tt (real y \textbar\ real sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{rayleigh\_lccdf}}\texttt{(real\ y\ \textbar{}\ real\ sigma)}\newline
The log of the Rayleigh complementary cumulative distribution of y
given scale sigma

\index{{\tt \bfseries rayleigh\_rng }!{\tt (reals sigma): R}|hyperpage}

\texttt{R} \textbf{\texttt{rayleigh\_rng}}\texttt{(reals\ sigma)}\newline
Generate a Rayleigh variate with scale sigma; may only be used in
generated quantities block. For a description of argument and return
types, see section \protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{positive-lower-bounded-distributions}{%
\chapter{Positive Lower-Bounded Distributions}\label{positive-lower-bounded-distributions}}

The positive lower-bounded probabilities have support on real values
above some positive minimum value.

\hypertarget{pareto-distribution}{%
\section{Pareto distribution}\label{pareto-distribution}}

\hypertarget{probability-density-function-18}{%
\subsection{Probability density function}\label{probability-density-function-18}}

If \(y_{\text{min}} \in \mathbb{R}^+\) and \(\alpha \in \mathbb{R}^+\),
then for \(y \in \mathbb{R}^+\) with \(y \geq y_{\text{min}}\), \[
\text{Pareto}(y|y_{\text{min}},\alpha) = \frac{\displaystyle
\alpha\,y_{\text{min}}^\alpha}{\displaystyle y^{\alpha+1}}. \]

\hypertarget{sampling-statement-42}{%
\subsection{Sampling statement}\label{sampling-statement-42}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{pareto}}\texttt{(y\_min,\ alpha)}

Increment target log probability density with \texttt{pareto\_lupdf(y\ \textbar{}\ y\_min,\ alpha)}.
\index{{\tt \bfseries pareto }!sampling statement|hyperpage}

\hypertarget{stan-functions-41}{%
\subsection{Stan functions}\label{stan-functions-41}}

\index{{\tt \bfseries pareto\_lpdf }!{\tt (reals y \textbar\ reals y\_min, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ y\_min,\ reals\ alpha)}\newline
The log of the Pareto density of y given positive minimum value y\_min
and shape alpha

\index{{\tt \bfseries pareto\_lupdf }!{\tt (reals y \textbar\ reals y\_min, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ y\_min,\ reals\ alpha)}\newline
The log of the Pareto density of y given positive minimum value y\_min
and shape alpha dropping constant additive terms

\index{{\tt \bfseries pareto\_cdf }!{\tt (reals y, reals y\_min, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_cdf}}\texttt{(reals\ y,\ reals\ y\_min,\ reals\ alpha)}\newline
The Pareto cumulative distribution function of y given positive
minimum value y\_min and shape alpha

\index{{\tt \bfseries pareto\_lcdf }!{\tt (reals y \textbar\ reals y\_min, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ y\_min,\ reals\ alpha)}\newline
The log of the Pareto cumulative distribution function of y given
positive minimum value y\_min and shape alpha

\index{{\tt \bfseries pareto\_lccdf }!{\tt (reals y \textbar\ reals y\_min, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ y\_min,\ reals\ alpha)}\newline
The log of the Pareto complementary cumulative distribution function
of y given positive minimum value y\_min and shape alpha

\index{{\tt \bfseries pareto\_rng }!{\tt (reals y\_min, reals alpha): R}|hyperpage}

\texttt{R} \textbf{\texttt{pareto\_rng}}\texttt{(reals\ y\_min,\ reals\ alpha)}\newline
Generate a Pareto variate with positive minimum value y\_min and shape
alpha; may only be used in transformed data and generated quantities blocks. For a
description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{pareto-type-2-distribution}{%
\section{Pareto type 2 distribution}\label{pareto-type-2-distribution}}

\hypertarget{probability-density-function-19}{%
\subsection{Probability density function}\label{probability-density-function-19}}

If \(\mu \in \mathbb{R}\), \(\lambda \in \mathbb{R}^+\), and \(\alpha \in \mathbb{R}^+\), then for \(y \geq \mu\), \[
\mathrm{Pareto\_Type\_2}(y|\mu,\lambda,\alpha) = \
\frac{\alpha}{\lambda} \, \left( 1+\frac{y-\mu}{\lambda}
\right)^{-(\alpha+1)} \! . \]

Note that the Lomax distribution is a Pareto Type 2 distribution with
\(\mu=0\).

\hypertarget{sampling-statement-43}{%
\subsection{Sampling statement}\label{sampling-statement-43}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{pareto\_type\_2}}\texttt{(mu,\ lambda,\ alpha)}

Increment target log probability density with \texttt{pareto\_type\_2\_lupdf(y\ \textbar{}\ mu,\ lambda,\ alpha)}.
\index{{\tt \bfseries pareto\_type\_2 }!sampling statement|hyperpage}

\hypertarget{stan-functions-42}{%
\subsection{Stan functions}\label{stan-functions-42}}

\index{{\tt \bfseries pareto\_type\_2\_lpdf }!{\tt (reals y \textbar\ reals mu, reals lambda, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_type\_2\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ lambda,\ reals\ alpha)}\newline
The log of the Pareto Type 2 density of y given location mu, scale
lambda, and shape alpha

\index{{\tt \bfseries pareto\_type\_2\_lupdf }!{\tt (reals y \textbar\ reals mu, reals lambda, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_type\_2\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ lambda,\ reals\ alpha)}\newline
The log of the Pareto Type 2 density of y given location mu, scale
lambda, and shape alpha dropping constant additive terms

\index{{\tt \bfseries pareto\_type\_2\_cdf }!{\tt (reals y, reals mu, reals lambda, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_type\_2\_cdf}}\texttt{(reals\ y,\ reals\ mu,\ reals\ lambda,\ reals\ alpha)}\newline
The Pareto Type 2 cumulative distribution function of y given location
mu, scale lambda, and shape alpha

\index{{\tt \bfseries pareto\_type\_2\_lcdf }!{\tt (reals y \textbar\ reals mu, reals lambda, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_type\_2\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ lambda,\ reals\ alpha)}\newline
The log of the Pareto Type 2 cumulative distribution function of y
given location mu, scale lambda, and shape alpha

\index{{\tt \bfseries pareto\_type\_2\_lccdf }!{\tt (reals y \textbar\ reals mu, reals lambda, reals alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{pareto\_type\_2\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ lambda,\ reals\ alpha)}\newline
The log of the Pareto Type 2 complementary cumulative distribution
function of y given location mu, scale lambda, and shape alpha

\index{{\tt \bfseries pareto\_type\_2\_rng }!{\tt (reals mu, reals lambda, reals alpha): R}|hyperpage}

\texttt{R} \textbf{\texttt{pareto\_type\_2\_rng}}\texttt{(reals\ mu,\ reals\ lambda,\ reals\ alpha)}\newline
Generate a Pareto Type 2 variate with location mu, scale lambda, and
shape alpha; may only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{wiener-first-passage-time-distribution}{%
\section{Wiener First Passage Time Distribution}\label{wiener-first-passage-time-distribution}}

\hypertarget{probability-density-function-20}{%
\subsection{Probability density function}\label{probability-density-function-20}}

If \(\alpha \in \mathbb{R}^+\), \(\tau \in \mathbb{R}^+\), \(\beta \in [0, 1]\) and \(\delta \in \mathbb{R}\), then for \(y > \tau\), \[
\text{Wiener}(y|\alpha, \tau, \beta, \delta) =
\frac{\alpha^3}{(y-\tau)^{3/2}} \exp \! \left(- \delta \alpha \beta -
\frac{\delta^2(y-\tau)}{2}\right) \sum_{k = - \infty}^{\infty} (2k +
\beta) \phi \! \left(\frac{2k \alpha + \beta}{\sqrt{y - \tau}}\right)
\] where \(\phi(x)\) denotes the standard normal density function; see
(\protect\hyperlink{ref-Feller1968}{Feller 1968}), (\protect\hyperlink{ref-NavarroFuss2009}{Navarro and Fuss 2009}).

\hypertarget{sampling-statement-44}{%
\subsection{Sampling statement}\label{sampling-statement-44}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{wiener}}\texttt{(alpha,\ tau,\ beta,\ delta)}

Increment target log probability density with \texttt{wiener\_lupdf(y\ \textbar{}\ alpha,\ tau,\ beta,\ delta)}.
\index{{\tt \bfseries wiener }!sampling statement|hyperpage}

\hypertarget{stan-functions-43}{%
\subsection{Stan functions}\label{stan-functions-43}}

\index{{\tt \bfseries wiener\_lpdf }!{\tt (reals y \textbar\ reals alpha, reals tau, reals beta, reals delta): real}|hyperpage}

\texttt{real} \textbf{\texttt{wiener\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ tau,\ reals\ beta,\ reals\ delta)}\newline
The log of the Wiener first passage time density of y given boundary
separation alpha, non-decision time tau, a-priori bias beta and drift
rate delta

\index{{\tt \bfseries wiener\_lupdf }!{\tt (reals y \textbar\ reals alpha, reals tau, reals beta, reals delta): real}|hyperpage}

\texttt{real} \textbf{\texttt{wiener\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ tau,\ reals\ beta,\ reals\ delta)}\newline
The log of the Wiener first passage time density of y given boundary
separation alpha, non-decision time tau, a-priori bias beta and drift
rate delta dropping constant additive terms

\hypertarget{boundaries}{%
\subsection{boundaries}\label{boundaries}}

Stan returns the first passage time of the accumulation process over
the upper boundary only. To get the result for the lower boundary, use
\[ \text{wiener}(y | \alpha, \tau, 1 - \beta, - \delta) \] For more
details, see the appendix of \protect\hyperlink{ref-Vandekerckhove-Wabersich:2014}{Vandekerckhove and Wabersich} (\protect\hyperlink{ref-Vandekerckhove-Wabersich:2014}{2014}).

\hypertarget{continuous-distributions-on-0-1}{%
\chapter{Continuous Distributions on {[}0, 1{]}}\label{continuous-distributions-on-0-1}}

The continuous distributions with outcomes in the interval \([0,1]\) are
used to characterized bounded quantities, including probabilities.

\hypertarget{beta-distribution}{%
\section{Beta distribution}\label{beta-distribution}}

\hypertarget{probability-density-function-21}{%
\subsection{Probability density function}\label{probability-density-function-21}}

If \(\alpha \in \mathbb{R}^+\) and \(\beta \in \mathbb{R}^+\), then for
\(\theta \in (0,1)\), \[ \text{Beta}(\theta|\alpha,\beta) =
\frac{1}{\mathrm{B}(\alpha,\beta)} \, \theta^{\alpha - 1} \, (1 -
\theta)^{\beta - 1} , \] where the beta function \(\mathrm{B}()\) is as
defined in section \protect\hyperlink{betafun}{combinatorial functions}.

\emph{\textbf{Warning:}} If \(\theta = 0\) or \(\theta = 1\), then the probability
is 0 and the log probability is \(-\infty\). Similarly, the
distribution requires strictly positive parameters, \(\alpha, \beta > 0\).

\hypertarget{sampling-statement-45}{%
\subsection{Sampling statement}\label{sampling-statement-45}}

\texttt{theta\ \textasciitilde{}} \textbf{\texttt{beta}}\texttt{(alpha,\ beta)}

Increment target log probability density with \texttt{beta\_lupdf(theta\ \textbar{}\ alpha,\ beta)}.
\index{{\tt \bfseries beta }!sampling statement|hyperpage}

\hypertarget{stan-functions-44}{%
\subsection{Stan functions}\label{stan-functions-44}}

\index{{\tt \bfseries beta\_lpdf }!{\tt (reals theta \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_lpdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the beta density of \texttt{theta} in \([0,1]\) given positive prior
successes (plus one) alpha and prior failures (plus one) beta

\index{{\tt \bfseries beta\_lupdf }!{\tt (reals theta \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_lupdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the beta density of \texttt{theta} in \([0,1]\) given positive prior
successes (plus one) alpha and prior failures (plus one) beta
dropping constant additive terms

\index{{\tt \bfseries beta\_cdf }!{\tt (reals theta, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_cdf}}\texttt{(reals\ theta,\ reals\ alpha,\ reals\ beta)}\newline
The beta cumulative distribution function of \texttt{theta} in \([0,1]\) given
positive prior successes (plus one) alpha and prior failures (plus
one) beta

\index{{\tt \bfseries beta\_lcdf }!{\tt (reals theta \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_lcdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the beta cumulative distribution function of \texttt{theta} in
\([0,1]\) given positive prior successes (plus one) alpha and prior
failures (plus one) beta

\index{{\tt \bfseries beta\_lccdf }!{\tt (reals theta \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_lccdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the beta complementary cumulative distribution function of
\texttt{theta} in \([0,1]\) given positive prior successes (plus one) alpha and
prior failures (plus one) beta

\index{{\tt \bfseries beta\_rng }!{\tt (reals alpha, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{beta\_rng}}\texttt{(reals\ alpha,\ reals\ beta)}\newline
Generate a beta variate with positive prior successes (plus one) alpha
and prior failures (plus one) beta; may only be used in transformed data and
generated quantities blocks. For a description of argument and return types, see
section \protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{beta-proportion-distribution}{%
\section{Beta proportion distribution}\label{beta-proportion-distribution}}

\hypertarget{probability-density-function-22}{%
\subsection{Probability density function}\label{probability-density-function-22}}

If \(\mu \in (0, 1)\) and \(\kappa \in \mathbb{R}^+\), then for \(\theta \in (0,1)\), \[ \mathrm{Beta\_Proportion}(\theta|\mu,\kappa) =
\frac{1}{\mathrm{B}(\mu \kappa, (1 - \mu) \kappa)} \,
\theta^{\mu\kappa - 1} \, (1 - \theta)^{(1 - \mu)\kappa- 1} , \] where
the beta function \(\mathrm{B}()\) is as defined in section
\protect\hyperlink{betafun}{combinatorial functions}.

\emph{\textbf{Warning:}} If \(\theta = 0\) or \(\theta = 1\), then the probability
is 0 and the log probability is \(-\infty\). Similarly, the
distribution requires \(\mu \in (0, 1)\) and strictly positive
parameter, \(\kappa > 0\).

\hypertarget{sampling-statement-46}{%
\subsection{Sampling statement}\label{sampling-statement-46}}

\texttt{theta\ \textasciitilde{}} \textbf{\texttt{beta\_proportion}}\texttt{(mu,\ kappa)}

Increment target log probability density with \texttt{beta\_proportion\_lupdf(theta\ \textbar{}\ mu,\ kappa)}.
\index{{\tt \bfseries beta\_proportion }!sampling statement|hyperpage}

\hypertarget{stan-functions-45}{%
\subsection{Stan functions}\label{stan-functions-45}}

\index{{\tt \bfseries beta\_proportion\_lpdf }!{\tt (reals theta \textbar\ reals mu, reals kappa): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_proportion\_lpdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ mu,\ reals\ kappa)}\newline
The log of the beta\_proportion density of \texttt{theta} in \((0,1)\) given
mean mu and precision kappa

\index{{\tt \bfseries beta\_proportion\_lupdf }!{\tt (reals theta \textbar\ reals mu, reals kappa): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_proportion\_lupdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ mu,\ reals\ kappa)}\newline
The log of the beta\_proportion density of \texttt{theta} in \((0,1)\) given
mean mu and precision kappa dropping constant additive terms

\index{{\tt \bfseries beta\_proportion\_lcdf }!{\tt (reals theta \textbar\ reals mu, reals kappa): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_proportion\_lcdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ mu,\ reals\ kappa)}\newline
The log of the beta\_proportion cumulative distribution function of
\texttt{theta} in \((0,1)\) given mean mu and precision kappa

\index{{\tt \bfseries beta\_proportion\_lccdf }!{\tt (reals theta \textbar\ reals mu, reals kappa): real}|hyperpage}

\texttt{real} \textbf{\texttt{beta\_proportion\_lccdf}}\texttt{(reals\ theta\ \textbar{}\ reals\ mu,\ reals\ kappa)}\newline
The log of the beta\_proportion complementary cumulative distribution
function of \texttt{theta} in \((0,1)\) given mean mu and precision kappa

\index{{\tt \bfseries beta\_proportion\_rng }!{\tt (reals mu, reals kappa): R}|hyperpage}

\texttt{R} \textbf{\texttt{beta\_proportion\_rng}}\texttt{(reals\ mu,\ reals\ kappa)}\newline
Generate a beta\_proportion variate with mean mu and precision kappa;
may only be used in transformed data and generated quantities blocks.
For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{circular-distributions}{%
\chapter{Circular Distributions}\label{circular-distributions}}

Circular distributions are defined for finite values y in any interval
of length \(2\pi\).

\hypertarget{von-mises-distribution}{%
\section{Von Mises distribution}\label{von-mises-distribution}}

\hypertarget{probability-density-function-23}{%
\subsection{Probability density function}\label{probability-density-function-23}}

If \(\mu \in \mathbb{R}\) and \(\kappa \in \mathbb{R}^+\), then for \(y \in \mathbb{R}\), \[ \text{VonMises}(y|\mu,\kappa) =
\frac{\exp(\kappa\cos(y-\mu))}{2\pi I_0(\kappa)} \!. \] In order for
this density to properly normalize, \(y\) must be restricted to some
interval \((c, c + 2\pi)\) of length \(2 \pi\), because \[ \int_{c}^{c +
2\pi} \text{VonMises}(y|\mu,\kappa) dy = 1. \] Similarly, if \(\mu\) is
a parameter, it will typically be restricted to the same range as \(y\).

If \(\kappa > 0\), a von Mises distribution with its \(2 \pi\) interval of
support centered around its location \(\mu\) will have a single mode at \(\mu\);
for example, restricting \(y\) to \((-\pi,\pi)\) and taking \(\mu = 0\) leads to
a single local optimum at the mode \(\mu\). If the location \(\mu\) is
not in the center of the support, the density is circularly translated
and there will be a second local maximum at the boundary furthest from
the mode. Ideally, the parameterization and support will be set up so
that the bulk of the probability mass is in a continuous interval
around the mean \(\mu\).

For \(\kappa = 0\), the Von Mises distribution corresponds to the
circular uniform distribution with density \(1 / (2 \pi)\) (independently
of the values of \(y\) or \(\mu\)).

\hypertarget{sampling-statement-47}{%
\subsection{Sampling statement}\label{sampling-statement-47}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{von\_mises}}\texttt{(mu,\ kappa)}

Increment target log probability density with \texttt{von\_mises\_lupdf(y\ \textbar{}\ mu,\ kappa)}.
\index{{\tt \bfseries von\_mises }!sampling statement|hyperpage}

\hypertarget{stan-functions-46}{%
\subsection{Stan functions}\label{stan-functions-46}}

\index{{\tt \bfseries von\_mises\_lpdf }!{\tt (reals y \textbar\ reals mu, reals kappa): R}|hyperpage}

\texttt{R} \textbf{\texttt{von\_mises\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ kappa)}\newline
The log of the von mises density of y given location mu and scale
kappa.

\index{{\tt \bfseries von\_mises\_lupdf }!{\tt (reals y \textbar\ reals mu, reals kappa): R}|hyperpage}

\texttt{R} \textbf{\texttt{von\_mises\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ mu,\ reals\ kappa)}\newline
The log of the von mises density of y given location mu and scale
kappa dropping constant additive terms.

\index{{\tt \bfseries von\_mises\_rng }!{\tt (reals mu, reals kappa): R}|hyperpage}

\texttt{R} \textbf{\texttt{von\_mises\_rng}}\texttt{(reals\ mu,\ reals\ kappa)}\newline
Generate a Von Mises variate with location mu and scale kappa (i.e.
returns values in the interval \([(\mu \mod 2\pi)-\pi,(\mu \mod 2\pi)+\pi]\)); may only be used in transformed data and generated quantities
blocks. For a description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{numerical-stability}{%
\subsection{Numerical stability}\label{numerical-stability}}

Evaluating the Von Mises distribution for \(\kappa > 100\) is
numerically unstable in the current implementation. Nathanael I.
Lichti suggested the following workaround on the Stan users group,
based on the fact that as \(\kappa \rightarrow \infty\), \[
\text{VonMises}(y|\mu,\kappa) \rightarrow \text{Normal}(\mu, \sqrt{1 /
\kappa}). \] The workaround is to replace \texttt{y\ \textasciitilde{}\ von\_mises(mu,kappa)}
with

\begin{verbatim}
 if (kappa < 100)
   y ~ von_mises(mu, kappa);
 else
   y ~ normal(mu, sqrt(1 / kappa));
\end{verbatim}

\hypertarget{bounded-continuous-distributions}{%
\chapter{Bounded Continuous Distributions}\label{bounded-continuous-distributions}}

The bounded continuous probabilities have support on a finite interval
of real numbers.

\hypertarget{uniform-distribution}{%
\section{Uniform distribution}\label{uniform-distribution}}

\hypertarget{probability-density-function-24}{%
\subsection{Probability density function}\label{probability-density-function-24}}

If \(\alpha \in \mathbb{R}\) and \(\beta \in (\alpha,\infty)\), then for
\(y \in [\alpha,\beta]\), \[ \text{Uniform}(y|\alpha,\beta) =
\frac{1}{\beta - \alpha} . \]

\hypertarget{sampling-statement-48}{%
\subsection{Sampling statement}\label{sampling-statement-48}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{uniform}}\texttt{(alpha,\ beta)}

Increment target log probability density with \texttt{uniform\_lupdf(y\ \textbar{}\ alpha,\ beta)}.
\index{{\tt \bfseries uniform }!sampling statement|hyperpage}

\hypertarget{stan-functions-47}{%
\subsection{Stan functions}\label{stan-functions-47}}

\index{{\tt \bfseries uniform\_lpdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{uniform\_lpdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the uniform density of y given lower bound alpha and upper
bound beta

\index{{\tt \bfseries uniform\_lupdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{uniform\_lupdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the uniform density of y given lower bound alpha and upper
bound beta dropping constant additive terms

\index{{\tt \bfseries uniform\_cdf }!{\tt (reals y, reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{uniform\_cdf}}\texttt{(reals\ y,\ reals\ alpha,\ reals\ beta)}\newline
The uniform cumulative distribution function of y given lower bound
alpha and upper bound beta

\index{{\tt \bfseries uniform\_lcdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{uniform\_lcdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the uniform cumulative distribution function of y given
lower bound alpha and upper bound beta

\index{{\tt \bfseries uniform\_lccdf }!{\tt (reals y \textbar\ reals alpha, reals beta): real}|hyperpage}

\texttt{real} \textbf{\texttt{uniform\_lccdf}}\texttt{(reals\ y\ \textbar{}\ reals\ alpha,\ reals\ beta)}\newline
The log of the uniform complementary cumulative distribution function
of y given lower bound alpha and upper bound beta

\index{{\tt \bfseries uniform\_rng  }!{\tt (reals alpha, reals beta): R}|hyperpage}

\texttt{R} \textbf{\texttt{uniform\_rng}}\texttt{(reals\ alpha,\ reals\ beta)}\newline
Generate a uniform variate with lower bound alpha and upper bound
beta; may only be used in transformed data and generated quantities blocks. For a
description of argument and return types, see section
\protect\hyperlink{prng-vectorization}{vectorized PRNG functions}.

\hypertarget{distributions-over-unbounded-vectors}{%
\chapter{Distributions over Unbounded Vectors}\label{distributions-over-unbounded-vectors}}

The unbounded vector probability distributions have support on all of
\(\mathbb{R}^K\) for some fixed \(K\).

\hypertarget{multivariate-normal-distribution}{%
\section{Multivariate normal distribution}\label{multivariate-normal-distribution}}

\hypertarget{probability-density-function-25}{%
\subsection{Probability density function}\label{probability-density-function-25}}

If \(K \in \mathbb{N}\), \(\mu \in \mathbb{R}^K\), and \(\Sigma \in \mathbb{R}^{K \times K}\) is symmetric and positive definite, then
for \(y \in \mathbb{R}^K\), \[ \text{MultiNormal}(y|\mu,\Sigma) =
\frac{1}{\left( 2 \pi \right)^{K/2}} \ \frac{1}{\sqrt{|\Sigma|}} \
\exp \! \left( \! - \frac{1}{2} (y - \mu)^{\top} \, \Sigma^{-1} \, (y
- \mu) \right) \! , \] where \(|\Sigma|\) is the absolute determinant of
\(\Sigma\).

\hypertarget{sampling-statement-49}{%
\subsection{Sampling statement}\label{sampling-statement-49}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multi\_normal}}\texttt{(mu,\ Sigma)}

Increment target log probability density with \texttt{multi\_normal\_lupdf(y\ \textbar{}\ mu,\ Sigma)}.
\index{{\tt \bfseries multi\_normal }!sampling statement|hyperpage}

\hypertarget{stan-functions-48}{%
\subsection{Stan functions}\label{stan-functions-48}}

The multivariate normal probability function is overloaded to allow
the variate vector \(y\) and location vector \(\mu\) to be vectors or row
vectors (or to mix the two types). The density function is also
vectorized, so it allows arrays of row vectors or vectors as
arguments; see section \protect\hyperlink{prob-vectorization}{vectorized function signatures} for a description of
vectorization.

\index{{\tt \bfseries multi\_normal\_lpdf }!{\tt (vectors y \textbar\ vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of vector(s) y given
location vector(s) mu and covariance matrix Sigma

\index{{\tt \bfseries multi\_normal\_lupdf }!{\tt (vectors y \textbar\ vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of vector(s) y given
location vector(s) mu and covariance matrix Sigma dropping constant additive
terms

\index{{\tt \bfseries multi\_normal\_lpdf }!{\tt (vectors y \textbar\ row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of vector(s) y given
location row vector(s) mu and covariance matrix Sigma

\index{{\tt \bfseries multi\_normal\_lupdf }!{\tt (vectors y \textbar\ row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of vector(s) y given
location row vector(s) mu and covariance matrix Sigma dropping constant additive
terms

\index{{\tt \bfseries multi\_normal\_lpdf }!{\tt (row\_vectors y \textbar\ vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of row vector(s) y given
location vector(s) mu and covariance matrix Sigma

\index{{\tt \bfseries multi\_normal\_lupdf }!{\tt (row\_vectors y \textbar\ vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of row vector(s) y given
location vector(s) mu and covariance matrix Sigma dropping constant additive
terms

\index{{\tt \bfseries multi\_normal\_lpdf }!{\tt (row\_vectors y \textbar\ row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of row vector(s) y given
location row vector(s) mu and covariance matrix Sigma

\index{{\tt \bfseries multi\_normal\_lupdf }!{\tt (row\_vectors y \textbar\ row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate normal density of row vector(s) y given
location row vector(s) mu and covariance matrix Sigma dropping constant additive
terms

Although there is a direct multi-normal RNG function, if more than one
result is required, it's much more efficient to Cholesky factor the
covariance matrix and call \texttt{multi\_normal\_cholesky\_rng}; see section
\protect\hyperlink{multi-normal-cholesky-fun}{multi-variate normal, cholesky parameterization}.

\index{{\tt \bfseries multi\_normal\_rng }!{\tt (vector mu, matrix Sigma): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{multi\_normal\_rng}}\texttt{(vector\ mu,\ matrix\ Sigma)}\newline
Generate a multivariate normal variate with location mu and covariance
matrix Sigma; may only be used in transformed data and generated quantities blocks

\index{{\tt \bfseries multi\_normal\_rng }!{\tt (row\_vector mu, matrix Sigma): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{multi\_normal\_rng}}\texttt{(row\_vector\ mu,\ matrix\ Sigma)}\newline
Generate a multivariate normal variate with location mu and covariance
matrix Sigma; may only be used in transformed data and generated quantities blocks

\index{{\tt \bfseries multi\_normal\_rng }!{\tt (vectors mu, matrix Sigma): vectors}|hyperpage}

\texttt{vectors} \textbf{\texttt{multi\_normal\_rng}}\texttt{(vectors\ mu,\ matrix\ Sigma)}\newline
Generate an array of multivariate normal variates with locations mu
and covariance matrix Sigma; may only be used in transformed data and generated
quantities blocks

\index{{\tt \bfseries multi\_normal\_rng }!{\tt (row\_vectors mu, matrix Sigma): vectors}|hyperpage}

\texttt{vectors} \textbf{\texttt{multi\_normal\_rng}}\texttt{(row\_vectors\ mu,\ matrix\ Sigma)}\newline
Generate an array of multivariate normal variates with locations mu
and covariance matrix Sigma; may only be used in transformed data and generated
quantities blocks

\hypertarget{multivariate-normal-distribution-precision-parameterization}{%
\section{Multivariate normal distribution, precision parameterization}\label{multivariate-normal-distribution-precision-parameterization}}

\hypertarget{probability-density-function-26}{%
\subsection{Probability density function}\label{probability-density-function-26}}

If \(K \in \mathbb{N}\), \(\mu \in \mathbb{R}^K\), and \(\Omega \in \mathbb{R}^{K \times K}\) is symmetric and positive definite, then
for \(y \in \mathbb{R}^K\), \[ \text{MultiNormalPrecision}(y|\mu,\Omega)
= \text{MultiNormal}(y|\mu,\Omega^{-1}) \]

\hypertarget{sampling-statement-50}{%
\subsection{Sampling statement}\label{sampling-statement-50}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multi\_normal\_prec}}\texttt{(mu,\ Omega)}

Increment target log probability density with \texttt{multi\_normal\_prec\_lupdf(y\ \textbar{}\ mu,\ Omega)}.
\index{{\tt \bfseries multi\_normal\_prec }!sampling statement|hyperpage}

\hypertarget{stan-functions-49}{%
\subsection{Stan functions}\label{stan-functions-49}}

\index{{\tt \bfseries multi\_normal\_prec\_lpdf }!{\tt (vectors y \textbar\ vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of vector(s) y given
location vector(s) mu and positive definite precision matrix Omega

\index{{\tt \bfseries multi\_normal\_prec\_lupdf }!{\tt (vectors y \textbar\ vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of vector(s) y given
location vector(s) mu and positive definite precision matrix Omega
dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_prec\_lpdf }!{\tt (vectors y \textbar\ row\_vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of vector(s) y given
location row vector(s) mu and positive definite precision matrix Omega

\index{{\tt \bfseries multi\_normal\_prec\_lupdf }!{\tt (vectors y \textbar\ row\_vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of vector(s) y given
location row vector(s) mu and positive definite precision matrix Omega
dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_prec\_lpdf }!{\tt (row\_vectors y \textbar\ vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of row vector(s) y given
location vector(s) mu and positive definite precision matrix Omega

\index{{\tt \bfseries multi\_normal\_prec\_lupdf }!{\tt (row\_vectors y \textbar\ vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of row vector(s) y given
location vector(s) mu and positive definite precision matrix Omega
dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_prec\_lpdf }!{\tt (row\_vectors y \textbar\ row\_vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of row vector(s) y given
location row vector(s) mu and positive definite precision matrix Omega

\index{{\tt \bfseries multi\_normal\_prec\_lupdf }!{\tt (row\_vectors y \textbar\ row\_vectors mu, matrix Omega): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_prec\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ Omega)}\newline
The log of the multivariate normal density of row vector(s) y given
location row vector(s) mu and positive definite precision matrix Omega
dropping constant additive terms

\hypertarget{multi-normal-cholesky-fun}{%
\section{Multivariate normal distribution, Cholesky parameterization}\label{multi-normal-cholesky-fun}}

\hypertarget{probability-density-function-27}{%
\subsection{Probability density function}\label{probability-density-function-27}}

If \(K \in \mathbb{N}\), \(\mu \in \mathbb{R}^K\), and \(L \in \mathbb{R}^{K \times K}\) is lower triangular and such that \(LL^{\top}\)
is positive definite, then for \(y \in \mathbb{R}^K\), \[
\text{MultiNormalCholesky}(y|\mu,L) =
\text{MultiNormal}(y|\mu,LL^{\top}). \] If \(L\) is lower triangular and
\(LL^{top}\) is a \(K \times K\) positive definite matrix, then \(L_{k,k}\)
must be strictly positive for \(k \in 1{:}K\). If an \(L\) is provided
that is not the Cholesky factor of a positive-definite matrix, the
probability functions will raise errors.

\hypertarget{sampling-statement-51}{%
\subsection{Sampling statement}\label{sampling-statement-51}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multi\_normal\_cholesky}}\texttt{(mu,\ L)}

Increment target log probability density with \texttt{multi\_normal\_cholesky\_lupdf(y\ \textbar{}\ mu,\ L)}.
\index{{\tt \bfseries multi\_normal\_cholesky }!sampling statement|hyperpage}

\hypertarget{stan-functions-50}{%
\subsection{Stan functions}\label{stan-functions-50}}

\index{{\tt \bfseries multi\_normal\_cholesky\_lpdf }!{\tt (vectors y \textbar\ vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of vector(s) y given
location vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L

\index{{\tt \bfseries multi\_normal\_cholesky\_lupdf }!{\tt (vectors y \textbar\ vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of vector(s) y given
location vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_cholesky\_lpdf }!{\tt (vectors y \textbar\ row\_vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of vector(s) y given
location row vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L

\index{{\tt \bfseries multi\_normal\_cholesky\_lupdf }!{\tt (vectors y \textbar\ row\_vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of vector(s) y given
location row vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_cholesky\_lpdf }!{\tt (row\_vectors y \textbar\ vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of row vector(s) y given
location vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L

\index{{\tt \bfseries multi\_normal\_cholesky\_lupdf }!{\tt (row\_vectors y \textbar\ vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of row vector(s) y given
location vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_cholesky\_lpdf }!{\tt (row\_vectors y \textbar\ row\_vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of row vector(s) y given
location row vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L

\index{{\tt \bfseries multi\_normal\_cholesky\_lupdf }!{\tt (row\_vectors y \textbar\ row\_vectors mu, matrix L): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_normal\_cholesky\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ row\_vectors\ mu,\ matrix\ L)}\newline
The log of the multivariate normal density of row vector(s) y given
location row vector(s) mu and lower-triangular Cholesky factor of the
covariance matrix L dropping constant additive terms

\index{{\tt \bfseries multi\_normal\_cholesky\_rng }!{\tt (vector mu, matrix L): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{multi\_normal\_cholesky\_rng}}\texttt{(vector\ mu,\ matrix\ L)}\newline
Generate a multivariate normal variate with location mu and
lower-triangular Cholesky factor of the covariance matrix L; may only
be used in transformed data and generated quantities blocks

\index{{\tt \bfseries multi\_normal\_cholesky\_rng }!{\tt (row\_vector mu, matrix L): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{multi\_normal\_cholesky\_rng}}\texttt{(row\_vector\ mu,\ matrix\ L)}\newline
Generate a multivariate normal variate with location mu and
lower-triangular Cholesky factor of the covariance matrix L; may only
be used in transformed data and generated quantities blocks

\index{{\tt \bfseries multi\_normal\_cholesky\_rng }!{\tt (vectors mu, matrix L): vectors}|hyperpage}

\texttt{vectors} \textbf{\texttt{multi\_normal\_cholesky\_rng}}\texttt{(vectors\ mu,\ matrix\ L)}\newline
Generate an array of multivariate normal variates with locations mu
and lower-triangular Cholesky factor of the covariance matrix L; may
only be used in transformed data and generated quantities blocks

\index{{\tt \bfseries multi\_normal\_cholesky\_rng }!{\tt (row\_vectors mu, matrix L): vectors}|hyperpage}

\texttt{vectors} \textbf{\texttt{multi\_normal\_cholesky\_rng}}\texttt{(row\_vectors\ mu,\ matrix\ L)}\newline
Generate an array of multivariate normal variates with locations mu
and lower-triangular Cholesky factor of the covariance matrix L; may
only be used in transformed data and generated quantities blocks

\hypertarget{multivariate-gaussian-process-distribution}{%
\section{Multivariate Gaussian process distribution}\label{multivariate-gaussian-process-distribution}}

\hypertarget{probability-density-function-28}{%
\subsection{Probability density function}\label{probability-density-function-28}}

If \(K,N \in \mathbb{N}\), \(\Sigma \in \mathbb{R}^{N \times N}\) is
symmetric, positive definite kernel matrix and \(w \in \mathbb{R}^{K}\)
is a vector of positive inverse scales, then for \(y \in \mathbb{R}^{K \times N}\), \[ \text{MultiGP}(y|\Sigma,w) = \prod_{i=1}^{K}
\text{MultiNormal}(y_i|0,w_i^{-1} \Sigma), \] where \(y_i\) is the \(i\)th
row of \(y\). This is used to efficiently handle Gaussian Processes
with multi-variate outputs where only the output dimensions share a
kernel function but vary based on their scale. Note that this
function does not take into account the mean prediction.

\hypertarget{sampling-statement-52}{%
\subsection{Sampling statement}\label{sampling-statement-52}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multi\_gp}}\texttt{(Sigma,\ w)}

Increment target log probability density with \texttt{multi\_gp\_lupdf(y\ \textbar{}\ Sigma,\ w)}.
\index{{\tt \bfseries multi\_gp }!sampling statement|hyperpage}

\hypertarget{stan-functions-51}{%
\subsection{Stan functions}\label{stan-functions-51}}

\index{{\tt \bfseries multi\_gp\_lpdf }!{\tt (matrix y \textbar\ matrix Sigma, vector w): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_gp\_lpdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ Sigma,\ vector\ w)}\newline
The log of the multivariate GP density of matrix y given kernel matrix
Sigma and inverses scales w

\index{{\tt \bfseries multi\_gp\_lupdf }!{\tt (matrix y \textbar\ matrix Sigma, vector w): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_gp\_lupdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ Sigma,\ vector\ w)}\newline
The log of the multivariate GP density of matrix y given kernel matrix
Sigma and inverses scales w dropping constant additive terms

\hypertarget{multivariate-gaussian-process-distribution-cholesky-parameterization}{%
\section{Multivariate Gaussian process distribution, Cholesky parameterization}\label{multivariate-gaussian-process-distribution-cholesky-parameterization}}

\hypertarget{probability-density-function-29}{%
\subsection{Probability density function}\label{probability-density-function-29}}

If \(K,N \in \mathbb{N}\), \(L \in \mathbb{R}^{N \times N}\) is lower
triangular and such that \(LL^{\top}\) is positive definite kernel
matrix (implying \(L_{n,n} > 0\) for \(n \in 1{:}N\)), and \(w \in \mathbb{R}^{K}\) is a vector of positive inverse scales, then for \(y \in \mathbb{R}^{K \times N}\), \[ \text{MultiGPCholesky}(y \, | \ L,w)
= \prod_{i=1}^{K} \text{MultiNormal}(y_i|0,w_i^{-1} LL^{\top}), \]
where \(y_i\) is the \(i\)th row of \(y\). This is used to efficiently
handle Gaussian Processes with multi-variate outputs where only the
output dimensions share a kernel function but vary based on their
scale. If the model allows parameterization in terms of Cholesky
factor of the kernel matrix, this distribution is also more efficient
than \(\text{MultiGP}()\). Note that this function does not take into
account the mean prediction.

\hypertarget{sampling-statement-53}{%
\subsection{Sampling statement}\label{sampling-statement-53}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multi\_gp\_cholesky}}\texttt{(L,\ w)}

Increment target log probability density with \texttt{multi\_gp\_cholesky\_lupdf(y\ \textbar{}\ L,\ w)}.
\index{{\tt \bfseries multi\_gp\_cholesky }!sampling statement|hyperpage}

\hypertarget{stan-functions-52}{%
\subsection{Stan functions}\label{stan-functions-52}}

\index{{\tt \bfseries multi\_gp\_cholesky\_lpdf }!{\tt (matrix y \textbar\ matrix L, vector w): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_gp\_cholesky\_lpdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ L,\ vector\ w)}\newline
The log of the multivariate GP density of matrix y given
lower-triangular Cholesky factor of the kernel matrix L and inverses
scales w

\index{{\tt \bfseries multi\_gp\_cholesky\_lupdf }!{\tt (matrix y \textbar\ matrix L, vector w): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_gp\_cholesky\_lupdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ L,\ vector\ w)}\newline
The log of the multivariate GP density of matrix y given
lower-triangular Cholesky factor of the kernel matrix L and inverses
scales w dropping constant additive terms

\hypertarget{multivariate-student-t-distribution}{%
\section{Multivariate Student-t distribution}\label{multivariate-student-t-distribution}}

\hypertarget{probability-density-function-30}{%
\subsection{Probability density function}\label{probability-density-function-30}}

If \(K \in \mathbb{N}\), \(\nu \in \mathbb{R}^+\), \(\mu \in \mathbb{R}^K\),
and \(\Sigma \in \mathbb{R}^{K \times K}\) is symmetric and positive
definite, then for \(y \in \mathbb{R}^K\), \[ \begin{array}{l}
\text{MultiStudentT}(y\,|\,\nu,\,\mu,\,\Sigma) \\  =
\frac{1}{\pi^{K/2}} \ \frac{1}{\nu^{K/2}} \ \frac{\Gamma\!\left((\nu +
K)/2\right)}      {\Gamma(\nu/2)} \ \frac{1}{\sqrt{\left| \Sigma
\right|}} \ \left( 1 + \frac{1}{\nu} \, \left(y - \mu\right)^{\top} \,
\Sigma^{-1} \, \left(y - \mu\right) \right)^{-(\nu + K)/2} \! .
\end{array} \]

\hypertarget{sampling-statement-54}{%
\subsection{Sampling statement}\label{sampling-statement-54}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{multi\_student\_t}}\texttt{(nu,\ mu,\ Sigma)}

Increment target log probability density with \texttt{multi\_student\_t\_lupdf(y\ \textbar{}\ nu,\ mu,\ Sigma)}.
\index{{\tt \bfseries multi\_student\_t }!sampling statement|hyperpage}

\hypertarget{stan-functions-53}{%
\subsection{Stan functions}\label{stan-functions-53}}

\index{{\tt \bfseries multi\_student\_t\_lpdf }!{\tt (vectors y \textbar\ real nu, vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ real\ nu,\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of vector(s) y given
degrees of freedom nu, location vector(s) mu, and scale matrix Sigma

\index{{\tt \bfseries multi\_student\_t\_lupdf }!{\tt (vectors y \textbar\ real nu, vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ real\ nu,\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of vector(s) y given
degrees of freedom nu, location vector(s) mu, and scale matrix Sigma
dropping constant additive terms

\index{{\tt \bfseries multi\_student\_t\_lpdf }!{\tt (vectors y \textbar\ real nu, row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lpdf}}\texttt{(vectors\ y\ \textbar{}\ real\ nu,\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of vector(s) y given
degrees of freedom nu, location row vector(s) mu, and scale matrix
Sigma

\index{{\tt \bfseries multi\_student\_t\_lupdf }!{\tt (vectors y \textbar\ real nu, row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lupdf}}\texttt{(vectors\ y\ \textbar{}\ real\ nu,\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of vector(s) y given
degrees of freedom nu, location row vector(s) mu, and scale matrix
Sigma dropping constant additive terms

\index{{\tt \bfseries multi\_student\_t\_lpdf }!{\tt (row\_vectors y \textbar\ real nu, vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ real\ nu,\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of row vector(s) y
given degrees of freedom nu, location vector(s) mu, and scale matrix
Sigma

\index{{\tt \bfseries multi\_student\_t\_lupdf }!{\tt (row\_vectors y \textbar\ real nu, vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ real\ nu,\ vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of row vector(s) y
given degrees of freedom nu, location vector(s) mu, and scale matrix
Sigma dropping constant additive terms

\index{{\tt \bfseries multi\_student\_t\_lpdf }!{\tt (row\_vectors y \textbar\ real nu, row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lpdf}}\texttt{(row\_vectors\ y\ \textbar{}\ real\ nu,\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of row vector(s) y
given degrees of freedom nu, location row vector(s) mu, and scale
matrix Sigma

\index{{\tt \bfseries multi\_student\_t\_lupdf }!{\tt (row\_vectors y \textbar\ real nu, row\_vectors mu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{multi\_student\_t\_lupdf}}\texttt{(row\_vectors\ y\ \textbar{}\ real\ nu,\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
The log of the multivariate Student-\(t\) density of row vector(s) y
given degrees of freedom nu, location row vector(s) mu, and scale
matrix Sigma dropping constant additive terms

\index{{\tt \bfseries multi\_student\_t\_rng }!{\tt (real nu, vector mu, matrix Sigma): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{multi\_student\_t\_rng}}\texttt{(real\ nu,\ vector\ mu,\ matrix\ Sigma)}\newline
Generate a multivariate Student-\(t\) variate with degrees of freedom
nu, location mu, and scale matrix Sigma; may only be used in transformed data
and generated quantities blocks

\index{{\tt \bfseries multi\_student\_t\_rng }!{\tt (real nu, row\_vector mu, matrix Sigma): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{multi\_student\_t\_rng}}\texttt{(real\ nu,\ row\_vector\ mu,\ matrix\ Sigma)}\newline
Generate a multivariate Student-\(t\) variate with degrees of freedom
nu, location mu, and scale matrix Sigma; may only be used in transfomed data
and generated quantities blocks

\index{{\tt \bfseries multi\_student\_t\_rng }!{\tt (real nu, vectors mu, matrix Sigma): vectors}|hyperpage}

\texttt{vectors} \textbf{\texttt{multi\_student\_t\_rng}}\texttt{(real\ nu,\ vectors\ mu,\ matrix\ Sigma)}\newline
Generate an array of multivariate Student-\(t\) variates with degrees of
freedom nu, locations mu, and scale matrix Sigma; may only be used in
transformed data and generated quantities blocks

\index{{\tt \bfseries multi\_student\_t\_rng }!{\tt (real nu, row\_vectors mu, matrix Sigma): vectors}|hyperpage}

\texttt{vectors} \textbf{\texttt{multi\_student\_t\_rng}}\texttt{(real\ nu,\ row\_vectors\ mu,\ matrix\ Sigma)}\newline
Generate an array of multivariate Student-\(t\) variates with degrees of
freedom nu, locations mu, and scale matrix Sigma; may only be used in
transformed data andgenerated quantities blocks

\hypertarget{gaussian-dynamic-linear-models}{%
\section{Gaussian dynamic linear models}\label{gaussian-dynamic-linear-models}}

A Gaussian Dynamic Linear model is defined as follows, For \(t \in 1, \dots, T\), \[   \begin{aligned}[t]     y_{t} &\sim N(F' \theta_{t}, V)
\\     \theta_{t} &\sim N(G \theta_{t - 1}, W) \\     \theta_{0} &\sim
N(m_{0}, C_{0})   \end{aligned} \] where \(y\) is \(n \times T\) matrix
where rows are variables and columns are observations. These functions
calculate the log-likelihood of the observations marginalizing over
the latent states (\(p(y | F, G, V, W, m_{0}, C_{0})\)). This
log-likelihood is a system that is calculated using the Kalman Filter.
If \(V\) is diagonal, then a more efficient algorithm which sequentially
processes observations and avoids a matrix inversions can be used
(\protect\hyperlink{ref-DurbinKoopman:2001}{Durbin and Koopman 2001, sec. 6.4}).

\hypertarget{sampling-statement-55}{%
\subsection{Sampling statement}\label{sampling-statement-55}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{gaussian\_dlm\_obs}}\texttt{(F,\ G,\ V,\ W,\ m0,\ C0)}

Increment target log probability density with \texttt{gaussian\_dlm\_obs\_lupdf(y\ \textbar{}\ F,\ G,\ V,\ W,\ m0,\ C0)}.
\index{{\tt \bfseries gaussian\_dlm\_obs }!sampling statement|hyperpage}

\hypertarget{stan-functions-54}{%
\subsection{Stan functions}\label{stan-functions-54}}

The following two functions differ in the type of their V, the first
taking a full observation covariance matrix V~and the second a vector
V~representing the diagonal of the observation covariance matrix.
The sampling statement defined in the previous section works with
either type of observation V.

\index{{\tt \bfseries gaussian\_dlm\_obs\_lpdf }!{\tt (matrix y \textbar\ matrix F, matrix G, matrix V, matrix W, vector m0, matrix C0): real}|hyperpage}

\texttt{real} \textbf{\texttt{gaussian\_dlm\_obs\_lpdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ F,\ matrix\ G,\ matrix\ V,\ matrix\ W,\ vector\ m0,\ matrix\ C0)}\newline
The log of the density of the Gaussian Dynamic Linear model with
observation matrix y in which rows are variables and columns are
observations, design matrix F, transition matrix G, observation
covariance matrix V, system covariance matrix W, and the initial state
is distributed normal with mean m0 and covariance C0.

\index{{\tt \bfseries gaussian\_dlm\_obs\_lupdf }!{\tt (matrix y \textbar\ matrix F, matrix G, matrix V, matrix W, vector m0, matrix C0): real}|hyperpage}

\texttt{real} \textbf{\texttt{gaussian\_dlm\_obs\_lupdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ F,\ matrix\ G,\ matrix\ V,\ matrix\ W,\ vector\ m0,\ matrix\ C0)}\newline
The log of the density of the Gaussian Dynamic Linear model with
observation matrix y in which rows are variables and columns are
observations, design matrix F, transition matrix G, observation
covariance matrix V, system covariance matrix W, and the initial state
is distributed normal with mean m0 and covariance C0. This function drops
constant additive terms.

\index{{\tt \bfseries gaussian\_dlm\_obs\_lpdf }!{\tt (matrix y \textbar\ matrix F, matrix G, vector V, matrix W, vector m0, matrix C0): real}|hyperpage}

\texttt{real} \textbf{\texttt{gaussian\_dlm\_obs\_lpdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ F,\ matrix\ G,\ vector\ V,\ matrix\ W,\ vector\ m0,\ matrix\ C0)}\newline
The log of the density of the Gaussian Dynamic Linear model with
observation matrix y in which rows are variables and columns are
observations, design matrix F, transition matrix G, observation
covariance matrix with diagonal V, system covariance matrix W, and the
initial state is distributed normal with mean m0 and covariance C0.

\index{{\tt \bfseries gaussian\_dlm\_obs\_lupdf }!{\tt (matrix y \textbar\ matrix F, matrix G, vector V, matrix W, vector m0, matrix C0): real}|hyperpage}

\texttt{real} \textbf{\texttt{gaussian\_dlm\_obs\_lupdf}}\texttt{(matrix\ y\ \textbar{}\ matrix\ F,\ matrix\ G,\ vector\ V,\ matrix\ W,\ vector\ m0,\ matrix\ C0)}\newline
The log of the density of the Gaussian Dynamic Linear model with
observation matrix y in which rows are variables and columns are
observations, design matrix F, transition matrix G, observation
covariance matrix with diagonal V, system covariance matrix W, and the
initial state is distributed normal with mean m0 and covariance C0.
This function drops constant additive terms.

\hypertarget{simplex-distributions}{%
\chapter{Simplex Distributions}\label{simplex-distributions}}

The simplex probabilities have support on the unit \(K\)-simplex for a
specified \(K\). A \(K\)-dimensional vector \(\theta\) is a unit
\(K\)-simplex if \(\theta_k \geq 0\) for \(k \in \{1,\ldots,K\}\) and
\(\sum_{k = 1}^K \theta_k = 1\).

\hypertarget{dirichlet-distribution}{%
\section{Dirichlet distribution}\label{dirichlet-distribution}}

\hypertarget{probability-density-function-31}{%
\subsection{Probability density function}\label{probability-density-function-31}}

If \(K \in \mathbb{N}\) and \(\alpha \in (\mathbb{R}^+)^{K}\), then for
\(\theta \in \text{$K$-simplex}\), \[ \text{Dirichlet}(\theta|\alpha) =
\frac{\Gamma \! \left( \sum_{k=1}^K \alpha_k \right)}
{\prod_{k=1}^K \Gamma(\alpha_k)} \ \prod_{k=1}^K \theta_k^{\alpha_k -
1} . \]

\emph{\textbf{Warning:}} If any of the components of \(\theta\) satisfies
\(\theta_i = 0\) or \(\theta_i = 1\), then the probability is 0 and the
log probability is \(-\infty\). Similarly, the distribution requires
strictly positive parameters, with \(\alpha_i > 0\) for each \(i\).

\hypertarget{meaning-of-dirichlet-parameters}{%
\subsection{Meaning of Dirichlet parameters}\label{meaning-of-dirichlet-parameters}}

A symmetric Dirichlet prior is \([\alpha, \ldots, \alpha]^{\top}\). To
code this in Stan,

\begin{verbatim}
 data {
   int<lower = 1> K;
   real<lower = 0> alpha;
 }
 generated quantities {
   vector[K] theta = dirichlet_rng(rep_vector(alpha, K));
 }
\end{verbatim}

Taking \(K = 10\), here are the first five draws for \(\alpha = 1\).
For \(\alpha = 1\), the distribution is uniform over simplexes.

\begin{verbatim}
 1) 0.17 0.05 0.07 0.17 0.03 0.13 0.03 0.03 0.27 0.05
 2) 0.08 0.02 0.12 0.07 0.52 0.01 0.07 0.04 0.01 0.06
 3) 0.02 0.03 0.22 0.29 0.17 0.10 0.09 0.00 0.05 0.03
 4) 0.04 0.03 0.21 0.13 0.04 0.01 0.10 0.04 0.22 0.18
 5) 0.11 0.22 0.02 0.01 0.06 0.18 0.33 0.04 0.01 0.01
\end{verbatim}

That does not mean it's uniform over the marginal probabilities of
each element. As the size of the simplex grows, the marginal draws
become more and more concentrated below (not around) \(1/K\). When one
component of the simplex is large, the others must all be relatively
small to compensate. For example, in a uniform distribution on
\(10\)-simplexes, the probability that a component is greater than the
mean of \(1/10\) is only 39\%. Most of the posterior marginal
probability mass for each component is in the interval \((0, 0.1)\).

When the \(\alpha\) value is small, the draws gravitate to the corners
of the simplex. Here are the first five draws for \(\alpha = 0.001\).

\begin{verbatim}
 1) 3e-203 0e+00 2e-298 9e-106 1e+000 0e+00 0e+000 1e-047 0e+00 4e-279
 2) 1e+000 0e+00 5e-279 2e-014 1e-275 0e+00 3e-285 9e-147 0e+00 0e+000
 3) 1e-308 0e+00 1e-213 0e+000 0e+000 8e-75 0e+000 1e+000 4e-58 7e-112
 4) 6e-166 5e-65 3e-068 3e-147 0e+000 1e+00 3e-249 0e+000 0e+00 0e+000
 5) 2e-091 0e+00 0e+000 0e+000 1e-060 0e+00 4e-312 1e+000 0e+00 0e+000
\end{verbatim}

Each row denotes a draw. Each draw has a single value that rounds to
one and other values that are very close to zero or rounded down to
zero.

As \(\alpha\) increases, the draws become increasingly uniform. For
\(\alpha = 1000\),

\begin{verbatim}
 1) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10
 2) 0.10 0.10 0.09 0.10 0.10 0.10 0.11 0.10 0.10 0.10
 3) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10
 4) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10
 5) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10
\end{verbatim}

\hypertarget{sampling-statement-56}{%
\subsection{Sampling statement}\label{sampling-statement-56}}

\texttt{theta\ \textasciitilde{}} \textbf{\texttt{dirichlet}}\texttt{(alpha)}

Increment target log probability density with \texttt{dirichlet\_lupdf(theta\ \textbar{}\ alpha)}.
\index{{\tt \bfseries dirichlet }!sampling statement|hyperpage}

\hypertarget{stan-functions-55}{%
\subsection{Stan functions}\label{stan-functions-55}}

\index{{\tt \bfseries dirichlet\_lpdf }!{\tt (vector theta | vector alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{dirichlet\_lpdf}}\texttt{(vector\ theta\ \textbar{}\ vector\ alpha)}\newline
The log of the Dirichlet density for simplex theta given prior counts
(plus one) alpha

\index{{\tt \bfseries dirichlet\_lupdf }!{\tt (vector theta | vector alpha): real}|hyperpage}

\texttt{real} \textbf{\texttt{dirichlet\_lupdf}}\texttt{(vector\ theta\ \textbar{}\ vector\ alpha)}\newline
The log of the Dirichlet density for simplex theta given prior counts
(plus one) alpha dropping constant additive terms

\index{{\tt \bfseries dirichlet\_rng }!{\tt (vector alpha): vector}|hyperpage}

\texttt{vector} \textbf{\texttt{dirichlet\_rng}}\texttt{(vector\ alpha)}\newline
Generate a Dirichlet variate with prior counts (plus one) alpha; may
only be used in transformed data and generated quantities blocks

\hypertarget{correlation-matrix-distributions}{%
\chapter{Correlation Matrix Distributions}\label{correlation-matrix-distributions}}

The correlation matrix distributions have support on the (Cholesky
factors of) correlation matrices. A Cholesky factor \(L\) for a \(K \times K\) correlation matrix \(\Sigma\) of dimension \(K\) has rows of
unit length so that the diagonal of \(L L^{\top}\) is the unit
\(K\)-vector. Even though models are usually conceptualized in terms of
correlation matrices, it is better to operationalize them in terms of
their Cholesky factors. If you are interested in the posterior
distribution of the correlations, you can recover them in the
generated quantities block via

\begin{verbatim}
 generated quantities {
   corr_matrix[K] Sigma;
   Sigma = multiply_lower_tri_self_transpose(L);
 }
\end{verbatim}

\hypertarget{lkj-correlation}{%
\section{LKJ correlation distribution}\label{lkj-correlation}}

\hypertarget{probability-density-function-32}{%
\subsection{Probability density function}\label{probability-density-function-32}}

For \(\eta > 0\), if \(\Sigma\) a positive-definite, symmetric matrix with
unit diagonal (i.e., a correlation matrix), then \[
\text{LkjCorr}(\Sigma|\eta) \propto \det \left( \Sigma \right)^{(\eta
- 1)}. \] The expectation is the identity matrix for any positive
value of the shape parameter \(\eta\), which can be interpreted like the
shape parameter of a symmetric beta distribution:

\begin{itemize}
\item
  if \(\eta = 1\), then the density is uniform over correlation
  matrices of order \(K\);
\item
  if \(\eta > 1\), the identity matrix is the modal correlation
  matrix, with a sharper peak in the density at the identity matrix
  for larger \(\eta\); and
\item
  for \(0 < \eta < 1\), the density has a trough at the identity
  matrix.
\item
  if \(\eta\) were an unknown parameter, the Jeffreys prior is
  proportional to \(\sqrt{2\sum_{k=1}^{K-1}\left( \psi_1\left(\eta+\frac{K-k-1}{2}\right) - 2\psi_1\left(2\eta+K-k-1 \right)\right)}\), where \(\psi_1()\) is the trigamma function
\end{itemize}

See (\protect\hyperlink{ref-LewandowskiKurowickaJoe:2009}{Lewandowski, Kurowicka, and Joe 2009}) for definitions. However, it is
much better computationally to work directly with the Cholesky factor
of \(\Sigma\), so this distribution should never be explicitly used in
practice.

\hypertarget{sampling-statement-57}{%
\subsection{Sampling statement}\label{sampling-statement-57}}

\texttt{y\ \textasciitilde{}} \textbf{\texttt{lkj\_corr}}\texttt{(eta)}

Increment target log probability density with \texttt{lkj\_corr\_lupdf(y\ \textbar{}\ eta)}.
\index{{\tt \bfseries lkj\_corr }!sampling statement|hyperpage}

\hypertarget{stan-functions-56}{%
\subsection{Stan functions}\label{stan-functions-56}}

\index{{\tt \bfseries lkj\_corr\_lpdf }!{\tt (matrix y \textbar\ real eta): real}|hyperpage}

\texttt{real} \textbf{\texttt{lkj\_corr\_lpdf}}\texttt{(matrix\ y\ \textbar{}\ real\ eta)}\newline
The log of the LKJ density for the correlation matrix y given
nonnegative shape eta. \texttt{lkj\_corr\_cholesky\_lpdf} is faster, more numerically
stable, uses less memory, and should be preferred to this.

\index{{\tt \bfseries lkj\_corr\_lupdf }!{\tt (matrix y \textbar\ real eta): real}|hyperpage}

\texttt{real} \textbf{\texttt{lkj\_corr\_lupdf}}\texttt{(matrix\ y\ \textbar{}\ real\ eta)}\newline
The log of the LKJ density for the correlation matrix y given
nonnegative shape eta dropping constant additive terms.
\texttt{lkj\_corr\_cholesky\_lupdf} is faster, more numerically stable, uses less memory,
and should be preferred to this.

\index{{\tt \bfseries lkj\_corr\_rng }!{\tt (int K, real eta): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{lkj\_corr\_rng}}\texttt{(int\ K,\ real\ eta)}\newline
Generate a LKJ random correlation matrix of order K with shape eta;
may only be used in transformed data and generated quantities blocks

\hypertarget{cholesky-lkj-correlation-distribution}{%
\section{Cholesky LKJ correlation distribution}\label{cholesky-lkj-correlation-distribution}}

Stan provides an implicit parameterization of the LKJ correlation
matrix density in terms of its Cholesky factor, which you should use
rather than the explicit parameterization in the previous section. For
example, if \texttt{L} is a Cholesky factor of a correlation matrix, then

\begin{verbatim}
 L ~ lkj_corr_cholesky(2.0); # implies L * L' ~ lkj_corr(2.0);
\end{verbatim}

Because Stan requires models to have support on all valid constrained
parameters, \texttt{L} will almost always\footnote{It is possible to build up a valid \texttt{L} within Stan, but that
  would then require Jacobian adjustments to imply the intended
  posterior.} be a parameter declared
with the type of a Cholesky factor for a correlation matrix; for
example,

\begin{verbatim}
 parameters {   cholesky_factor_corr[K] L;   # rather than corr_matrix[K] Sigma;   // ...
\end{verbatim}

\hypertarget{probability-density-function-33}{%
\subsection{Probability density function}\label{probability-density-function-33}}

For \(\eta > 0\), if \(L\) is a \(K \times K\) lower-triangular Cholesky
factor of a symmetric positive-definite matrix with unit diagonal
(i.e., a correlation matrix), then \[ \text{LkjCholesky}(L|\eta)
\propto \left|J\right|\det(L L^\top)^{(\eta - 1)} = \prod_{k=2}^K
L_{kk}^{K-k+2\eta-2}. \] See the previous section for details on
interpreting the shape parameter \(\eta\). Note that even if \(\eta=1\),
it is still essential to evaluate the density function because the
density of \(L\) is not constant, regardless of the value of \(\eta\),
even though the density of \(LL^\top\) is constant iff \(\eta=1\).

A lower triangular \(L\) is a Cholesky factor for a correlation matrix
if and only if \(L_{k,k} > 0\) for \(k \in 1{:}K\) and each row \(L_k\) has
unit Euclidean length.

\hypertarget{sampling-statement-58}{%
\subsection{Sampling statement}\label{sampling-statement-58}}

\texttt{L\ \textasciitilde{}} \textbf{\texttt{lkj\_corr\_cholesky}}\texttt{(eta)}

Increment target log probability density with \texttt{lkj\_corr\_cholesky\_lupdf(L\ \textbar{}\ eta)}.
\index{{\tt \bfseries lkj\_corr\_cholesky }!sampling statement|hyperpage}

\hypertarget{stan-functions-57}{%
\subsection{Stan functions}\label{stan-functions-57}}

\index{{\tt \bfseries lkj\_corr\_cholesky\_lpdf }!{\tt (matrix L \textbar\ real eta): real}|hyperpage}

\texttt{real} \textbf{\texttt{lkj\_corr\_cholesky\_lpdf}}\texttt{(matrix\ L\ \textbar{}\ real\ eta)}\newline
The log of the LKJ density for the lower-triangular Cholesky factor L
of a correlation matrix given shape eta

\index{{\tt \bfseries lkj\_corr\_cholesky\_lupdf }!{\tt (matrix L \textbar\ real eta): real}|hyperpage}

\texttt{real} \textbf{\texttt{lkj\_corr\_cholesky\_lupdf}}\texttt{(matrix\ L\ \textbar{}\ real\ eta)}\newline
The log of the LKJ density for the lower-triangular Cholesky factor L
of a correlation matrix given shape eta dropping constant additive terms

\index{{\tt \bfseries lkj\_corr\_cholesky\_rng }!{\tt (int K, real eta): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{lkj\_corr\_cholesky\_rng}}\texttt{(int\ K,\ real\ eta)}\newline
Generate a random Cholesky factor of a correlation matrix of order K
that is distributed LKJ with shape eta; may only be used in transformed data
and generated quantities blocks

\hypertarget{covariance-matrix-distributions}{%
\chapter{Covariance Matrix Distributions}\label{covariance-matrix-distributions}}

The covariance matrix distributions have support on symmetric,
positive-definite \(K \times K\) matrices.

\hypertarget{wishart-distribution}{%
\section{Wishart distribution}\label{wishart-distribution}}

\hypertarget{probability-density-function-34}{%
\subsection{Probability density function}\label{probability-density-function-34}}

If \(K \in \mathbb{N}\), \(\nu \in (K-1,\infty)\), and \(S \in \mathbb{R}^{K \times K}\) is symmetric and positive definite, then for
symmetric and positive-definite \(W \in \mathbb{R}^{K \times K}\), \[
\text{Wishart}(W|\nu,S) = \frac{1}{2^{\nu K / 2}} \ \frac{1}{\Gamma_K
\! \left( \frac{\nu}{2} \right)} \ \left| S \right|^{-\nu/2} \ \left|
W \right|^{(\nu - K - 1)/2} \ \exp \! \left(- \frac{1}{2} \
\text{tr}\left( S^{-1} W \right) \right) \! , \] where \(\text{tr}()\)
is the matrix trace function, and \(\Gamma_K()\) is the multivariate
Gamma function, \[ \Gamma_K(x) = \frac{1}{\pi^{K(K-1)/4}} \
\prod_{k=1}^K \Gamma \left( x + \frac{1 - k}{2} \right) \!. \]

\hypertarget{sampling-statement-59}{%
\subsection{Sampling statement}\label{sampling-statement-59}}

\texttt{W\ \textasciitilde{}} \textbf{\texttt{wishart}}\texttt{(nu,\ Sigma)}

Increment target log probability density with \texttt{wishart\_lupdf(W\ \textbar{}\ nu,\ Sigma)}.
\index{{\tt \bfseries wishart }!sampling statement|hyperpage}

\hypertarget{stan-functions-58}{%
\subsection{Stan functions}\label{stan-functions-58}}

\index{{\tt \bfseries wishart\_lpdf }!{\tt (matrix W \textbar\ real nu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{wishart\_lpdf}}\texttt{(matrix\ W\ \textbar{}\ real\ nu,\ matrix\ Sigma)}\newline
The log of the Wishart density for symmetric and positive-definite
matrix W given degrees of freedom nu and symmetric and
positive-definite scale matrix Sigma

\index{{\tt \bfseries wishart\_lupdf }!{\tt (matrix W \textbar\ real nu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{wishart\_lupdf}}\texttt{(matrix\ W\ \textbar{}\ real\ nu,\ matrix\ Sigma)}\newline
The log of the Wishart density for symmetric and positive-definite
matrix W given degrees of freedom nu and symmetric and
positive-definite scale matrix Sigma dropping constant additive terms

\index{{\tt \bfseries wishart\_rng }!{\tt (real nu, matrix Sigma): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{wishart\_rng}}\texttt{(real\ nu,\ matrix\ Sigma)}\newline
Generate a Wishart variate with degrees of freedom nu and symmetric
and positive-definite scale matrix Sigma; may only be used in
transformed data and generated quantities blocks

\hypertarget{inverse-wishart-distribution}{%
\section{Inverse Wishart distribution}\label{inverse-wishart-distribution}}

\hypertarget{probability-density-function-35}{%
\subsection{Probability density function}\label{probability-density-function-35}}

If \(K \in \mathbb{N}\), \(\nu \in (K-1,\infty)\), and \(S \in \mathbb{R}^{K \times K}\) is symmetric and positive definite, then
for symmetric and positive-definite \(W \in \mathbb{R}^{K \times K}\),
\[ \text{InvWishart}(W|\nu,S) = \frac{1}{2^{\nu K / 2}} \
\frac{1}{\Gamma_K \! \left( \frac{\nu}{2} \right)} \ \left| S
\right|^{\nu/2} \ \left| W \right|^{-(\nu + K + 1)/2} \ \exp \! \left(
- \frac{1}{2} \ \text{tr}(SW^{-1}) \right) \! . \]

\hypertarget{sampling-statement-60}{%
\subsection{Sampling statement}\label{sampling-statement-60}}

\texttt{W\ \textasciitilde{}} \textbf{\texttt{inv\_wishart}}\texttt{(nu,\ Sigma)}

Increment target log probability density with \texttt{inv\_wishart\_lupdf(W\ \textbar{}\ nu,\ Sigma)}.
\index{{\tt \bfseries inv\_wishart }!sampling statement|hyperpage}

\hypertarget{stan-functions-59}{%
\subsection{Stan functions}\label{stan-functions-59}}

\index{{\tt \bfseries inv\_wishart\_lpdf }!{\tt (matrix W \textbar\ real nu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_wishart\_lpdf}}\texttt{(matrix\ W\ \textbar{}\ real\ nu,\ matrix\ Sigma)}\newline
The log of the inverse Wishart density for symmetric and
positive-definite matrix W given degrees of freedom nu and symmetric
and positive-definite scale matrix Sigma

\index{{\tt \bfseries inv\_wishart\_lupdf }!{\tt (matrix W \textbar\ real nu, matrix Sigma): real}|hyperpage}

\texttt{real} \textbf{\texttt{inv\_wishart\_lupdf}}\texttt{(matrix\ W\ \textbar{}\ real\ nu,\ matrix\ Sigma)}\newline
The log of the inverse Wishart density for symmetric and
positive-definite matrix W given degrees of freedom nu and symmetric
and positive-definite scale matrix Sigma dropping constant additive terms

\index{{\tt \bfseries inv\_wishart\_rng }!{\tt (real nu, matrix Sigma): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{inv\_wishart\_rng}}\texttt{(real\ nu,\ matrix\ Sigma)}\newline
Generate an inverse Wishart variate with degrees of freedom nu and
symmetric and positive-definite scale matrix Sigma; may only be used
in transformed data and generated quantities blocks

\hypertarget{additional-distributions}{%
\chapter*{Additional Distributions}\label{additional-distributions}}
\addcontentsline{toc}{chapter}{Additional Distributions}

\hypertarget{hidden-markov-models}{%
\chapter{Hidden Markov Models}\label{hidden-markov-models}}

An elementary first-order Hidden Markov model
is a probabilistic model over
\(N\) observations, \(y_n\), and \(N\) hidden states, \(x_n\),
which can be fully defined by the conditional distributions
\(p(y_n \mid x_n, \phi)\) and \(p(x_n \mid x_{n - 1}, \phi)\).
Here we make the dependency on additional model parameters, \(\phi\),
explicit.
When \(x\) is continuous, the user can explicitly encode these distributions
in Stan and use Markov chain Monte Carlo to integrate \(x\) out.

When each state \(x\) takes a value over a discrete and finite set, say \(\{1, 2, ..., K\}\),
we can take advantage of the dependency structure
to marginalize \(x\) and compute \(p(y \mid \phi)\).
We start by defining the conditional observational distribution,
stored in a \(K \times N\) matrix \(\omega\) with
\[
\omega_{kn} = p(y_n \mid x_n = k, \phi).
\]
Next, we introduce the \(K \times K\) transition matrix, \(\Gamma\), with
\[
  \Gamma_{ij} = p(x_n = j \mid x_{n - 1} = i, \phi).
\]
Each row defines a probability distribution
and must therefore be a simplex (i.e.~its components must add to 1).
Currently, Stan only supports stationary transitions where a single
transition matrix is used for all transitions.
Finally we define the initial state \(K\)-vector \(\rho\), with
\[
  \rho_k = p(x_0 = k \mid \phi).
\]

The Stan functions that support this type of model are special
in that the user does not explicitly pass \(y\) and \(\phi\) as arguments.
Instead, the user passes \(\log \omega\), \(\Gamma\), and \(\rho\),
which in turn depend on \(y\) and \(\phi\).

\hypertarget{hmm-stan-functions}{%
\section{Stan functions}\label{hmm-stan-functions}}

\index{{\tt \bfseries hmm\_marginal }!{\tt (matrix log\_omega, matrix Gamma, vector rho): real}|hyperpage}

\texttt{real} \textbf{\texttt{hmm\_marginal}}\texttt{(matrix\ log\_omega,\ matrix\ Gamma,\ vector\ rho)}\newline
Returns the log probability density of \(y\), with \(x_n\) integrated out at each iteration.
The arguments represent (1) the log density of each output, (2) the transition matrix, and (3) the initial state vector.

\begin{itemize}
\item
  \emph{\texttt{log\_omega}}: \(\log \omega_{kn} = \log p(y_n \mid x_n = k, \phi)\), log density of each output,
\item
  \emph{\texttt{Gamma}}: \(\Gamma_{ij} = p(x_n = j | x_{n - 1} = i, \phi)\), the transition matrix,
\item
  \emph{\texttt{rho}}: \(\rho_k = p(x_0 = k \mid \phi)\), the initial state probability.
\end{itemize}

\index{{\tt \bfseries hmm\_latent\_rng }!{\tt (matrix log\_omega, matrix Gamma, vector rho): int[]}|hyperpage}

\texttt{int{[}{]}} \textbf{\texttt{hmm\_latent\_rng}}\texttt{(matrix\ log\_omega,\ matrix\ Gamma,\ vector\ rho)}\newline
Returns a length \(N\) array of integers over \(\{1, ..., K\}\),
sampled from the joint posterior distribution of the hidden states,
\(p(x \mid \phi, y)\).
May be only used in transformed data and generated quantities.

\index{{\tt \bfseries hmm\_hidden\_state\_prob }!{\tt (matrix log\_omega, matrix Gamma, vector rho): matrix}|hyperpage}

\texttt{matrix} \textbf{\texttt{hmm\_hidden\_state\_prob}}\texttt{(matrix\ log\_omega,\ matrix\ Gamma,\ vector\ rho)}\newline
Returns the matrix of marginal posterior probabilities of each hidden state value. This will be a \(K \times N\) matrix.
The \(n^\mathrm{th}\) column is a simplex of probabilities for the \(n^\mathrm{th}\) variable.
Moreover, let \(A\) be the output. Then
\(A_{ij} = p(x_j = i \mid \phi, y)\).
This function may only be used in transformed data and generated quantities.

\hypertarget{appendix}{%
\chapter*{Appendix}\label{appendix}}
\addcontentsline{toc}{chapter}{Appendix}

\hypertarget{math-functions}{%
\chapter{Mathematical Functions}\label{math-functions}}

This appendix provides the definition of several mathematical
functions used throughout the manual.

\hypertarget{beta-appendix}{%
\section{Beta}\label{beta-appendix}}

The beta function, \(\text{B}(a, b)\), computes the normalizing
constant for the beta distribution, and is defined for \(a > 0\) and \(b > 0\) by \[ \text{B}(a,b) \ = \ \int_0^1 u^{a - 1} (1 - u)^{b - 1} \,
du \ = \ \frac{\Gamma(a) \, \Gamma(b)}{\Gamma(a+b)} \, , \]
where \(\Gamma(x)\) is the \protect\hyperlink{gamma-appendix}{Gamma function}.

\hypertarget{inc-beta-appendix}{%
\section{Incomplete beta}\label{inc-beta-appendix}}

The incomplete beta function, \(\text{B}(x; a, b)\), is defined for \(x \in [0, 1]\) and \(a, b \geq 0\) such that \(a + b \neq 0\) by \[
\text{B}(x; \, a, b) \ = \ \int_0^x u^{a - 1} \, (1 - u)^{b - 1} \,
du, \] where \(\text{B}(a, b)\) is the beta function defined in
\protect\hyperlink{beta-appendix}{appendix}. If \(x = 1\), the incomplete beta function
reduces to the beta function, \(\text{B}(1; a, b) = \text{B}(a, b)\).

The regularized incomplete beta function divides the incomplete beta
function by the beta function, \[ I_x(a, b) \ = \ \frac{\text{B}(x; \,
a, b)}{B(a, b)} \, . \]

\hypertarget{gamma-appendix}{%
\section{Gamma}\label{gamma-appendix}}

The gamma function, \(\Gamma(x)\), is the generalization of the
factorial function to continuous variables, defined so that for
positive integers \(n\), \[ \Gamma(n+1) = n! \] Generalizing to all
positive numbers and non-integer negative numbers, \[ \Gamma(x) =
\int_0^{\infty} u^{x - 1} \exp(-u) \, du. \]

\hypertarget{digamma-appendix}{%
\section{Digamma}\label{digamma-appendix}}

The digamma function \(\Psi\) is the derivative of the \(\log \Gamma\)
function, \[ \Psi(u) \ = \ \frac{d}{d u} \log \Gamma(u) \ = \
\frac{1}{\Gamma(u)} \ \frac{d}{d u} \Gamma(u). \]

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-Bailey:2005}{}%
Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. 2005. {``A Comparison of Three High-Precision Quadrature Schemes.''} \emph{Experiment. Math.} 14 (3): 317--29. \url{https://projecteuclid.org:443/euclid.em/1128371757}.

\leavevmode\hypertarget{ref-BowlingEtAl:2009}{}%
Bowling, Shannon R., Mohammad T. Khasawneh, Sittichai Kaewkuekool, and Byung Rae Cho. 2009. {``A Logistic Approximation to the Cumulative Normal Distribution.''} \emph{Journal of Industrial Engineering and Management} 2 (1): 114--27.

\leavevmode\hypertarget{ref-Ding:18}{}%
Ding, Peng, and Joseph K. Blitzstein. 2018. {``On the Gaussian Mixture Representation of the Laplace Distribution.''} \emph{The American Statistician} 72 (2): 172--74. \url{https://doi.org/10.1080/00031305.2017.1291448}.

\leavevmode\hypertarget{ref-DurbinKoopman:2001}{}%
Durbin, J., and S. J. Koopman. 2001. \emph{Time Series Analysis by State Space Methods}. New York: Oxford University Press.

\leavevmode\hypertarget{ref-Feller1968}{}%
Feller, William. 1968. \emph{An Introduction to Probability Theory and Its Applications}. Vol. 1. 3. Wiley, New York.

\leavevmode\hypertarget{ref-GelmanEtAl:2013}{}%
Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. \emph{Bayesian Data Analysis}. Third Edition. London: Chapman \& Hall / CRC Press.

\leavevmode\hypertarget{ref-Eigen:2013}{}%
Guennebaud, Gal, Benot Jacob, and others. 2010. {``Eigen V3.''} http://eigen.tuxfamily.org.

\leavevmode\hypertarget{ref-Hindmarsh:2005}{}%
Hindmarsh, Alan C, Peter N Brown, Keith E Grant, Steven L Lee, Radu Serban, Dan E Shumaker, and Carol S Woodward. 2005. {``{SUNDIALS}: Suite of Nonlinear and Differential/Algebraic Equation Solvers.''} \emph{ACM Transactions on Mathematical Software (TOMS)} 31 (3): 363--96.

\leavevmode\hypertarget{ref-minpack:1980}{}%
Jorge J. More, Kenneth E. Hillstrom, Burton S. Garbow. 1980. \emph{User Guide for MINPACK-1}. 9700 South Cass Avenue, Argonne, Illinois 60439: Argonne National Laboratory.

\leavevmode\hypertarget{ref-LewandowskiKurowickaJoe:2009}{}%
Lewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. {``Generating Random Correlation Matrices Based on Vines and Extended Onion Method.''} \emph{Journal of Multivariate Analysis} 100: 1989--2001.

\leavevmode\hypertarget{ref-Mori:1978}{}%
Mori, Masatake. 1978. {``An IMT-Type Double Exponential Formula for Numerical Integration.''} \emph{Publications of the Research Institute for Mathematical Sciences} 14 (3): 713--29. \url{https://doi.org/10.2977/prims/1195188835}.

\leavevmode\hypertarget{ref-NavarroFuss2009}{}%
Navarro, Danielle J, and Ian G Fuss. 2009. {``Fast and Accurate Calculations for First-Passage Times in {W}iener Diffusion Models.''} \emph{Journal of Mathematical Psychology} 53 (4): 222--30.

\leavevmode\hypertarget{ref-Powell:1970}{}%
Powell, Michael J. D. 1970. {``A Hybrid Method for Nonlinear Equations.''} In \emph{Numerical Methods for Nonlinear Algebraic Equations}, edited by P. Rabinowitz. Gordon; Breach.

\leavevmode\hypertarget{ref-Takahasi:1974}{}%
Takahasi, Hidetosi, and Masatake Mori. 1974. {``Double Exponential Formulas for Numerical Integration.''} \emph{Publications of the Research Institute for Mathematical Sciences} 9 (3): 721--41. \url{https://doi.org/10.2977/prims/1195192451}.

\leavevmode\hypertarget{ref-Tanaka:2009}{}%
Tanaka, Ken'ichiro, Masaaki Sugihara, Kazuo Murota, and Masatake Mori. 2009. {``Function Classes for Double Exponential Integration Formulas.''} \emph{Numerische Mathematik} 111 (4): 631--55. \url{https://doi.org/10.1007/s00211-008-0195-1}.

\leavevmode\hypertarget{ref-Vandekerckhove-Wabersich:2014}{}%
Vandekerckhove, Joachim, and Dominik Wabersich. 2014. {``The {R}{W}iener Package: An {R} Package Providing Distribution Functions for the {W}iener Diffusion Model.''} \emph{The R Journal} 6/1. \url{http://journal.r-project.org/archive/2014-1/vandekerckhove-wabersich.pdf}.

\end{CSLReferences}

{\footnotesize
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Index}
\printindex
}

\backmatter

\end{document}
